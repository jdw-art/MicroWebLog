2024-08-04 17:10:28.041 [http-nio-8088-exec-1] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 17:10:28.041 [http-nio-8088-exec-5] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 17:10:28.041 [http-nio-8088-exec-6] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 17:10:28.041 [http-nio-8088-exec-3] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 17:10:28.041 [http-nio-8088-exec-4] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 17:10:28.480 [http-nio-8088-exec-1] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 17:10:28.480 [http-nio-8088-exec-3] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 17:10:28.480 [http-nio-8088-exec-5] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 17:10:28.480 [http-nio-8088-exec-6] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 17:10:28.480 [http-nio-8088-exec-4] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 17:10:28.929 [http-nio-8088-exec-2] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 17:10:28.929 [http-nio-8088-exec-2] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 17:10:28.936 [http-nio-8088-exec-8] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 17:10:28.936 [http-nio-8088-exec-8] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 17:10:29.196 [http-nio-8088-exec-7] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 17:10:29.196 [http-nio-8088-exec-9] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 17:10:29.196 [http-nio-8088-exec-7] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 17:10:29.196 [http-nio-8088-exec-9] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 17:10:29.257 [http-nio-8088-exec-10] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 17:10:29.258 [http-nio-8088-exec-10] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 17:10:32.017 [http-nio-8088-exec-9] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [前台获取统计信息], 入参: , 请求类: StatisticsController, 请求方法: findInfo =================================== 
2024-08-04 17:10:32.017 [http-nio-8088-exec-6] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [前台获取统计信息], 入参: , 请求类: StatisticsController, 请求方法: findInfo =================================== 
2024-08-04 17:10:32.017 [http-nio-8088-exec-5] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [前台获取标签列表], 入参: , 请求类: TagController, 请求方法: findTagList =================================== 
2024-08-04 17:10:32.017 [http-nio-8088-exec-3] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [前台获取博客详情], 入参: , 请求类: BlogSettingsController, 请求方法: findDetail =================================== 
2024-08-04 17:10:32.017 [http-nio-8088-exec-2] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [前台获取博客详情], 入参: , 请求类: BlogSettingsController, 请求方法: findDetail =================================== 
2024-08-04 17:10:32.017 [http-nio-8088-exec-7] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [前台获取标签列表], 入参: , 请求类: TagController, 请求方法: findTagList =================================== 
2024-08-04 17:10:36.222 [http-nio-8088-exec-10] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [获取知识库数据], 入参: , 请求类: WikiController, 请求方法: findWikiList =================================== 
2024-08-04 17:10:36.222 [http-nio-8088-exec-1] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [获取知识库数据], 入参: , 请求类: WikiController, 请求方法: findWikiList =================================== 
2024-08-04 17:10:37.537 [http-nio-8088-exec-8] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [前台获取分类列表], 入参: {"size":10}, 请求类: CategoryController, 请求方法: findCategoryList =================================== 
2024-08-04 17:10:37.537 [http-nio-8088-exec-4] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [前台获取分类列表], 入参: {"size":10}, 请求类: CategoryController, 请求方法: findCategoryList =================================== 
2024-08-04 17:10:41.873 [http-nio-8088-exec-2] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [前台获取博客详情], 耗时: 9833ms, 出参: {"success":true,"message":null,"errorCode":null,"data":{"logo":"http://110.41.141.141:9000/weblog/weblog/9853f8be13cb4f7fae00e3f5233dd688.png","name":"WebLog","author":"Jacob","introduction":"求知若饥，虚心若愚","avatar":"http://110.41.141.141:9000/weblog/weblog/cf0958d87787449fb05aae4cc84015c6.jpg","githubHomepage":"https://github.com/jdw-art","csdnHomepage":"https://www.csdn.net/?spm=1010.2135.3001.4476","giteeHomepage":"","zhihuHomepage":"https://www.zhihu.com/people/54-10-50-93"}} =================================== 
2024-08-04 17:10:41.874 [http-nio-8088-exec-8] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [前台获取分类列表], 耗时: 4415ms, 出参: {"success":true,"message":null,"errorCode":null,"data":[{"id":29,"name":"Java","articlesTotal":7},{"id":31,"name":"SNN","articlesTotal":6},{"id":30,"name":"工具","articlesTotal":3},{"id":33,"name":"SpringBoot","articlesTotal":3},{"id":34,"name":"Redis","articlesTotal":2},{"id":41,"name":"高可用","articlesTotal":2},{"id":32,"name":"Spring","articlesTotal":1},{"id":35,"name":"RabbitMQ","articlesTotal":1},{"id":36,"name":"MySQL","articlesTotal":1},{"id":37,"name":"MongoDB","articlesTotal":1}]} =================================== 
2024-08-04 17:10:41.874 [http-nio-8088-exec-7] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [前台获取标签列表], 耗时: 9843ms, 出参: {"success":true,"message":null,"errorCode":null,"data":[{"id":47,"name":"java","articlesTotal":14},{"id":48,"name":"脉冲神经网络","articlesTotal":6},{"id":49,"name":"snn","articlesTotal":6},{"id":54,"name":"联想记忆模型","articlesTotal":1},{"id":55,"name":"集合","articlesTotal":1},{"id":56,"name":"并发","articlesTotal":2},{"id":57,"name":"RabbitMQ","articlesTotal":2},{"id":58,"name":"spring","articlesTotal":4},{"id":59,"name":"springboot","articlesTotal":4},{"id":60,"name":"mongodb","articlesTotal":2},{"id":61,"name":"nosql","articlesTotal":1},{"id":62,"name":"redis","articlesTotal":2},{"id":63,"name":"mysql","articlesTotal":1},{"id":64,"name":"jvm","articlesTotal":1},{"id":65,"name":"elasticSearch","articlesTotal":1},{"id":66,"name":"SpringMVC","articlesTotal":1},{"id":67,"name":"计算机网络","articlesTotal":1},{"id":68,"name":"操作系统","articlesTotal":1},{"id":69,"name":"设计模式","articlesTotal":1},{"id":70,"name":"SpringCloud","articlesTotal":2},{"id":71,"name":"微服务","articlesTotal":4},{"id":72,"name":"jmm","articlesTotal":1},{"id":73,"name":"高可用","articlesTotal":3},{"id":74,"name":"系统设计","articlesTotal":3},{"id":76,"name":"海量数据","articlesTotal":1}]} =================================== 
2024-08-04 17:10:42.280 [http-nio-8088-exec-4] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [前台获取分类列表], 耗时: 4888ms, 出参: {"success":true,"message":null,"errorCode":null,"data":[{"id":29,"name":"Java","articlesTotal":7},{"id":31,"name":"SNN","articlesTotal":6},{"id":30,"name":"工具","articlesTotal":3},{"id":33,"name":"SpringBoot","articlesTotal":3},{"id":34,"name":"Redis","articlesTotal":2},{"id":41,"name":"高可用","articlesTotal":2},{"id":32,"name":"Spring","articlesTotal":1},{"id":35,"name":"RabbitMQ","articlesTotal":1},{"id":36,"name":"MySQL","articlesTotal":1},{"id":37,"name":"MongoDB","articlesTotal":1}]} =================================== 
2024-08-04 17:10:42.317 [http-nio-8088-exec-5] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [前台获取标签列表], 耗时: 10453ms, 出参: {"success":true,"message":null,"errorCode":null,"data":[{"id":47,"name":"java","articlesTotal":14},{"id":48,"name":"脉冲神经网络","articlesTotal":6},{"id":49,"name":"snn","articlesTotal":6},{"id":54,"name":"联想记忆模型","articlesTotal":1},{"id":55,"name":"集合","articlesTotal":1},{"id":56,"name":"并发","articlesTotal":2},{"id":57,"name":"RabbitMQ","articlesTotal":2},{"id":58,"name":"spring","articlesTotal":4},{"id":59,"name":"springboot","articlesTotal":4},{"id":60,"name":"mongodb","articlesTotal":2},{"id":61,"name":"nosql","articlesTotal":1},{"id":62,"name":"redis","articlesTotal":2},{"id":63,"name":"mysql","articlesTotal":1},{"id":64,"name":"jvm","articlesTotal":1},{"id":65,"name":"elasticSearch","articlesTotal":1},{"id":66,"name":"SpringMVC","articlesTotal":1},{"id":67,"name":"计算机网络","articlesTotal":1},{"id":68,"name":"操作系统","articlesTotal":1},{"id":69,"name":"设计模式","articlesTotal":1},{"id":70,"name":"SpringCloud","articlesTotal":2},{"id":71,"name":"微服务","articlesTotal":4},{"id":72,"name":"jmm","articlesTotal":1},{"id":73,"name":"高可用","articlesTotal":3},{"id":74,"name":"系统设计","articlesTotal":3},{"id":76,"name":"海量数据","articlesTotal":1}]} =================================== 
2024-08-04 17:10:42.439 [http-nio-8088-exec-9] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [前台获取统计信息], 耗时: 10576ms, 出参: {"success":true,"message":null,"errorCode":null,"data":{"articleTotalCount":31,"categoryTotalCount":15,"tagTotalCount":25,"pvTotalCount":880}} =================================== 
2024-08-04 17:10:42.439 [http-nio-8088-exec-6] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [前台获取统计信息], 耗时: 10576ms, 出参: {"success":true,"message":null,"errorCode":null,"data":{"articleTotalCount":31,"categoryTotalCount":15,"tagTotalCount":25,"pvTotalCount":880}} =================================== 
2024-08-04 17:10:42.680 [http-nio-8088-exec-3] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [前台获取博客详情], 耗时: 9833ms, 出参: {"success":true,"message":null,"errorCode":null,"data":{"logo":"http://110.41.141.141:9000/weblog/weblog/9853f8be13cb4f7fae00e3f5233dd688.png","name":"WebLog","author":"Jacob","introduction":"求知若饥，虚心若愚","avatar":"http://110.41.141.141:9000/weblog/weblog/cf0958d87787449fb05aae4cc84015c6.jpg","githubHomepage":"https://github.com/jdw-art","csdnHomepage":"https://www.csdn.net/?spm=1010.2135.3001.4476","giteeHomepage":"","zhihuHomepage":"https://www.zhihu.com/people/54-10-50-93"}} =================================== 
2024-08-04 17:10:43.863 [http-nio-8088-exec-7] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 17:10:43.863 [http-nio-8088-exec-4] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 17:10:43.863 [http-nio-8088-exec-3] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 17:10:43.863 [http-nio-8088-exec-8] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 17:10:43.863 [http-nio-8088-exec-9] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 17:10:43.863 [http-nio-8088-exec-5] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 17:10:43.863 [http-nio-8088-exec-6] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 17:10:43.863 [http-nio-8088-exec-2] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 17:10:48.479 [http-nio-8088-exec-2] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 17:10:48.479 [http-nio-8088-exec-2] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 17:10:48.480 [http-nio-8088-exec-3] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 17:10:48.481 [http-nio-8088-exec-3] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 17:10:48.489 [http-nio-8088-exec-5] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 17:10:48.490 [http-nio-8088-exec-5] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 17:10:48.496 [http-nio-8088-exec-8] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 17:10:48.497 [http-nio-8088-exec-8] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 17:10:48.519 [http-nio-8088-exec-8] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [获取知识库数据], 入参: , 请求类: WikiController, 请求方法: findWikiList =================================== 
2024-08-04 17:10:48.685 [http-nio-8088-exec-2] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [前台获取统计信息], 入参: , 请求类: StatisticsController, 请求方法: findInfo =================================== 
2024-08-04 17:10:48.806 [http-nio-8088-exec-3] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [前台获取分类列表], 入参: {"size":10}, 请求类: CategoryController, 请求方法: findCategoryList =================================== 
2024-08-04 17:10:48.810 [http-nio-8088-exec-3] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [前台获取分类列表], 耗时: 4ms, 出参: {"success":true,"message":null,"errorCode":null,"data":[{"id":29,"name":"Java","articlesTotal":7},{"id":31,"name":"SNN","articlesTotal":6},{"id":30,"name":"工具","articlesTotal":3},{"id":33,"name":"SpringBoot","articlesTotal":3},{"id":34,"name":"Redis","articlesTotal":2},{"id":41,"name":"高可用","articlesTotal":2},{"id":32,"name":"Spring","articlesTotal":1},{"id":35,"name":"RabbitMQ","articlesTotal":1},{"id":36,"name":"MySQL","articlesTotal":1},{"id":37,"name":"MongoDB","articlesTotal":1}]} =================================== 
2024-08-04 17:10:48.811 [http-nio-8088-exec-3] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 17:10:48.839 [http-nio-8088-exec-4] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 17:10:48.839 [http-nio-8088-exec-4] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 17:10:48.841 [http-nio-8088-exec-4] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [前台获取标签列表], 入参: , 请求类: TagController, 请求方法: findTagList =================================== 
2024-08-04 17:10:48.883 [http-nio-8088-exec-4] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [前台获取标签列表], 耗时: 42ms, 出参: {"success":true,"message":null,"errorCode":null,"data":[{"id":47,"name":"java","articlesTotal":14},{"id":48,"name":"脉冲神经网络","articlesTotal":6},{"id":49,"name":"snn","articlesTotal":6},{"id":54,"name":"联想记忆模型","articlesTotal":1},{"id":55,"name":"集合","articlesTotal":1},{"id":56,"name":"并发","articlesTotal":2},{"id":57,"name":"RabbitMQ","articlesTotal":2},{"id":58,"name":"spring","articlesTotal":4},{"id":59,"name":"springboot","articlesTotal":4},{"id":60,"name":"mongodb","articlesTotal":2},{"id":61,"name":"nosql","articlesTotal":1},{"id":62,"name":"redis","articlesTotal":2},{"id":63,"name":"mysql","articlesTotal":1},{"id":64,"name":"jvm","articlesTotal":1},{"id":65,"name":"elasticSearch","articlesTotal":1},{"id":66,"name":"SpringMVC","articlesTotal":1},{"id":67,"name":"计算机网络","articlesTotal":1},{"id":68,"name":"操作系统","articlesTotal":1},{"id":69,"name":"设计模式","articlesTotal":1},{"id":70,"name":"SpringCloud","articlesTotal":2},{"id":71,"name":"微服务","articlesTotal":4},{"id":72,"name":"jmm","articlesTotal":1},{"id":73,"name":"高可用","articlesTotal":3},{"id":74,"name":"系统设计","articlesTotal":3},{"id":76,"name":"海量数据","articlesTotal":1}]} =================================== 
2024-08-04 17:10:48.884 [http-nio-8088-exec-4] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 17:10:48.894 [http-nio-8088-exec-5] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [前台获取博客详情], 入参: , 请求类: BlogSettingsController, 请求方法: findDetail =================================== 
2024-08-04 17:10:48.926 [http-nio-8088-exec-2] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [前台获取统计信息], 耗时: 241ms, 出参: {"success":true,"message":null,"errorCode":null,"data":{"articleTotalCount":31,"categoryTotalCount":15,"tagTotalCount":25,"pvTotalCount":880}} =================================== 
2024-08-04 17:10:48.927 [http-nio-8088-exec-2] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 17:10:49.042 [http-nio-8088-exec-5] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [前台获取博客详情], 耗时: 3ms, 出参: {"success":true,"message":null,"errorCode":null,"data":{"logo":"http://110.41.141.141:9000/weblog/weblog/9853f8be13cb4f7fae00e3f5233dd688.png","name":"WebLog","author":"Jacob","introduction":"求知若饥，虚心若愚","avatar":"http://110.41.141.141:9000/weblog/weblog/cf0958d87787449fb05aae4cc84015c6.jpg","githubHomepage":"https://github.com/jdw-art","csdnHomepage":"https://www.csdn.net/?spm=1010.2135.3001.4476","giteeHomepage":"","zhihuHomepage":"https://www.zhihu.com/people/54-10-50-93"}} =================================== 
2024-08-04 17:10:49.046 [http-nio-8088-exec-5] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 17:10:50.285 [http-nio-8088-exec-1] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [获取知识库数据], 耗时: 13391ms, 出参: {"success":true,"message":null,"errorCode":null,"data":[{"id":6,"cover":"http://110.41.141.141:9000/weblog/fea0e0402a73479ab1603148b4187027.jpg","title":"中间件","summary":"后端开发中间件","isTop":false,"firstArticleId":39},{"id":5,"cover":"http://110.41.141.141:9000/weblog/weblog/ea44696cf3dd408584d04a46452bfbd6.jpg","title":"计算机基础","summary":"计算机基础知识","isTop":false,"firstArticleId":54},{"id":4,"cover":"http://110.41.141.141:9000/weblog/weblog/80061883b8514ae3a557fe8d7db8f6b2.jpg","title":"项目","summary":"项目整理","isTop":false,"firstArticleId":43},{"id":3,"cover":"http://110.41.141.141:9000/weblog/weblog/fb84627b093e4fa98ef9bdf5fbfe8cbc.jpg","title":"系统设计","summary":"系统设计","isTop":false,"firstArticleId":40},{"id":2,"cover":"http://110.41.141.141:9000/weblog/weblog/ca753e837e2344319d9f58a92d25403f.jpg","title":"Java","summary":"Java后端开发","isTop":false,"firstArticleId":33},{"id":1,"cover":"http://110.41.141.141:9000/weblog/weblog/2b484d7d659f4a3aaa9ba74271e55fda.jpg","title":"数据库","summary":"数据库，包括关系型数据库、非关系型数据库以及缓存等","isTop":false,"firstArticleId":47}]} =================================== 
2024-08-04 17:10:50.285 [http-nio-8088-exec-10] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [获取知识库数据], 耗时: 13391ms, 出参: {"success":true,"message":null,"errorCode":null,"data":[{"id":6,"cover":"http://110.41.141.141:9000/weblog/fea0e0402a73479ab1603148b4187027.jpg","title":"中间件","summary":"后端开发中间件","isTop":false,"firstArticleId":39},{"id":5,"cover":"http://110.41.141.141:9000/weblog/weblog/ea44696cf3dd408584d04a46452bfbd6.jpg","title":"计算机基础","summary":"计算机基础知识","isTop":false,"firstArticleId":54},{"id":4,"cover":"http://110.41.141.141:9000/weblog/weblog/80061883b8514ae3a557fe8d7db8f6b2.jpg","title":"项目","summary":"项目整理","isTop":false,"firstArticleId":43},{"id":3,"cover":"http://110.41.141.141:9000/weblog/weblog/fb84627b093e4fa98ef9bdf5fbfe8cbc.jpg","title":"系统设计","summary":"系统设计","isTop":false,"firstArticleId":40},{"id":2,"cover":"http://110.41.141.141:9000/weblog/weblog/ca753e837e2344319d9f58a92d25403f.jpg","title":"Java","summary":"Java后端开发","isTop":false,"firstArticleId":33},{"id":1,"cover":"http://110.41.141.141:9000/weblog/weblog/2b484d7d659f4a3aaa9ba74271e55fda.jpg","title":"数据库","summary":"数据库，包括关系型数据库、非关系型数据库以及缓存等","isTop":false,"firstArticleId":47}]} =================================== 
2024-08-04 17:10:50.285 [http-nio-8088-exec-8] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [获取知识库数据], 耗时: 1610ms, 出参: {"success":true,"message":null,"errorCode":null,"data":[{"id":6,"cover":"http://110.41.141.141:9000/weblog/fea0e0402a73479ab1603148b4187027.jpg","title":"中间件","summary":"后端开发中间件","isTop":false,"firstArticleId":39},{"id":5,"cover":"http://110.41.141.141:9000/weblog/weblog/ea44696cf3dd408584d04a46452bfbd6.jpg","title":"计算机基础","summary":"计算机基础知识","isTop":false,"firstArticleId":54},{"id":4,"cover":"http://110.41.141.141:9000/weblog/weblog/80061883b8514ae3a557fe8d7db8f6b2.jpg","title":"项目","summary":"项目整理","isTop":false,"firstArticleId":43},{"id":3,"cover":"http://110.41.141.141:9000/weblog/weblog/fb84627b093e4fa98ef9bdf5fbfe8cbc.jpg","title":"系统设计","summary":"系统设计","isTop":false,"firstArticleId":40},{"id":2,"cover":"http://110.41.141.141:9000/weblog/weblog/ca753e837e2344319d9f58a92d25403f.jpg","title":"Java","summary":"Java后端开发","isTop":false,"firstArticleId":33},{"id":1,"cover":"http://110.41.141.141:9000/weblog/weblog/2b484d7d659f4a3aaa9ba74271e55fda.jpg","title":"数据库","summary":"数据库，包括关系型数据库、非关系型数据库以及缓存等","isTop":false,"firstArticleId":47}]} =================================== 
2024-08-04 17:10:50.363 [http-nio-8088-exec-10] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 17:10:50.379 [http-nio-8088-exec-8] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 17:10:50.533 [http-nio-8088-exec-1] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 17:10:52.520 [http-nio-8088-exec-7] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 17:10:52.521 [http-nio-8088-exec-7] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 17:10:52.521 [http-nio-8088-exec-7] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [前台获取博客详情], 入参: , 请求类: BlogSettingsController, 请求方法: findDetail =================================== 
2024-08-04 17:10:52.526 [http-nio-8088-exec-7] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [前台获取博客详情], 耗时: 5ms, 出参: {"success":true,"message":null,"errorCode":null,"data":{"logo":"http://110.41.141.141:9000/weblog/weblog/9853f8be13cb4f7fae00e3f5233dd688.png","name":"WebLog","author":"Jacob","introduction":"求知若饥，虚心若愚","avatar":"http://110.41.141.141:9000/weblog/weblog/cf0958d87787449fb05aae4cc84015c6.jpg","githubHomepage":"https://github.com/jdw-art","csdnHomepage":"https://www.csdn.net/?spm=1010.2135.3001.4476","giteeHomepage":"","zhihuHomepage":"https://www.zhihu.com/people/54-10-50-93"}} =================================== 
2024-08-04 17:10:52.555 [http-nio-8088-exec-9] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 17:10:52.555 [http-nio-8088-exec-9] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 17:10:52.577 [http-nio-8088-exec-6] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 17:10:52.577 [http-nio-8088-exec-6] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 17:10:52.578 [http-nio-8088-exec-3] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 17:10:52.578 [http-nio-8088-exec-3] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 17:10:52.537 [http-nio-8088-exec-7] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 17:10:54.052 [http-nio-8088-exec-3] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [获取知识库文章上下页], 入参: {"id":1,"articleId":47}, 请求类: WikiController, 请求方法: findArticlePreNext =================================== 
2024-08-04 17:10:54.066 [http-nio-8088-exec-6] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [获取文章详情], 入参: {"articleId":47}, 请求类: ArticleController, 请求方法: findArticleDetail =================================== 
2024-08-04 17:10:54.887 [http-nio-8088-exec-3] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [获取知识库文章上下页], 耗时: 657ms, 出参: {"success":true,"message":null,"errorCode":null,"data":{"preArticle":null,"nextArticle":{"articleId":42,"articleTitle":"MongoDB"}}} =================================== 
2024-08-04 17:10:54.888 [http-nio-8088-exec-3] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 17:10:56.975 [http-nio-8088-exec-9] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [获取知识库目录数据], 入参: {"id":1}, 请求类: WikiController, 请求方法: findWikiCatalogList =================================== 
2024-08-04 17:10:57.290 [http-nio-8088-exec-9] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [获取知识库目录数据], 耗时: 260ms, 出参: {"success":true,"message":null,"errorCode":null,"data":[{"id":89,"articleId":null,"title":"关系型数据库","level":1,"children":[{"id":90,"articleId":47,"title":"MySQL","level":2,"children":null}]},{"id":91,"articleId":null,"title":"非关系型数据库","level":1,"children":[{"id":92,"articleId":42,"title":"MongoDB","level":2,"children":null}]},{"id":93,"articleId":null,"title":"缓存","level":1,"children":[{"id":94,"articleId":45,"title":"Redis","level":2,"children":null},{"id":95,"articleId":51,"title":"RedisIncrement","level":2,"children":null}]}]} =================================== 
2024-08-04 17:10:57.291 [http-nio-8088-exec-9] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 17:10:58.353 [http-nio-8088-exec-6] INFO  c.j.weblog.event.subscriber.ReadArticleSubscriber - ==> threadName: http-nio-8088-exec-6
2024-08-04 17:10:58.360 [http-nio-8088-exec-6] INFO  c.j.weblog.event.subscriber.ReadArticleSubscriber - ==> 文章阅读事件消费成功，articleId: 47
2024-08-04 17:10:58.849 [http-nio-8088-exec-6] INFO  c.j.weblog.event.subscriber.ReadArticleSubscriber - ==> 文章阅读量 +1 操作成功，articleId: 47
2024-08-04 17:10:59.282 [http-nio-8088-exec-6] INFO  c.j.weblog.event.subscriber.ReadArticleSubscriber - ==> 当日文章 PV 访问量 +1 操作成功，date: 2024-08-04
2024-08-04 17:10:59.334 [http-nio-8088-exec-6] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [获取文章详情], 耗时: 5217ms, 出参: {"success":true,"message":null,"errorCode":null,"data":{"title":"MySQL","content":"<h2 id=\"1事务的四大特性\">1、事务的四大特性</h2>\n<p>事务特性ACID：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability）。</p>\n<ul>\n<li><strong>原子性</strong>：事务包含的所有操作要么全部成功，要么全部失败回滚。</li>\n<li><strong>一致性</strong>：一个事务在执行前和执行后都必须处于一致性状态。如a和b账户共1000块，两人之间转账无论成功还是失败之后，两个账户的总和都必须是1000块。</li>\n<li><strong>隔离性</strong>：和隔离级别相关，如<code>read committed</code>，<strong>一个事务只能读取到已经提交的修改</strong>。</li>\n<li><strong>持久性</strong>：一个事务一旦被提交了，那么对于数据库中的数据的改变将是永久性的，即使在数据库系统遇到故障的情况下也不会丢失提交事务的操作。</li>\n</ul>\n<h2 id=\"2数据库的三大范式\">2、数据库的三大范式</h2>\n<ol>\n<li>\n<p><strong>第一范式1NF</strong></p>\n<p><strong>确保数据库字段的原子性。</strong> 数据库中的字段不可再分</p>\n<p>比如字段 userInfo : 广东省 10086' ，依照第一范式必须拆分成 userInfo : 广东省 userTel : 10086\n两个字段。</p>\n</li>\n<li>\n<p><strong>第二范式2NF</strong></p>\n<p><strong>首先要满足第一范式，另外还要包含两个内容，一是表必须有主键，二是非主键必须完全依赖于主键，而不能依赖于主键的一部分。</strong></p>\n<p>举个例子。假定选课关系表为student_course (student_no, student_name, age, course_name,\ngrade, credit)，主键为(student_no, course_name)。其中学分完全依赖于课程名称，姓名年龄完全依\n赖学号，不符合第二范式，会导致数据冗余（学生选n门课，姓名年龄有n条记录）、插入异常（插入一\n门新课，因为没有学号，无法保存新课记录）等问题。</p>\n<p>应该拆分成三个表：学生： student (stuent_no, student_name, 年龄)；课程：\ncourse (course_name, credit)；选课关系： student_course_relation (student_no, course_name,\ngrade)。</p>\n</li>\n<li>\n<p><strong>第三范式3NF</strong></p>\n<p><strong>首先要满足第二范式，另外非主键列必须直接依赖于主键，不能存在传递依赖。即不能存在：非主键列A依赖于非主键列B，非主键列B依赖于主键的情况。</strong></p>\n<p>假定学生关系表为Student(student_no, student_name, age, academy_id, academy_telephone)，主\n键为&quot;学号&quot;，其中学院id依赖于学号，而学院地点和学院电话依赖于学院id，存在传递依赖，不符合第三\n范式。</p>\n<p>可以把学生关系表分为如下两个表：学生：(student_no, student_name, age, academy_id)；学院：\n(academy_id, academy_telephone)。</p>\n</li>\n</ol>\n<p><strong>2NF和3NF的区别：</strong></p>\n<ul>\n<li>2NF依据是非主键列是否完全依赖主键，还是依赖于主键的一部分。</li>\n<li>3NF依据是非主键列是直接依赖于主键，还是直接依赖于非主键。</li>\n</ul>\n<p><strong>第一范式：<strong>1NF是对属性的</strong>原子性约束</strong>，要求属性具有原子性，不可再分解；\n<strong>第二范式：<strong>2NF是对记录的</strong>惟一性约束</strong>，要求记录有惟一标识，即实体的惟一性；\n<strong>第三范式：<strong>3NF是对</strong>字段冗余性的约束</strong>，即任何字段不能由其他字段派生出来，它要求字段没有冗余。</p>\n<blockquote>\n<p>没有冗余的数据库设计可以做到。但是，没有冗余的数据库未必是最好的数据库，有时为了提高运行效率，就必须降低范式标准，适当保留冗余数据。具体做法是：在概念数据模型设计时遵守第三范式，降低范式标准的工作放到物理数据模型设计时考虑。降低范式就是增加字段，允许冗余。</p>\n</blockquote>\n<h2 id=\"3事务隔离的级别有哪些\">3、事务隔离的级别有哪些</h2>\n<p>先解释一下脏读、不可重复读和幻读的概念：</p>\n<ul>\n<li><strong>脏读：</strong> 在一个事务处理过程中读取了另一个未提交事务中的数据。</li>\n<li><strong>不可重复读：</strong> 对于数据库中的某行记录，一个事务范围内多次查询却返回了不同的数据值，这是由于在查询间隔，另一个事务修改了数据并提交了。</li>\n<li><strong>幻读：</strong> 当某个事务在读取某个范围内的记录时，另一个事务又在该范围内插入了新的记录。对幻读的正确理解是，一个事务内的读取操作的结论不足以支撑之后业务的执行。假设事务要新增一条记录，主键为id，在新增之前执行了select，发现没有id为xxx的记录，但插入时却出现了主键冲突，这就属于幻读，读取不到的记录却发生了主键冲突，是因为记录实际上已经被其他的事务插入了，但当前事务不可见。</li>\n</ul>\n<p>不可重复读和脏读的区别：<strong>脏读时某一事务读取了另一个事务未提交的脏数据，而不可重复读则是读取了前一个事务提交的数据</strong>。</p>\n<p>事务隔离就是为了解决上面提到的脏读、不可重复读、幻读这几个问题。</p>\n<p>MySQL数据库提供了四种隔离级别：</p>\n<ul>\n<li><strong>Serializable（串行化）</strong>：通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。</li>\n<li><strong>Repeatable read（可重复读）</strong>：<strong>MySQL的默认事务隔离级别，它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行，解决了不可重复读的问题</strong>。</li>\n<li><strong>Read committed（读已提交）</strong>：一个事务只能看见已经提交事务所作的改变，可避免脏读的发生。</li>\n<li><strong>Read uncommitted（读未提交）</strong>：所有事务都可以看见其他未提交事务的执行结果。</li>\n</ul>\n<p><strong>查看隔离级别：</strong></p>\n<pre><code class=\"language-sq\">select @@transaction_isolation;\n</code></pre>\n<p><strong>设置隔离级别：</strong></p>\n<pre><code class=\"language-sql\">set session transaction isolation level read uncommitted;\n</code></pre>\n<h2 id=\"4生产环境数据库一般用的隔离级别\">4、生产环境数据库一般用的隔离级别</h2>\n<p>生产环境一般使用读已提交（RC）隔离级别。为什么不使用可重复读（RR）隔离级别？</p>\n<blockquote>\n<p>可重复读(Repeatable Read)，简称为RR 读已提交(Read Commited)，简称为RC</p>\n</blockquote>\n<ul>\n<li>在可重复读（RR）隔离级别下，存在<strong>间隙锁</strong>，导致出现死锁的几率比读已提交（RC）大得多。</li>\n<li>在可重复读（RR）隔离级别下，<strong>条件列未命中索引会锁表</strong>，而在读已提交（RC）隔离级别下，只锁行。也就是说，读以提交的并发性要比可重复读高。</li>\n<li>并且大部分场景下，不可重复读是可以接受的。毕竟数据都已经提交了，读出来本身没有太大问题。</li>\n</ul>\n<h2 id=\"5编码和字符集的关系\">5、编码和字符集的关系</h2>\n<p>我们平时可以在编辑器上输入各种中文英文字母，但这些都是给人读的，不是给计算机读的，其实<strong>计算机真正保存和传输数据都是以二进制0101的格式进行的</strong>。</p>\n<p>那么就需要有一个规则，把中文和英文字母转化为二进制。其中d对应十六进制下的64，它可以转换为01二进制的格式。于是字母和数字就这样一一对应起来了，这就是ASCII编码格式。</p>\n<p>它用<strong>一个字节</strong>，也就是<code>8位</code>来标识字符，基础符号有128个，扩展符号也是128个。也就只能表示下<strong>英文字母和数字</strong>。</p>\n<p>这明显不够用。于是，为了标识<strong>中文</strong>，出现了<strong>GB2312</strong>的编码格式。为了标识<strong>希腊语</strong>，出现了<strong>greek</strong>编码格式，为了标识<strong>俄语</strong>，整了<strong>cp866</strong>编码格式。</p>\n<p>为了统一它们，于是出现了<strong>Unicode编码格式</strong>，它用了2~4个字节来表示字符，这样理论上所有符号都能被收录进去，并且它还完全兼容ASCII的编码，也就是说，同样是字母d，在ASCII用64表示，在Unicode里还是用64来表示。</p>\n<p>但<strong>不同的地方是ASCII编码用1个字节来表示，而Unicode用则两个字节来表示。</strong></p>\n<p>同样都是字母d，unicode比ascii多使用了一个字节，如下：</p>\n<pre><code class=\"language-\\\">D   ASCII:           01100100\nD Unicode:  00000000 01100100\n</code></pre>\n<p>可以看到，上面的unicode编码，前面的都是0，其实用不上，但还占了个字节，有点浪费。如果我们能做到该隐藏时隐藏，这样就能省下不少空间，按这个思路，就是就有了<strong>UTF-8编码</strong>。</p>\n<p>总结一下，<strong>按照一定规则把符号和二进制码对应起来，这就是编码。而把n多这种已经编码的字符聚在一起，就是我们常说的字符集</strong>。</p>\n<p>比如utf-8字符集就是所有utf-8编码格式的字符的合集。</p>\n<p>想看下mysql支持哪些字符集。可以执行 <code>show charset;</code></p>\n<h2 id=\"6utf8和utf8mb4的区别\">6、utf8和utf8mb4的区别</h2>\n<p>上面提到utf-8是在unicode的基础上做的优化，既然unicode有办法表示所有字符，那utf-8也一样可以表示所有字符，为了避免混淆，我在后面叫它<strong>大utf8</strong>。</p>\n<p>mysql支持的字符集中有utf8和utf8mb4。</p>\n<p>先说<strong>utf8mb4</strong>编码，mb4就是<strong>most bytes 4</strong>的意思，从上图最右边的<code>Maxlen</code>可以看到，它最大支持用<strong>4个字节</strong>来表示字符，它几乎可以用来表示目前已知的所有的字符。</p>\n<p>再说mysql字符集里的<strong>utf8</strong>，它是数据库的<strong>默认字符集</strong>。但注意，<strong>此utf8非彼utf8</strong>，我们叫它<strong>小utf8</strong>字符集。为什么这么说，因为从Maxlen可以看出，它最多支持用3个字节去表示字符，按utf8mb4的命名方式，准确点应该叫它<strong>utf8mb3</strong>。</p>\n<p>utf8 就像是阉割版的utf8mb4，只支持部分字符。比如<code>emoji</code>表情，它就不支持。</p>\n<p>而mysql支持的字符集里，第三列，<strong>collation</strong>，它是指<strong>字符集的比较规则</strong>。</p>\n<p>比如，&quot;debug&quot;和&quot;Debug&quot;是同一个单词，但它们大小写不同，该不该判为同一个单词呢。</p>\n<p>这时候就需要用到collation了。</p>\n<p>通过<code>SHOW COLLATION WHERE Charset = 'utf8mb4';</code>可以查看到<code>utf8mb4</code>下支持什么比较规则。</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/e89f7eae13cd4f3a933a6dffd7b73bdb.png\">\n</p>\n<p>如果<code>collation = utf8mb4_general_ci</code>，是指使用utf8mb4字符集的前提下，<strong>挨个字符进行比较</strong>（<code>general</code>），并且不区分大小写（<code>_ci，case insensitice</code>）。</p>\n<p>这种情况下，&quot;debug&quot;和&quot;Debug&quot;是同一个单词。</p>\n<p>如果改成<code>collation=utf8mb4_bin</code>，就是指<strong>挨个比较二进制位大小</strong>。</p>\n<p>于是&quot;debug&quot;和&quot;Debug&quot;就不是同一个单词。</p>\n<p><strong>那utf8mb4对比utf8有什么劣势吗？</strong></p>\n<p>我们知道数据库表里，字段类型如果是<code>char(2)</code>的话，里面的<code>2</code>是指<strong>字符个数</strong>，也就是说<strong>不管这张表用的是什么编码的字符集</strong>，都能放上2个字符。</p>\n<p>而char又是<strong>固定长度</strong>，为了能放下2个utf8mb4的字符，char会默认保留<code>2*4（maxlen=4）= 8</code>个字节的空间。</p>\n<p>如果是utf8mb3，则会默认保留 <code>2 * 3 (maxlen=3) = 6</code>个字节的空间。也就是说，在这种情况下，<strong>utf8mb4会比utf8mb3多使用一些空间。</strong></p>\n<p><strong>对于固定长度的char类型的字符串，utf8mb4会比utf8多使用一些空间</strong>。</p>\n<h2 id=\"7索引\">7、索引</h2>\n<h3 id=\"71索引的定义\">7.1、索引的定义</h3>\n<p>索引是存储引擎用于提高数据库表的访问速度的一种数据结构。</p>\n<h3 id=\"72索引的优缺点\">7.2、索引的优缺点</h3>\n<p><strong>优点：</strong></p>\n<ul>\n<li><strong>加快数据查找速度。</strong></li>\n<li>为用来排序或者是分组的字段添加索引，可以<strong>加快分组和排序的速度。</strong></li>\n<li><strong>加快表与表之间的连接。</strong></li>\n</ul>\n<p><strong>缺点：</strong></p>\n<ul>\n<li>建立索引需要<strong>占用物理空间</strong>。</li>\n<li>会<strong>降低表的增删改的效率</strong>，因为每次对表记录进行增删改，需要进行动态维护索引，导致增删改的时间变长。</li>\n</ul>\n<h3 id=\"73索引的作用\">7.3、索引的作用</h3>\n<p>数据是存储在磁盘上的，查询数据时，如果没有索引，就需要加载所有的数据到内存上，一次进行检索，读取磁盘次数较多。有了索引，就不需要加载全部数据了，因为B+树的高度一般在2-4层，最多只需要读取2-4次磁盘，查询速度大大提升。</p>\n<h3 id=\"74什么情况下需要建立索引\">7.4、什么情况下需要建立索引</h3>\n<ol>\n<li>经常用于<strong>查询</strong>的字段</li>\n<li>经常用于<strong>连接</strong>的字段建立索引，可以加快连接的速度。</li>\n<li>经常需要<strong>排序</strong>的字段建立索引，因为索引已经排好序，可以加快排序查询速度。</li>\n</ol>\n<h3 id=\"75什么情况下不建立索引\">7.5、什么情况下不建立索引</h3>\n<ol>\n<li><strong>where条件中用不到的字段</strong>不适合建立索引。</li>\n<li><strong>表记录较少</strong>。比如只有几百条数据，没必要加索引。</li>\n<li>需要<strong>经常增删改</strong>，需要评估是否适合加索引。</li>\n<li><strong>参与列计算的列</strong>不适合建立索引。</li>\n<li><strong>区分度不高的字段</strong>不适合建立索引，如性别，只有男/女/未知三个值。加了索引，查询效率也不会提高。</li>\n</ol>\n<h3 id=\"76索引的数据结构\">7.6、索引的数据结构</h3>\n<p>索引的数据结构主要有B+树和哈希表，对应的索引分别是<strong>B+树索引和哈希索引</strong>。InnoDB引擎的索引类型有B+树索引和哈希索引，默认的索引类型为B+树索引。</p>\n<p><strong>B+树索引</strong></p>\n<p>B+ 树是基于B 树和叶子节点顺序访问指针进行实现，它具有B树的平衡性，并且通过顺序访问指针来提高区间查询的性能。</p>\n<p>在 B+ 树中，节点中的 <code>key</code> 从左到右递增排列，如果某个指针的左右相邻 <code>key</code> 分别是 keyi 和 keyi+1，则该指针指向节点的所有 <code>key</code> 大于等于 keyi 且小于等于 keyi+1。</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/2d2a398bc5e94876a66d4e8359570895.png\">\n</p>\n<p>进行查找操作时，首先在根节点进行二分查找，找到key 所在的指针，然后递归地在指针所指向的节点进行查找。直到查找到叶子节点，然后在叶子节点上进行二分查找，找出key 所对应的数据项。</p>\n<p>MySQL 数据库使用最多的索引类型是BTREE 索引，底层基于B+树数据结构来实现。</p>\n<pre><code class=\"language-cmd\">mysql&gt; show index from blog\\G;\n*************************** 1. row ***************************\n        Table: blog\n   Non_unique: 0\n     Key_name: PRIMARY\n Seq_in_index: 1\n  Column_name: blog_id\n    Collation: A\n  Cardinality: 4\n     Sub_part: NULL\n       Packed: NULL\n         Null:\n   Index_type: BTREE\n      Comment:\nIndex_comment:\n      Visible: YES\n   Expression: NULL\n\n</code></pre>\n<p><strong>哈希索引</strong></p>\n<p>哈希索引是基于哈希表实现的，对于每一行数据，存储引擎会对索引列进行哈希计算得到哈希码，并且哈希算法要尽量保证不同的列值计算出的哈希码值是不同的，将哈希码的值作为哈希表的key值，将指向数据行的指针作为哈希表的value值。这样查找一个数据的时间复杂度就是O(1)，一般多用于精确查找。</p>\n<h3 id=\"77hash索引和b树索引的区别\">7.7、Hash索引和B+树索引的区别</h3>\n<ul>\n<li>哈希索引<strong>不支持排序</strong>，因为哈希表是无序的。</li>\n<li>哈希索引<strong>不支持范围查找</strong>。</li>\n<li>哈希索引<strong>不支持模糊查询</strong>以及多列索引的最左前缀匹配。</li>\n<li>因为哈希表会存在哈希冲突，所以<strong>哈希索引的性能是不稳定的，而B+树索引的性能相对稳定</strong>，每次查询都是从根节点到叶子节点。</li>\n</ul>\n<h3 id=\"78为什么b树比b树更适合实现数据库索引\">7.8、为什么B+树比B树更适合实现数据库索引</h3>\n<ul>\n<li>由于<strong>B+树的数据都存储在叶子节点中</strong>，叶子节点均为索引，方便扫库，只需要扫一遍叶子节点即可，但是B树因为其分支同样存储着数据，我们要找到具体的数据，需要进行一次中序遍历按序来扫，所以B+树更适合在区间查询的情况，而在数据库中基于范围的查询是非常频繁的，所以通常使用B+树用于数据库索引。</li>\n<li><strong>B+树的节点只存储索引key值，具体信息的地址存在于叶子节点的地址中</strong>。这就使<strong>以页为单位的索引中可以存放更多的节点</strong>。减少更多的I/O支出。</li>\n<li><strong>B+树的查询效率更加稳定，任何关键字的查找必须走一条从根结点到叶子结点的路</strong>。所有关键字查询的路径长度相同，导致每一个数据的查询效率相当。</li>\n</ul>\n<hr />\n<ul>\n<li>B+ 树的非叶子节点不存放实际的记录数据，仅存放索引，因此数据量相同的情况下，相比存储即存索引又存记录的 B 树，B+树的非叶子节点可以存放更多的索引，因此 B+ 树可以比 B 树更「矮胖」，查询底层节点的磁盘 I/O次数会更少。</li>\n<li>B+ 树有大量的冗余节点（所有非叶子节点都是冗余索引），这些冗余索引让 B+ 树在插入、删除的效率都更高，比如删除根节点的时候，不会像 B 树那样会发生复杂的树的变化；</li>\n<li>B+ 树叶子节点之间用链表连接了起来，有利于范围查询，而 B 树要实现范围查询，因此只能通过树的遍历来完成范围查询，这会涉及多个节点的磁盘 I/O 操作，范围查询效率不如 B+ 树。</li>\n</ul>\n<h3 id=\"79索引的分类\">7.9、索引的分类</h3>\n<ol>\n<li>\n<p><strong>主键索引</strong>：名为primary的唯一非空索引，不允许有空值。</p>\n</li>\n<li>\n<p><strong>唯一索引</strong>：索引列的值必须是唯一的，允许有空值。主键索引和唯一索引的区别在于：唯一索引字段可以为null且可以存在多个null值，而主键索引字段不可以为null。唯一索引的用途：唯一标识数据库中的每条记录，主要用来防止重复数据插入。创建唯一索引的SQL语句为：</p>\n<pre><code class=\"language-sql\">ALTER TABLE table_name\nADD CONSTRAINT constraint_name UNIQUE KEY(column_1,column_2,...);\n</code></pre>\n</li>\n<li>\n<p><strong>组合索引</strong>：在表中的多个字段组合上创建的索引，只有在查询条件中使用了这些字段的左边字段时，索引才会被使用，使用组合索引时需遵循最左前缀原则。</p>\n</li>\n<li>\n<p><strong>全文索引</strong>：只能在CHAR 、VARCHAR 和TEXT 类型字段上使用全文索引。</p>\n</li>\n<li>\n<p><strong>普通索引</strong>：普通索引是最基本的索引，它没有任何限制，值可以为空。</p>\n</li>\n</ol>\n<h3 id=\"710什么是最左匹配原则\">7.10、什么是最左匹配原则</h3>\n<p><strong>如果 SQL 语句中用到了组合索引中的最左边的索引，那么这条 SQL 语句就可以利用这个组合索引去进行匹配。当遇到范围查询(<code>&gt;</code>、<code>&lt;</code>、<code>between</code>、<code>like</code>)就会停止匹配，后面的字段不会用到索引</strong>。</p>\n<p>对<code>(a,b,c)</code>建立索引，查询条件使用 a/ab/abc 会走索引，使用 bc 不会走索引。</p>\n<p>对<code>(a,b,c,d)</code>建立索引，查询条件为<code>a = 1 and b = 2 and c &gt; 3 and d = 4</code>，那么a、b和c三个字段能用到索引，而d无法使用索引。因为遇到了范围查询。</p>\n<p>如下图，对(a, b) 建立索引，a 在索引树中是全局有序的，而 b 是全局无序，局部有序（当a相等时，会根据b进行排序）。直接执行<code>b = 2</code>这种查询条件无法使用索引。</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/fe96a1d64f5a4eaf9181803fe8fc1294.png\">\n</p>\n<p>当a的值确定的时候，b是有序的。例如<code>a = 1</code>时，b值为1，2是有序的状态。当<code>a = 2</code>时候，b的值为1，4也是有序状态。 当执行<code>a = 1 and b = 2</code>时a和b字段能用到索引。而执行<code>a &gt; 1 and b = 2</code>时，a字段能用到索引，b字段用不到索引。因为a的值此时是一个范围，不是固定的，在这个范围内b值不是有序的，因此b字段无法使用索引。</p>\n<h3 id=\"711什么是聚集索引\">7.11、什么是聚集索引</h3>\n<p>InnoDB使用<strong>表的主键构造主键索引树</strong>，同时叶子节点中存放的即为整张表的记录数据。聚集索引叶子节点的存储是逻辑上连续的，使用双向链表连接，叶子节点按照主键的顺序排序，因此对于主键的排序查找和范围查找速度比较快。如果表中没有显示指定主键，则会<strong>选择表中的第一个不允许为NULL 的唯一索引</strong>。如果没有主键也没有合适的唯一索引，那么InnoDB 内部会生成一个隐藏的主键作为聚集索引，这个隐藏的主键长度为6个字节，它的值会随着数据的插入自增。</p>\n<h3 id=\"712什么是覆盖索引\">7.12、什么是覆盖索引</h3>\n<p><em>InnoDB 的数据是按「数据页」为单位来读写的，默认数据页大小为 16 KB。每个数据页之间通过双向链表的形式组织起来，物理上不连续，但是逻辑上连续。</em></p>\n<p><em>数据页内包含用户记录，每个记录之间用单向链表的方式组织起来，为了加快在数据页内高效查询记录，设计了一个页目录，页目录存储各个槽（分组），且主键值是有序的，于是可以通过二分查找法的方式进行检索从而提高效率。</em></p>\n<p><em>为了高效查询记录所在的数据页，InnoDB 采用 b+ 树作为索引，每个节点都是一个数据页。</em></p>\n<p><em>如果叶子节点存储的是实际数据的就是聚簇索引，一个表只能有一个聚簇索引；如果叶子节点存储的不是实际数据，而是主键值则就是二级索引，一个表中可以有多个二级索引。</em></p>\n<p><em>在使用二级索引进行查找数据时，如果查询的数据能在二级索引找到，那么就是「索引覆盖」操作，如果查询的数据不在二级索引里，就需要先在二级索引找到主键值，需要去聚簇索引中获得数据行，这个过程就叫作「回表」。</em></p>\n<p><strong>覆盖索引（covering index ，或称为索引覆盖）即从非主键索引中就能查到的记录，而不需要查询主键索引中的记录，避免了回表的产生减少了树的搜索次数，显著提升性能</strong>。不是所有类型的索引都可以成为覆盖索引。覆盖索引要存储索引列的值，而哈希索引、全文索引不存储索引列的值，所以<strong>MySQL使用b+树索引做覆盖索引</strong>。</p>\n<p>对于使用了覆盖索引的查询，在查询前面使用<code>explain</code>，输出的extra列会显示为<code>using index</code>。</p>\n<p>比如<code>user_like</code> 用户点赞表，组合索引为<code>(user_id, blog_id)</code>，<code>user_id</code>和<code>blog_id</code>都不为<code>null</code>。</p>\n<pre><code class=\"language-sql\">explain select blog_id from user_like where user_id = 13;\n</code></pre>\n<p><code>explain</code>结果的<code>Extra</code>列为<code>Using index</code>，查询的列被索引覆盖，并且where筛选条件符合最左前缀原则，通过<strong>索引查找</strong>就能直接找到符合条件的数据，不需要回表查询数据。</p>\n<pre><code class=\"language-sql\">explain select user_id from user_like where blog_id = 1;\n</code></pre>\n<p><code>explain</code>结果的<code>Extra</code>列为<code>Using where; Using index</code>， 查询的列被索引覆盖，where筛选条件不符合最左前缀原则，无法通过索引查找找到符合条件的数据，但可以通过<strong>索引扫描</strong>找到符合条件的数据，也不需要回表查询数据。</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/01d86030cd5e4303bb22af9d6167c413.png\">\n</p>\n<blockquote>\n<p><em>如果某个查询语句使用了二级索引，但是查询的数据不是主键值，这时在二级索引找到主键值后，需要去聚簇索引中获得数据行，这个过程就叫作「回表」，也就是说要查两个 B+ 树才能查到数据。不过，当查询的数据是主键值时，因为只在二级索引就能查询到，不用再去聚簇索引查，这个过程就叫作「索引覆盖」，也就是只需要查一个 B+ 树就能找到数据。</em></p>\n</blockquote>\n<h3 id=\"713索引的设计原则\">7.13、索引的设计原则</h3>\n<ul>\n<li>\n<p>对于经常作为查询条件的字段，应该建立索引，以提高查询速度</p>\n</li>\n<li>\n<p>为经常需要排序、分组和联合操作的字段建立索引</p>\n</li>\n<li>\n<p>索引列的<strong>区分度越高</strong>，索引的效果越好。比如使用性别这种区分度很低的列作为索引，效果就会很差。</p>\n</li>\n<li>\n<p>避免给&quot;大字段&quot;建立索引。尽量使用数据量小的字段作为索引。因为<code>MySQL</code>在维护索引的时候是会将字段值一起维护的，那这样必然会导致索引占用更多的空间，另外在排序的时候需要花费更多的时间去对比。</p>\n</li>\n<li>\n<p>尽量使用<strong>短索引</strong>，对于较长的字符串进行索引时应该指定一个较短的前缀长度，因为较小的索引涉及到的磁盘I/O较少，查询速度更快。</p>\n</li>\n<li>\n<p>索引不是越多越好，每个索引都需要额外的物理空间，维护也需要花费时间。</p>\n</li>\n<li>\n<p>频繁增删改的字段不要建立索引。假设某个字段频繁修改，那就意味着需要频繁的重建索引，这必然影响MySQL的性能</p>\n</li>\n<li>\n<p>利用<strong>最左前缀原则</strong>。</p>\n</li>\n<li>\n<p>自增主键可以让主键索引尽量地保持递增顺序插入，避免了页分裂，因此索引更紧凑，在查询的时候，效率也就更高。</p>\n</li>\n</ul>\n<h3 id=\"714索引什么时候会失效\">7.14、索引什么时候会失效</h3>\n<p>导致索引失效的情况：</p>\n<ul>\n<li>对于组合索引，不是使用组合索引最左边的字段，则不会使用索引</li>\n<li>以%开头的like查询如<code>%abc</code>，无法使用索引；非%开头的like查询如<code>abc%</code>，相当于范围查询，会使用索引</li>\n<li>查询条件中列类型是字符串，没有使用引号，可能会因为类型不同发生隐式转换，使索引失效</li>\n<li>判断索引列是否不等于某个值时</li>\n<li>对索引列进行运算</li>\n<li>查询条件使用<code>or</code>连接，也会导致索引失效</li>\n</ul>\n<h3 id=\"715什么是前缀索引\">7.15、什么是前缀索引</h3>\n<p>有时需要在很长的字符列上创建索引，这会造成索引特别大且慢。使用前缀索引可以避免这个问题。</p>\n<p><strong>前缀索引是指对文本或者字符串的前几个字符建立索引</strong>，这样索引的长度更短，查询速度更快。</p>\n<p>创建前缀索引的关键在于选择足够长的前缀以<strong>保证较高的索引选择性</strong>。索引选择性越高查询效率就越高，因为选择性高的索引可以让MySQL在查找时过滤掉更多的数据行。</p>\n<p>建立前缀索引的方式：</p>\n<pre><code class=\"language-sql\">// email列创建前缀索引\nALTER TABLE table_name ADD KEY(column_name(prefix_length));\n</code></pre>\n<h2 id=\"8常见的存储引擎有哪些\">8、常见的存储引擎有哪些</h2>\n<p>MySQL中常用的四种存储引擎分别是： <strong>MyISAM</strong>、<strong>InnoDB</strong>、<strong>MEMORY</strong>、<strong>ARCHIVE</strong>。MySQL 5.5版本后默认的存储引擎为<code>InnoDB</code>。</p>\n<p><strong>InnoDB存储引擎</strong></p>\n<p>InnoDB是MySQL<strong>默认的事务型存储引擎</strong>，使用最广泛，基于聚簇索引建立的。InnoDB内部做了很多优化，如能够自动在内存中创建自适应hash索引，以加速读操作。</p>\n<p><strong>优点</strong>：支持事务和崩溃修复能力；引入了行级锁和外键约束。</p>\n<p><strong>缺点</strong>：占用的数据空间相对较大。</p>\n<p><strong>适用场景</strong>：需要事务支持，并且有较高的并发读写频率。</p>\n<p><strong>MyISAM存储引擎</strong></p>\n<p>数据以紧密格式存储。对于只读数据，或者表比较小、可以容忍修复操作，可以使用MyISAM引擎。MyISAM会将表存储在两个文件中，数据文件<code>.MYD</code>和索引文件<code>.MYI</code>。</p>\n<p><strong>优点</strong>：访问速度快。</p>\n<p><strong>缺点</strong>：MyISAM不支持事务和行级锁，不支持崩溃后的安全恢复，也不支持外键。</p>\n<p><strong>适用场景</strong>：对事务完整性没有要求；表的数据都是只读的。</p>\n<p><strong>MEMORY存储引擎</strong></p>\n<p>MEMORY引擎将数据全部放在内存中，访问速度较快，但是一旦系统奔溃的话，数据都会丢失。</p>\n<p>MEMORY引擎默认使用哈希索引，将键的哈希值和指向数据行的指针保存在哈希索引中。</p>\n<p><strong>优点</strong>：访问速度较快。</p>\n<p><strong>缺点</strong>：</p>\n<ol>\n<li>哈希索引数据不是按照索引值顺序存储，无法用于排序。</li>\n<li>不支持部分索引匹配查找，因为哈希索引是使用索引列的全部内容来计算哈希值的。</li>\n<li>只支持等值比较，不支持范围查询。</li>\n<li>当出现哈希冲突时，存储引擎需要遍历链表中所有的行指针，逐行进行比较，直到找到符合条件的行。</li>\n</ol>\n<p><strong>ARCHIVE存储引擎</strong></p>\n<p>ARCHIVE存储引擎非常适合存储大量独立的、作为历史记录的数据。ARCHIVE提供了压缩功能，拥有高效的插入速度，但是这种引擎不支持索引，所以查询性能较差。</p>\n<h2 id=\"9myisam和innodb的区别\">9、MyISAM和InnoDB的区别</h2>\n<ul>\n<li>\n<p><strong>存储结构的区别</strong>。每个MyISAM在磁盘上存储成三个文件。文件的名字以表的名字开始，扩展名指出文件类型。 .frm文件存储表定义。数据文件的扩展名为.MYD (MYData)。索引文件的扩展名是.MYI (MYIndex)。InnoDB所有的表都保存在同一个数据文件中（也可能是多个文件，或者是独立的表空间文件），InnoDB表的大小只受限于操作系统文件的大小，一般为2GB。</p>\n</li>\n<li>\n<p><strong>存储空间的区别</strong>。MyISAM支持支持三种不同的存储格式：静态表(默认，但是注意数据末尾不能有空格，会被去掉)、动态表、压缩表。当表在创建之后并导入数据之后，不会再进行修改操作，可以使用压缩表，极大的减少磁盘的空间占用。InnoDB需要更多的内存和存储，它会在主内存中建立其专用的缓冲池用于高速缓冲数据和索引。</p>\n</li>\n<li>\n<p><strong>可移植性、备份及恢复</strong>。MyISAM数据是以文件的形式存储，所以在跨平台的数据转移中会很方便。在备份和恢复时可单独针对某个表进行操作。对于InnoDB，可行的方案是拷贝数据文件、备份 binlog，或者用mysqldump，在数据量达到几十G的时候就相对麻烦了。</p>\n</li>\n<li>\n<p><strong>是否支持行级锁</strong>。MyISAM 只支持表级锁，用户在操作myisam表时，select，update，delete，insert语句都会给表自动加锁，如果加锁以后的表满足insert并发的情况下，可以在表的尾部插入新的数据。而InnoDB 支持行级锁和表级锁，默认为行级锁。行锁大幅度提高了多用户并发操作的性能。</p>\n</li>\n<li>\n<p><strong>是否支持事务和崩溃后的安全恢复</strong>。 MyISAM 不提供事务支持。而InnoDB 提供事务支持，具有事务、回滚和崩溃修复能力。</p>\n</li>\n<li>\n<p><strong>是否支持外键</strong>。MyISAM不支持，而InnoDB支持。</p>\n</li>\n<li>\n<p><strong>是否支持MVCC</strong>。MyISAM不支持，InnoDB支持。应对高并发事务，MVCC比单纯的加锁更高效。</p>\n</li>\n<li>\n<p><strong>是否支持聚集索引</strong>。MyISAM不支持聚集索引，InnoDB支持聚集索引。</p>\n</li>\n<li>\n<p><strong>全文索引</strong>。MyISAM支持 FULLTEXT类型的全文索引。InnoDB不支持FULLTEXT类型的全文索引，但是innodb可以使用sphinx插件支持全文索引，并且效果更好。</p>\n</li>\n<li>\n<p><strong>表主键</strong>。MyISAM允许没有任何索引和主键的表存在，索引都是保存行的地址。对于InnoDB，如果没有设定主键或者非空唯一索引，就会自动生成一个6字节的主键(用户不可见)。</p>\n</li>\n<li>\n<p><strong>表的行数</strong>。MyISAM保存有表的总行数，如果<code>select count(*) from table</code>;会直接取出该值。InnoDB没有保存表的总行数，如果使用select count(*) from table；就会遍历整个表，消耗相当大，但是在加了where条件后，MyISAM和InnoDB处理的方式都一样。</p>\n</li>\n</ul>\n<hr />\n<ul>\n<li>\n<p><strong>存储结构的区别</strong>。每个MyISAM在磁盘上存储成三个文件（索引文件、数据文件和表结构文件）。InnoDB所有的表都保存在同一个数据文件中（也可能是多个文件，或者是独立的表空间文件）。</p>\n</li>\n<li>\n<p><strong>存储空间的区别</strong>。MyISAM支持三种不同的存储格式：静态表、动态表、压缩表。当表在创建之后并导入数据之后，不会再进行修改操作，可以使用压缩表，极大的减少磁盘的空间占用。InnoDB需要更多的内存和存储，它会在主内存中建立其专用的缓冲池用于高速缓冲数据和索引。</p>\n</li>\n<li>\n<p><strong>可移植性、备份及恢复</strong>。MyISAM数据是以文件的形式存储，所以在跨平台的数据转移中会很方便。对于InnoDB，可行的方案是拷贝数据文件、备份 binlog，或者用mysqldump，在数据量达到几十G的时候就相对麻烦了。</p>\n</li>\n<li>\n<p><strong>是否支持行级锁</strong>。MyISAM 只支持表级锁，而InnoDB 支持行级锁和表级锁，默认为行级锁。行锁大幅度提高了多用户并发操作的性能。</p>\n</li>\n<li>\n<p><strong>是否支持事务和崩溃后的安全恢复</strong>。 MyISAM 不提供事务支持。而InnoDB 提供事务支持，具有事务、回滚和崩溃修复能力。</p>\n</li>\n<li>\n<p><strong>是否支持外键</strong>。MyISAM不支持，而InnoDB支持。</p>\n</li>\n<li>\n<p><strong>是否支持MVCC</strong>。MyISAM不支持，InnoDB支持。应对高并发事务，MVCC比单纯的加锁更高效。</p>\n</li>\n<li>\n<p><strong>是否支持聚集索引</strong>。MyISAM不支持聚集索引，InnoDB支持聚集索引。</p>\n</li>\n<li>\n<p><strong>全文索引</strong>。MyISAM支持 FULLTEXT类型的全文索引。InnoDB不支持FULLTEXT类型的全文索引，但是innodb可以使用sphinx插件支持全文索引，并且效果更好。</p>\n</li>\n<li>\n<p><strong>表主键</strong>。MyISAM允许没有任何索引和主键的表存在，索引都是保存行的地址。对于InnoDB，如果没有设定主键或者非空唯一索引，就会自动生成一个6字节的主键(用户不可见)。</p>\n</li>\n<li>\n<p><strong>表的行数</strong>。MyISAM保存有表的总行数，如果<code>select count(*) from table</code>;会直接取出该值。InnoDB没有保存表的总行数，如果使用select count(*) from table；就会遍历整个表，消耗相当大，但是在加了where条件后，MyISAM和InnoDB处理的方式都一样。</p>\n</li>\n</ul>\n<h2 id=\"10mysql有哪些锁\">10、MySQL有哪些锁</h2>\n<p><strong>按锁粒度分类</strong>，有行级锁、表级锁和页级锁。</p>\n<ol>\n<li>行级锁是mysql中锁的粒度最细的一种锁。表示只针对当前操作的行进行加锁。行级锁能大大减少数据库操作的冲突，其加锁粒度最小，但加锁的开销也最大。行级锁的类型主要有三类：\n<ul>\n<li>Record Lock，记录锁，也就是仅仅把一条记录锁上；</li>\n<li>Gap Lock，间隙锁，锁定一个范围，但是不包含记录本身；</li>\n<li>Next-Key Lock：Record Lock + Gap Lock 的组合，锁定一个范围，并且锁定记录本身。</li>\n</ul>\n</li>\n<li>表级锁是mysql中锁定粒度最大的一种锁，表示对当前操作的整张表加锁，它实现简单，资源消耗较少，被大部分mysql引擎支持。最常使用的MyISAM与InnoDB都支持表级锁定。</li>\n<li>页级锁是 MySQL 中锁定粒度介于行级锁和表级锁中间的一种锁。表级锁速度快，但冲突多，行级冲突少，但速度慢。因此，采取了折衷的页级锁，一次锁定相邻的一组记录。</li>\n</ol>\n<p><strong>按锁级别分类</strong>，有共享锁、排他锁和意向锁。</p>\n<ol>\n<li>共享锁又称读锁，是读取操作创建的锁。其他用户可以并发读取数据，但任何事务都不能对数据进行修改（获取数据上的排他锁），直到已释放所有共享锁。</li>\n<li>排他锁又称写锁、独占锁，如果事务T对数据A加上排他锁后，则其他事务不能再对A加任何类型的封锁。获准排他锁的事务既能读数据，又能修改数据。</li>\n<li>意向锁是表级锁，其设计目的主要是为了在一个事务中揭示下一行将要被请求锁的类型。InnoDB 中的两个表锁：\n<ul>\n<li>意向共享锁（IS）：表示事务准备给数据行加入共享锁，也就是说一个数据行加共享锁前必须先取得该表的IS锁；</li>\n<li>意向排他锁（IX）：类似上面，表示事务准备给数据行加入排他锁，说明事务在一个数据行加排他锁前必须先取得该表的IX锁。</li>\n</ul>\n</li>\n</ol>\n<p>意向锁是 InnoDB 自动加的，不需要用户干预。</p>\n<p><strong>对于INSERT、UPDATE和DELETE，InnoDB 会自动给涉及的数据加排他锁；对于一般的SELECT语句，InnoDB 不会加任何锁</strong>，事务可以通过以下语句显式加共享锁或排他锁。</p>\n<p>共享锁：<code>SELECT … LOCK IN SHARE MODE;</code></p>\n<p>排他锁：<code>SELECT … FOR UPDATE;</code></p>\n<h2 id=\"11mvcc-实现原理\">11、MVCC 实现原理</h2>\n<p>我们需要了解两个知识：</p>\n<p>Read View 中四个字段作用；\n聚簇索引记录中两个跟事务有关的隐藏列；\n那 Read View 到底是个什么东西？</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/26e9aa37ce3440db94c8e15459fe2e53.png\">\n</p>\n<p>Read View 有四个重要的字段：</p>\n<ul>\n<li><em><strong>m_ids</strong></em> ：指的是在创建 Read View 时，当前数据库中「活跃事务」的事务 id 列表，注意是一个列表，“活跃事务”指的就是，启动了但还没提交的事务。</li>\n<li><em><strong>min_trx_id</strong></em> ：指的是在创建 Read View 时，当前数据库中「活跃事务」中事务 id 最小的事务，也就是 m_ids 的最小值。</li>\n<li><em><strong>max_trx_id</strong></em> ：这个并不是 m_ids 的最大值，而是创建 Read View 时当前数据库中应该给下一个事务的 id 值，也就是全局事务中最大的事务 id 值 + 1；</li>\n<li><em><strong>creator_trx_id</strong></em> ：指的是创建该 Read View 的事务的事务 id。\n知道了 Read View 的字段，我们还需要了解聚簇索引记录中的两个隐藏列。</li>\n</ul>\n<p>假设在账户余额表插入一条小林余额为 100 万的记录，然后我把这两个隐藏列也画出来，该记录的整个示意图如下：</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/30528a510be44807b7535d94eacd1304.png\">\n</p>\n<p>对于使用 InnoDB 存储引擎的数据库表，它的聚簇索引记录中都包含下面两个隐藏列：</p>\n<ul>\n<li><em><strong>trx_id</strong></em>，当一个事务对某条聚簇索引记录进行改动时，就会把该事务的事务 id 记录在 trx_id 隐藏列里；</li>\n<li><em><strong>roll_pointer</strong></em>，每次对某条聚簇索引记录进行改动时，都会把旧版本的记录写入到 undo 日志中，然后这个隐藏列是个指针，指向每一个旧版本记录，于是就可以通过它找到修改前的记录。\n在创建 Read View 后，我们可以将记录中的 trx_id 划分这三种情况：</li>\n</ul>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/8dc914e744dc4d9aa514ed1bd0fef737.png\">\n</p>\n<p>一个事务去访问记录的时候，除了自己的更新记录总是可见之外，还有这几种情况：</p>\n<ul>\n<li>如果记录的 trx_id 值小于 Read View 中的 min_trx_id 值，表示这个版本的记录是在创建 Read View 前已经提交的事务生成的，所以该版本的记录对当前事务可见。</li>\n<li>如果记录的 trx_id 值大于等于 Read View 中的 max_trx_id 值，表示这个版本的记录是在创建 Read View 后才启动的事务生成的，所以该版本的记录对当前事务不可见。</li>\n<li>如果记录的 trx_id 值在 Read View 的 min_trx_id 和 max_trx_id 之间，需要判断 trx_id 是否在 m_ids 列表中：\n<ul>\n<li>如果记录的 trx_id 在 m_ids 列表中，表示生成该版本记录的活跃事务依然活跃着（还没提交事务），所以该版本的记录对当前事务不可见。</li>\n<li>如果记录的 trx_id 不在 m_ids列表中，表示生成该版本记录的活跃事务已经被提交，所以该版本的记录对当前事务可见。</li>\n</ul>\n</li>\n</ul>\n<p><em><strong>这种通过「版本链」来控制并发事务访问同一个记录时的行为就叫 MVCC（多版本并发控制）</strong></em>。</p>\n<h2 id=\"12快照读和当前读\">12、快照读和当前读</h2>\n<p>表记录有两种读取方式。</p>\n<ul>\n<li>快照读：<strong>读取的是快照版本</strong>。普通的<code>SELECT</code>就是快照读。通过mvcc来进行并发控制的，不用加锁。</li>\n<li>当前读：<strong>读取的是最新版本</strong>。<code>UPDATE、DELETE、INSERT、SELECT … LOCK IN SHARE MODE、SELECT … FOR UPDATE</code>是当前读。</li>\n</ul>\n<p>快照读情况下，InnoDB通过<code>mvcc</code>机制避免了幻读现象。而<code>mvcc</code>机制无法避免当前读情况下出现的幻读现象。因为当前读每次读取的都是最新数据，这时如果两次查询中间有其它事务插入数据，就会产生幻读。</p>\n<p>下面举个例子说明下：</p>\n<p>1、首先，user表只有两条记录，具体如下：</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/a0f3abf69e0e4a45868300c8f96ae0ad.png\">\n</p>\n<p>2、事务a和事务b同时开启事务<code>start transaction</code>；</p>\n<p>3、事务a插入数据然后提交；</p>\n<pre><code class=\"language-sql\">insert into user(user_name, user_password, user_mail, user_state) values('tyson', 'a', 'a', 0);\n</code></pre>\n<p>4、事务b执行全表的update；</p>\n<pre><code class=\"language-sql\">update user set user_name = 'a';\n</code></pre>\n<p>5、事务b然后执行查询，查到了事务a中插入的数据。（下图左边是事务b，右边是事务a。事务开始之前只有两条记录，事务a插入一条数据之后，事务b查询出来是三条数据）</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/f98b40f48d1641018a638e91fda8a69d.png\">\n</p>\n<p>以上就是当前读出现的幻读现象。</p>\n<p><strong>那么MySQL是如何避免幻读？</strong></p>\n<ul>\n<li>针对快照读（普通 select 语句），是通过 MVCC 方式解决了幻读，因为可重复读隔离级别下，事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，即使中途有其他事务插入了一条数据，是查询不出来这条数据的，所以就很好了避免幻读问题。</li>\n<li>针对当前读（select ... for update 等语句），是通过 next-key lock（记录锁+间隙锁）方式解决了幻读，因为当执行 select ... for update 语句的时候，会加上 next-key lock，如果有其他事务在 next-key lock 锁范围内插入了一条记录，那么这个插入语句就会被阻塞，无法成功插入，所以就很好了避免幻读问题。</li>\n</ul>\n<p>next-key包括两部分：行锁和间隙锁。行锁是加在索引上的锁，间隙锁是加在索引之间的。</p>\n<p><code>Serializable</code>隔离级别也可以避免幻读，会锁住整张表，并发性极低，一般不会使用。</p>\n<h2 id=\"13共享锁和排他锁\">13、共享锁和排他锁</h2>\n<p>SELECT 的读取锁定主要分为两种方式：共享锁和排他锁。</p>\n<pre><code class=\"language-sql\">select * from table where id&lt;6 lock in share mode;--共享锁\nselect * from table where id&lt;6 for update;--排他锁\n</code></pre>\n<p>这两种方式主要的不同在于<code>LOCK IN SHARE MODE </code>多个事务同时更新同一个表单时很容易造成死锁。</p>\n<p>申请排他锁的前提是，没有线程对该结果集的任何行数据使用排它锁或者共享锁，否则申请会受到阻塞。在进行事务操作时，MySQL会对查询结果集的每行数据添加排它锁，其他线程对这些数据的更改或删除操作会被阻塞（只能读操作），直到该语句的事务被<code>commit</code>语句或<code>rollback</code>语句结束为止。</p>\n<p><code>SELECT... FOR UPDATE</code> 使用注意事项：</p>\n<ol>\n<li><code>for update</code> 仅适用于innodb，且必须在事务范围内才能生效。</li>\n<li>根据主键进行查询，查询条件为<code>like</code>或者不等于，主键字段产生<strong>表锁</strong>。</li>\n<li>根据非索引字段进行查询，会产生<strong>表锁</strong>。</li>\n</ol>\n<h2 id=\"14bin-logredo-logundo-log\">14、bin log/redo log/undo log</h2>\n<p>MySQL日志主要包括查询日志、慢查询日志、事务日志、错误日志、二进制日志等。其中比较重要的是 <code>bin log</code>（二进制日志）和 <code>redo log</code>（重做日志）和 <code>undo log</code>（回滚日志）。</p>\n<p><strong>bin log</strong></p>\n<p><code>bin log</code>是MySQL数据库级别的文件，记录对MySQL数据库执行修改的所有操作，不会记录select和show语句，主要用于恢复数据库和同步数据库。</p>\n<p><strong>redo log</strong></p>\n<p><code>redo log</code>是innodb引擎级别，用来记录innodb存储引擎的事务日志，不管事务是否提交都会记录下来，用于数据恢复。当数据库发生故障，innoDB存储引擎会使用<code>redo log</code>恢复到发生故障前的时刻，以此来保证数据的完整性。将参数<code>innodb_flush_log_at_tx_commit</code>设置为1，那么在执行commit时会将<code>redo log</code>同步写到磁盘。</p>\n<p><strong>undo log</strong></p>\n<p>除了记录<code>redo log</code>外，当进行数据修改时还会记录<code>undo log</code>，<code>undo log</code>用于数据的撤回操作，它保留了记录修改前的内容。通过<code>undo log</code>可以实现事务回滚，并且可以根据<code>undo log</code>回溯到某个特定的版本的数据，<strong>实现MVCC</strong>。</p>\n<blockquote>\n<ul>\n<li><em><strong>实现事务回滚</strong></em>，保障事务的原子性。事务处理过程中，如果出现了错误或者用户执 行了 ROLLBACK 语句，MySQL 可以利用 undo log 中的历史数据将数据恢复到事务开始之前的状态。</li>\n<li><em><strong>实现 MVCC（多版本并发控制）关键因素之一</strong></em>。MVCC 是通过 ReadView + undo log 实现的。undo log 为每条记录保存多份历史数据，MySQL 在执行快照读（普通 select 语句）的时候，会根据事务的 Read View 里的信息，顺着 undo log 的版本链找到满足其可见性的记录。</li>\n</ul>\n<p><strong>Buffer Poll：</strong> MySQL 的数据都是存在磁盘中的，那么我们要更新一条记录的时候，得先要从磁盘读取该记录，然后在内存中修改这条记录。修改完这条记录后将记录放入缓存中，下次有查询语句命中了这条记录，直接读取缓存中的记录，就不需要从磁盘获取数据了。</p>\n<ul>\n<li>当读取数据时，如果数据存在于 Buffer Pool 中，客户端就会直接读取 Buffer Pool 中的数据，否则再去磁盘中读取。</li>\n<li>当修改数据时，如果数据存在于 Buffer Pool 中，那直接修改 Buffer Pool 中数据所在的页，然后将其页设置为脏页（该页的内存数据和磁盘上的数据已经不一致），为了减少磁盘I/O，不会立即将脏页写入磁盘，后续由后台线程选择一个合适的时机将脏页写入到磁盘。</li>\n</ul>\n<p><strong>redo log：</strong> Buffer Pool 是提高了读写效率没错，但是问题来了，Buffer Pool 是基于内存的，而内存总是不可靠，万一断电重启，还没来得及落盘的脏页数据就会丢失。\n为了防止断电导致数据丢失的问题，当有一条记录需要更新的时候，InnoDB 引擎就会先更新内存（同时标记为脏页），然后将本次对这个页的修改以 redo log 的形式记录下来，这个时候更新就算完成了。</p>\n<p><strong>bin log：</strong> MySQL 在完成一条更新操作后，Server 层还会生成一条 binlog，等之后事务提交的时候，会将该事物执行过程中产生的所有 binlog 统一写 入 binlog 文件。\nbinlog 文件是记录了所有数据库表结构变更和表数据修改的日志，不会记录查询类的操作，比如 SELECT 和 SHOW 操作。</p>\n</blockquote>\n<h2 id=\"15bin-log和redo-log有什么区别\">15、bin log和redo log有什么区别</h2>\n<ul>\n<li>\n<p>binlog 是 MySQL 的 Server 层实现的日志，所有存储引擎都可以使用；redo log 是 Innodb 存储引擎实现的日志；</p>\n</li>\n<li>\n<p>binlog 是追加写，写满一个文件，就创建一个新的文件继续写，不会覆盖以前的日志，保存的是全量的日志。redo log 是循环写，日志空间大小是固定，全部写满就从头开始，保存未被刷入磁盘的脏页日志。</p>\n</li>\n<li>\n<p><code>bin log</code>是逻辑日志，记录的是SQL语句的原始逻辑；<code>redo log</code>是物理日志，记录的是在某个数据页上做了什么修改。</p>\n</li>\n</ul>\n<h2 id=\"16讲一下mysql架构\">16、讲一下MySQL架构</h2>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/3feecf7a27754947b91011da413669b4.png\">\n</p>\n<p>MySQL主要分为 Server 层和存储引擎层：</p>\n<ul>\n<li><strong>Server 层</strong>：主要包括连接器、查询缓存、分析器、优化器、执行器等，所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图，函数等，还有一个通用的日志模块 binglog 日志模块。</li>\n<li><strong>存储引擎</strong>： 主要负责数据的存储和读取。server 层通过api与存储引擎进行通信。</li>\n</ul>\n<blockquote>\n<p>Server 层负责建立连接、分析和执行 SQL。MySQL 大多数的核心功能模块都在这实现，主要包括连接器，查询缓存、解析器、预处理器、优化器、执行器等。另外，所有的内置函数（如日期、时间、数学和加密函数等）和所有跨存储引擎的功能（如存储过程、触发器、视图等。）都在 Server 层实现。</p>\n<p>存储引擎层负责数据的存储和提取。支持 InnoDB、MyISAM、Memory 等多个存储引擎，不同的存储引擎共用一个 Server 层。现在最常用的存储引擎是 InnoDB，从 MySQL 5.5 版本开始， InnoDB 成为了 MySQL 的默认存储引擎。我们常说的索引数据结构，就是由存储引擎层实现的，不同的存储引擎支持的索引类型也不相同，比如 InnoDB 支持索引类型是 B+树 ，且是默认使用，也就是说在数据表中创建的主键索引和二级索引默认使用的是 B+ 树索引。</p>\n</blockquote>\n<p><strong>Server 层基本组件</strong></p>\n<ul>\n<li><strong>连接器：</strong> 当客户端连接 MySQL 时，server层会对其进行身份认证和权限校验。</li>\n<li><strong>查询缓存:</strong> 执行查询语句的时候，会先查询缓存，先校验这个 sql 是否执行过，如果有缓存这个 sql，就会直接返回给客户端，如果没有命中，就会执行后续的操作。</li>\n<li><strong>分析器:</strong> 没有命中缓存的话，SQL 语句就会经过分析器，主要分为两步，词法分析和语法分析，先看 SQL 语句要做什么，再检查 SQL 语句语法是否正确。</li>\n<li><strong>优化器：</strong> 优化器对查询进行优化，包括重写查询、决定表的读写顺序以及选择合适的索引等，生成执行计划。</li>\n<li><strong>执行器：</strong> 首先执行前会校验该用户有没有权限，如果没有权限，就会返回错误信息，如果有权限，就会根据执行计划去调用引擎的接口，返回结果。</li>\n</ul>\n<h2 id=\"17分库分表\">17、分库分表</h2>\n<p>当单表的数据量达到1000W或100G以后，优化索引、添加从库等可能对数据库性能提升效果不明显，此时就要考虑对其进行切分了。切分的目的就在于减少数据库的负担，缩短查询的时间。</p>\n<p>数据切分可以分为两种方式：垂直划分和水平划分。</p>\n<p><strong>垂直划分</strong></p>\n<p>垂直划分数据库是根据业务进行划分，例如购物场景，可以将库中涉及商品、订单、用户的表分别划分出成一个库，通过降低单库的大小来提高性能。同样的，分表的情况就是将一个大表根据业务功能拆分成一个个子表，例如商品基本信息和商品描述，商品基本信息一般会展示在商品列表，商品描述在商品详情页，可以将商品基本信息和商品描述拆分成两张表。</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/a5ceca78b476467ab019021543fa4244.png\">\n</p>\n<p><strong>优点</strong>：行记录变小，数据页可以存放更多记录，在查询时减少I/O次数。</p>\n<p><strong>缺点</strong>：</p>\n<ul>\n<li>主键出现冗余，需要管理冗余列；</li>\n<li>会引起表连接JOIN操作，可以通过在业务服务器上进行join来减少数据库压力；</li>\n<li>依然存在单表数据量过大的问题。</li>\n</ul>\n<p><strong>水平划分</strong></p>\n<p>水平划分是根据一定规则，例如时间或id序列值等进行数据的拆分。比如根据年份来拆分不同的数据库。每个数据库结构一致，但是数据得以拆分，从而提升性能。\n<img src=\"http://110.41.141.141:9000/weblog/weblog/72decfe442834c83aa1c8c3480fb85ef.png\">\n</p>\n<p><strong>优点</strong>：单库（表）的数据量得以减少，提高性能；切分出的表结构相同，程序改动较少。</p>\n<p><strong>缺点</strong>：</p>\n<ul>\n<li>分片事务一致性难以解决</li>\n<li>跨节点<code>join</code>性能差，逻辑复杂</li>\n<li>数据分片在扩容时需要迁移</li>\n</ul>\n<h2 id=\"18什么是分区表\">18、什么是分区表</h2>\n<p>分区是把一张表的数据分成N多个区块。分区表是一个独立的逻辑表，但是底层由多个物理子表组成。</p>\n<p>当查询条件的数据分布在某一个分区的时候，查询引擎只会去某一个分区查询，而不是遍历整个表。在管理层面，如果需要删除某一个分区的数据，只需要删除对应的分区即可。</p>\n<p>分区一般都是放在单机里的，用的比较多的是时间范围分区，方便归档。只不过分库分表需要代码实现，分区则是mysql内部实现。分库分表和分区并不冲突，可以结合使用。</p>\n<h2 id=\"19分区表类型\">19、分区表类型</h2>\n<p><strong>range分区</strong>，按照范围分区。比如按照时间范围分区</p>\n<pre><code class=\"language-sql\">CREATE TABLE test_range_partition(\n       id INT auto_increment,\n       createdate DATETIME,\n       primary key (id,createdate)\n   ) \n   PARTITION BY RANGE (TO_DAYS(createdate) ) (\n      PARTITION p201801 VALUES LESS THAN ( TO_DAYS('20180201') ),\n      PARTITION p201802 VALUES LESS THAN ( TO_DAYS('20180301') ),\n      PARTITION p201803 VALUES LESS THAN ( TO_DAYS('20180401') ),\n      PARTITION p201804 VALUES LESS THAN ( TO_DAYS('20180501') ),\n      PARTITION p201805 VALUES LESS THAN ( TO_DAYS('20180601') ),\n      PARTITION p201806 VALUES LESS THAN ( TO_DAYS('20180701') ),\n      PARTITION p201807 VALUES LESS THAN ( TO_DAYS('20180801') ),\n      PARTITION p201808 VALUES LESS THAN ( TO_DAYS('20180901') ),\n      PARTITION p201809 VALUES LESS THAN ( TO_DAYS('20181001') ),\n      PARTITION p201810 VALUES LESS THAN ( TO_DAYS('20181101') ),\n      PARTITION p201811 VALUES LESS THAN ( TO_DAYS('20181201') ),\n      PARTITION p201812 VALUES LESS THAN ( TO_DAYS('20190101') )\n   );\n</code></pre>\n<p>在<code>/var/lib/mysql/data/</code>可以找到对应的数据文件，每个分区表都有一个使用#分隔命名的表文件：</p>\n<pre><code class=\"language-sql\">   -rw-r----- 1 MySQL MySQL    65 Mar 14 21:47 db.opt\n   -rw-r----- 1 MySQL MySQL  8598 Mar 14 21:50 test_range_partition.frm\n   -rw-r----- 1 MySQL MySQL 98304 Mar 14 21:50 test_range_partition#P#p201801.ibd\n   -rw-r----- 1 MySQL MySQL 98304 Mar 14 21:50 test_range_partition#P#p201802.ibd\n   -rw-r----- 1 MySQL MySQL 98304 Mar 14 21:50 test_range_partition#P#p201803.ibd\n...\n</code></pre>\n<p><strong>list分区</strong></p>\n<p>list分区和range分区相似，主要区别在于list是枚举值列表的集合，range是连续的区间值的集合。对于list分区，分区字段必须是已知的，如果插入的字段不在分区时的枚举值中，将无法插入。</p>\n<pre><code class=\"language-sql\">create table test_list_partiotion\n   (\n       id int auto_increment,\n       data_type tinyint,\n       primary key(id,data_type)\n   )partition by list(data_type)\n   (\n       partition p0 values in (0,1,2,3,4,5,6),\n       partition p1 values in (7,8,9,10,11,12),\n       partition p2 values in (13,14,15,16,17)\n   );\n</code></pre>\n<p><strong>hash分区</strong></p>\n<p>可以将数据均匀地分布到预先定义的分区中。</p>\n<pre><code class=\"language-sql\">create table test_hash_partiotion\n   (\n       id int auto_increment,\n       create_date datetime,\n       primary key(id,create_date)\n   )partition by hash(year(create_date)) partitions 10;\n</code></pre>\n<h2 id=\"20分区的问题\">20、分区的问题</h2>\n<ul>\n<li>\n<p>打开和锁住所有底层表的成本可能很高。当查询访问分区表时，MySQL 需要打开并锁住所有的底层表，这个操作在分区过滤之前发生，所以无法通过分区过滤来降低此开销，会影响到查询速度。可以通过批量操作来降低此类开销，比如批量插入、<code>LOAD DATA INFILE</code>和一次删除多行数据。</p>\n</li>\n<li>\n<p>维护分区的成本可能很高。例如重组分区，会先创建一个临时分区，然后将数据复制到其中，最后再删除原分区。</p>\n</li>\n<li>\n<p>所有分区必须使用相同的存储引擎。</p>\n</li>\n</ul>\n<h2 id=\"21查询语句执行流程\">21、查询语句执行流程</h2>\n<p>查询语句的执行流程如下：权限校验、查询缓存、分析器、优化器、权限校验、执行器、引擎。</p>\n<p>举个例子，查询语句如下：</p>\n<pre><code class=\"language-sql\">select * from user where id &gt; 1 and name = '大彬';\n</code></pre>\n<ul>\n<li>\n<p>首先检查权限，没有权限则返回错误；</p>\n</li>\n<li>\n<p>MySQL8.0以前会查询缓存，缓存命中则直接返回，没有则执行下一步；</p>\n</li>\n<li>\n<p>词法分析和语法分析。提取表名、查询条件，检查语法是否有错误；</p>\n</li>\n<li>\n<p>两种执行方案，先查 <code>id &gt; 1</code> 还是 <code>name = '大彬'</code>，优化器根据自己的优化算法选择执行效率最好的方案；</p>\n</li>\n<li>\n<p>校验权限，有权限就调用数据库引擎接口，返回引擎的执行结果。</p>\n</li>\n</ul>\n<h2 id=\"22更新语句执行过程\">22、更新语句执行过程</h2>\n<p>更新语句执行流程如下：分析器、权限校验、执行器、引擎、<code>redo log</code>（<code>prepare</code>状态）、<code>binlog</code>、<code>redo log</code>（<code>commit</code>状态）</p>\n<p>举个例子，更新语句如下：</p>\n<pre><code class=\"language-sql\">update user set name = '大彬' where id = 1;\n</code></pre>\n<ol>\n<li>先查询到 id 为1的记录，有缓存会使用缓存。</li>\n<li>拿到查询结果，将 name 更新为大彬，然后调用引擎接口，写入更新数据，innodb 引擎将数据保存在内存中，同时记录<code>redo log</code>，此时<code>redo log</code>进入 <code>prepare</code>状态。</li>\n<li>执行器收到通知后记录<code>binlog</code>，然后调用引擎接口，提交<code>redo log</code>为<code>commit</code>状态。</li>\n<li>更新完成。</li>\n</ol>\n<p>为什么记录完<code>redo log</code>，不直接提交，而是先进入<code>prepare</code>状态？</p>\n<p>假设先写<code>redo log</code>直接提交，然后写<code>binlog</code>，写完<code>redo log</code>后，机器挂了，<code>binlog</code>日志没有被写入，那么机器重启后，这台机器会通过<code>redo log</code>恢复数据，但是这个时候<code>binlog</code>并没有记录该数据，后续进行机器备份的时候，就会丢失这一条数据，同时主从同步也会丢失这一条数据。</p>\n<h2 id=\"23exist和in的区别\">23、exist和in的区别</h2>\n<blockquote>\n<pre><code class=\"language-sql\">SELECT * FROM A WHERE cc IN (SELECT cc FROM B)；\n</code></pre>\n<pre><code class=\"language-sql\">SELECT * FROM A WHERE EXISTS (SELECT cc FROM B WHERE B.cc = A.cc)；\n</code></pre>\n<p><code>IN</code>是在A中选择一条记录，然后在B中查找该记录是否存在。<code>EXISTS</code>是在B中选择一条符合条件的记录，然后在A中查找该记录是否存在。</p>\n</blockquote>\n<ul>\n<li><strong>In</strong>：确定给定的值是否与子查询或列表中的值相匹配。in在查询的时候，首先查询子查询的表，然后将内表和外表做一个笛卡尔积，然后按照条件进行筛选。所以相对内表比较小的时候，in的速度较快。</li>\n<li><strong>exists</strong>：指定一个子查询，检测行的存在。循环遍历外表，然后看外表中的记录有没有和内表的数据一样的。匹配上就将结果放入结果集中。</li>\n</ul>\n<p><strong>子查询的表比较大的时候</strong>，使用<code>exists</code>可以有效减少总的循环次数来提升速度；<strong>当外查询的表比较大的时候</strong>，使用<code>in</code>可以有效减少对外查询表循环遍历来提升速度。</p>\n<h2 id=\"24mysql中int10和char10的区别\">24、MySQL中int(10)和char(10)的区别</h2>\n<p>int(10)中的10表示的是<strong>显示数据</strong>的长度，而char(10)表示的是<strong>存储数据</strong>的长度。</p>\n<h2 id=\"25truncatedelete与drop区别\">25、truncate、delete与drop区别</h2>\n<p><strong>相同点：</strong></p>\n<ol>\n<li><code>truncate</code>和不带<code>where</code>子句的<code>delete</code>、以及<code>drop</code>都会删除表内的数据。</li>\n<li><strong><code>drop</code>、<code>truncate</code>都是<code>DDL</code>语句（数据定义语言）</strong>，执行后会自动提交。</li>\n</ol>\n<p><strong>不同点：</strong></p>\n<ol>\n<li>truncate 和 delete 只删除数据不删除表的结构；<strong>drop 语句将删除表的结构、被依赖的约束、触发器、索引</strong>；</li>\n<li>一般来说，<strong>执行速度: drop &gt; truncate &gt; delete</strong>。</li>\n</ol>\n<h2 id=\"26having和where区别\">26、having和where区别？</h2>\n<ul>\n<li>二者作用的对象不同，<code>where</code>子句作用于表和视图，<code>having</code>作用于组。</li>\n<li><code>where</code>在数据分组前进行过滤，<code>having</code>在数据分组后进行过滤。</li>\n</ul>\n<h2 id=\"27什么是mysql主从同步\">27、什么是MySQL主从同步？</h2>\n<p>主从同步使得数据可以从一个数据库服务器复制到其他服务器上，在复制数据时，一个服务器充当主服务器（<code>master</code>），其余的服务器充当从服务器（<code>slave</code>）。</p>\n<p>因为复制是异步进行的，所以从服务器不需要一直连接着主服务器，从服务器甚至可以通过拨号断断续续地连接主服务器。通过配置文件，可以指定复制所有的数据库，某个数据库，甚至是某个数据库上的某个表。</p>\n<hr />\n<p><em><strong>MySQL 的主从复制</strong></em>依赖于 binlog ，也就是记录 MySQL 上的所有变化并以二进制形式保存在磁盘上。复制的过程就是将 binlog 中的数据从主库传输到从库上。</p>\n<p>这个过程一般是异步的，也就是主库上执行事务操作的线程不会等待复制 binlog 的线程同步完成。\n<img src=\"http://110.41.141.141:9000/weblog/weblog/69437c3c7def4cc1bec0f1de78dc13d3.png\">\n</p>\n<p>MySQL 集群的主从复制过程梳理成 3 个阶段：</p>\n<ul>\n<li><strong>写入 Binlog</strong>：主库写 binlog 日志，提交事务，并更新本地存储数据。</li>\n<li><strong>同步 Binlog</strong>：把 binlog 复制到所有从库上，每个从库把 binlog 写到暂存日志中。</li>\n<li><strong>回放 Binlog</strong>：回放 binlog，并更新存储引擎中的数据。</li>\n</ul>\n<p>具体详细过程如下：</p>\n<ul>\n<li>MySQL 主库在收到客户端提交事务的请求之后，会先写入 binlog，再提交事务，更新存储引擎中的数据，事务提交完成后，返回给客户端“操作成功”的响应。</li>\n<li>从库会创建一个专门的 I/O 线程，连接主库的 log dump 线程，来接收主库的 binlog 日志，再把 binlog 信息写入 relay log 的中继日志里，再返回给主库“复制成功”的响应。</li>\n<li>从库会创建一个用于回放 binlog 的线程，去读 relay log 中继日志，然后回放 binlog 更新存储引擎中的数据，最终实现主从的数据一致性。</li>\n</ul>\n<h2 id=\"28为什么要做主从同步\">28、为什么要做主从同步？</h2>\n<ol>\n<li><strong>读写分离</strong>，使数据库能支撑更大的并发。</li>\n<li><strong>在主服务器上生成实时数据，而在从服务器上分析这些数据</strong>，从而提高主服务器的性能。</li>\n<li><strong>数据备份</strong>，保证数据的安全。</li>\n</ol>\n<h2 id=\"29乐观锁和悲观锁是什么\">29、乐观锁和悲观锁是什么</h2>\n<p>数据库中的并发控制是确保在多个事务同时存取数据库中同一数据时不破坏事务的隔离性和一致性以及数据库的一致性。乐观锁和悲观锁是并发控制主要采用的技术手段。</p>\n<ul>\n<li><strong>悲观锁</strong>：假定会发生并发冲突，会对操作的数据进行加锁，直到提交事务，才会释放锁，其他事务才能进行修改。实现方式：使用数据库中的锁机制。</li>\n<li><strong>乐观锁</strong>：假设不会发生并发冲突，只在提交操作时检查数据是否被修改过。给表增加<code>version</code>字段，在修改提交之前检查<code>version</code>与原来取到的<code>version</code>值是否相等，若相等，表示数据没有被修改，可以更新，否则，数据为脏数据，不能更新。实现方式：乐观锁一般使用版本号机制或<code>CAS</code>算法实现。</li>\n</ul>\n<h2 id=\"30用过processlist吗\">30、用过processlist吗</h2>\n<p><code>show processlist</code> 或 <code>show full processlist</code> 可以查看当前 MySQL 是否有压力，正在运行的<code>SQL</code>，有没有慢<code>SQL</code>正在执行。返回参数如下：</p>\n<ol>\n<li>\n<p><strong>id</strong>：线程ID，可以用<code>kill id</code>杀死某个线程</p>\n</li>\n<li>\n<p><strong>db</strong>：数据库名称</p>\n</li>\n<li>\n<p><strong>user</strong>：数据库用户</p>\n</li>\n<li>\n<p><strong>host</strong>：数据库实例的IP</p>\n</li>\n<li>\n<p><strong>command</strong>：当前执行的命令，比如<code>Sleep</code>，<code>Query</code>，<code>Connect </code>等</p>\n</li>\n<li>\n<p><strong>time</strong>：消耗时间，单位秒</p>\n</li>\n<li>\n<p>state</p>\n<p>：执行状态，主要有以下状态：</p>\n<ul>\n<li>Sleep，线程正在等待客户端发送新的请求</li>\n<li>Locked，线程正在等待锁</li>\n<li>Sending data，正在处理<code>SELECT</code>查询的记录，同时把结果发送给客户端</li>\n<li>Kill，正在执行<code>kill</code>语句，杀死指定线程</li>\n<li>Connect，一个从节点连上了主节点</li>\n<li>Quit，线程正在退出</li>\n<li>Sorting for group，正在为<code>GROUP BY</code>做排序</li>\n<li>Sorting for order，正在为<code>ORDER BY</code>做排序</li>\n</ul>\n</li>\n<li>\n<p><strong>info</strong>：正在执行的<code>SQL</code>语句</p>\n</li>\n</ol>\n<h2 id=\"31mysql查询-limit-100010-和limit-10-速度一样快吗\">31、MySQL查询 limit 1000,10 和limit 10 速度一样快吗</h2>\n<p>两种查询方式。对应 <code>limit offset, size</code> 和 <code>limit size</code> 两种方式。</p>\n<p>而其实 <code>limit size</code> ，相当于 <code>limit 0, size</code>。也就是从0开始取size条数据。</p>\n<p>也就是说，两种方式的<strong>区别在于offset是否为0。</strong></p>\n<p>先来看下limit sql的内部执行逻辑。</p>\n<p>MySQL内部分为<strong>server层</strong>和<strong>存储引擎层</strong>。一般情况下存储引擎都用innodb。</p>\n<p>server层有很多模块，其中需要关注的是<strong>执行器</strong>是用于跟存储引擎打交道的组件。</p>\n<p>执行器可以通过调用存储引擎提供的接口，将一行行数据取出，当这些数据完全符合要求（比如满足其他where条件），则会放到<strong>结果集</strong>中，最后返回给调用mysql的<strong>客户端</strong>。</p>\n<p>以主键索引的limit执行过程为例：</p>\n<p>执行<code>select * from xxx order by id limit 0, 10;</code>，select后面带的是<strong>星号</strong>，也就是要求获得行数据的<strong>所有字段信息。</strong></p>\n<p>server层会调用innodb的接口，在innodb里的主键索引中获取到第0到10条<strong>完整行数据</strong>，依次返回给server层，并放到server层的结果集中，返回给客户端。</p>\n<p>把offset搞大点，比如执行的是：<code>select * from xxx order by id limit 500000, 10;</code></p>\n<p>server层会调用innodb的接口，由于这次的offset=500000，会在innodb里的主键索引中获取到第0到（500000 + 10）条<strong>完整行数据</strong>，<strong>返回给server层之后根据offset的值挨个抛弃，最后只留下最后面的size条</strong>，也就是10条数据，放到server层的结果集中，返回给客户端。</p>\n<p>可以看出，当offset非0时，server层会从引擎层获取到<strong>很多无用的数据</strong>，而获取的这些无用数据都是要耗时的。</p>\n<p>因此，mysql查询中 limit 1000,10 会比 limit 10 更慢。原因是 limit 1000,10 会取出1000+10条数据，并抛弃前1000条，这部分耗时更大。</p>\n<h2 id=\"32深分页怎么优化\">32、深分页怎么优化</h2>\n<p>还是以上面的SQL为空：<code>select * from xxx order by id limit 500000, 10;</code></p>\n<p><strong>方法一</strong>：</p>\n<p>从上面的分析可以看出，当offset非常大时，server层会从引擎层获取到很多无用的数据，而当select后面是*号时，就需要拷贝完整的行信息，<strong>拷贝完整数据</strong>相比<strong>只拷贝行数据里的其中一两个列字段</strong>更耗费时间。</p>\n<p>因为前面的offset条数据最后都是不要的，没有必要拷贝完整字段，所以可以将sql语句修改成：</p>\n<pre><code class=\"language-sql\">select * from xxx  where id &gt;=(select id from xxx order by id limit 500000, 1) order by id limit 10;\n</code></pre>\n<p>先执行子查询 <code>select id from xxx by id limit 500000, 1</code>, 这个操作，其实也是将在innodb中的主键索引中获取到<code>500000+1</code>条数据，然后server层会抛弃前500000条，只保留最后一条数据的id。</p>\n<p>但不同的地方在于，在返回server层的过程中，只会拷贝数据行内的id这一列，而不会拷贝数据行的所有列，当数据量较大时，这部分的耗时还是比较明显的。</p>\n<p>在拿到了上面的id之后，假设这个id正好等于500000，那sql就变成了</p>\n<pre><code class=\"language-sql\">select * from xxx  where id &gt;=500000 order by id limit 10;\n</code></pre>\n<p>这样innodb再走一次<strong>主键索引</strong>，通过B+树快速定位到id=500000的行数据，时间复杂度是lg(n)，然后向后取10条数据。</p>\n<p><strong>方法二：</strong></p>\n<p>将所有的数据<strong>根据id主键进行排序</strong>，然后分批次取，将当前批次的最大id作为下次筛选的条件进行查询。</p>\n<pre><code class=\"language-sql\">select * from xxx where id &gt; start_id order by id limit 10;\n</code></pre>\n<p>通过主键索引，每次定位到start_id的位置，然后往后遍历10个数据，这样不管数据多大，查询性能都较为稳定。</p>\n<ul>\n<li>\n<p>子查询+索引</p>\n<blockquote>\n<p>思路：通过将 select * 转变为 select id，把符合条件的 id 筛选出来后，最后通过嵌套查询的方式按顺序取出 id 对应的行。</p>\n<pre><code class=\"language-sql\">-- 优化前\nselect *\nfrom people\norder by create_time desc\nlimit 5000000, 10;\n\n-- 优化后\nselect a.*\nfrom people a\ninner join(\n select id\n from people\n order by create_time desc\n limit 5000000, 10\n) b ON a.id = b.id;\n</code></pre>\n</blockquote>\n</li>\n<li>\n<p>联合索引</p>\n<blockquote>\n<p>刚才我们优化后的 SQL 语句如下，是没有 where 条件的。</p>\n<pre><code class=\"language-sql\">select a.*\nfrom people a\ninner join(\n select id\n from people\n order by create_time desc\n limit 5000010, 10\n) b ON a.id = b.id;\n</code></pre>\n</blockquote>\n<blockquote>\n<p>为了更接近现实场景，假设我们要把 status = 1 的人筛出来，SQL 就要这么写：</p>\n<pre><code class=\"language-sql\">select a.*\nfrom people a\ninner join(\n select id\n from people\n where status=1\n order by create_time desc\n limit 5000010, 10\n) b ON a.id = b.id;\n</code></pre>\n</blockquote>\n<blockquote>\n<p>为了加速这个查询，我们可以创建一个 create_time 和 status 的联合索引，对应的 SQL 语句是：</p>\n<pre><code class=\"language-sql\">alter table people add index create_time_status(create_time, status);\n</code></pre>\n</blockquote>\n</li>\n<li>\n<p>where id &gt; start_id</p>\n<blockquote>\n<p>将上一页的查询结果中的最大 id 作为下一页查询的 where 条件，这样可以大幅减少扫描行数，提高查询性能。</p>\n<pre><code class=\"language-sql\">select a.*\nfrom people a\ninner join(\n select id\n from people\n where status=1\n       and id &gt; ${id}\n order by create_time desc\n limit 10\n) b ON a.id = b.id;\n</code></pre>\n</blockquote>\n</li>\n</ul>\n<h2 id=\"33高度为3的b树可以存放多少数据\">33、高度为3的B+树，可以存放多少数据</h2>\n<p>InnoDB存储引擎有自己的最小储存单元——页（Page）。</p>\n<p>查询InnoDB页大小的命令如下：</p>\n<pre><code>mysql&gt; show global status like 'innodb_page_size';\n+------------------+-------+\n| Variable_name    | Value |\n+------------------+-------+\n| Innodb_page_size | 16384 |\n+------------------+-------+\n</code></pre>\n<p>可以看出 <strong>innodb 默认的一页大小为 16384B = 16384/1024 = 16kb</strong>。</p>\n<p>在MySQL中，B+树一个节点的大小设为一页或页的倍数最为合适。因为如果一个节点的大小 &lt; 1页，那么读取这个节点的时候其实读取的还是一页，这样就造成了资源的浪费。</p>\n<p>B+树中<strong>非叶子节点存的是key + 指针</strong>；<strong>叶子节点存的是数据行</strong>。</p>\n<p>对于叶子节点，如果一行数据大小为1k，那么一页就能存16条数据。</p>\n<p>对于非叶子节点，如果key使用的是bigint，则为8字节，指针在MySQL中为6字节，一共是14字节，则16k能存放 16 * 1024 / 14 = 1170 个索引指针。</p>\n<p>于是可以算出，对于一颗高度为2的B+树，根节点存储索引指针节点，那么它有1170个叶子节点存储数据，每个叶子节点可以存储16条数据，一共 1170 x 16 = 18720 条数据。而对于高度为3的B+树，就可以存放 1170 x 1170 x 16 = 21902400 条数据（<strong>两千多万条数据</strong>），也就是对于两千多万条的数据，我们只需要<strong>高度为3</strong>的B+树就可以完成，通过主键查询只需要3次IO操作就能查到对应数据。</p>\n<p>所以在 InnoDB 中B+树高度一般为3层时，就能满足千万级的数据存储。</p>\n<h2 id=\"34mysql单表多大进行分库分表\">34、MySQL单表多大进行分库分表</h2>\n<p>目前主流的有两种说法：</p>\n<ol>\n<li>MySQL 单表数据量大于 2000 万行，性能会明显下降，考虑进行分库分表。</li>\n<li>阿里巴巴《Java 开发手册》提出单表行数超过 500 万行或者单表容量超过 2GB，才推荐进行分库分表。</li>\n</ol>\n<p>事实上，这个数值和实际记录的条数无关，而与 MySQL 的配置以及机器的硬件有关。因为MySQL为了提高性能，会将表的索引装载到内存中。在InnoDB buffer size 足够的情况下，其能完成全加载进内存，查询不会有问题。但是，当单表数据库到达某个量级的上限时，导致内存无法存储其索引，使得之后的 SQL 查询会产生磁盘 IO，从而导致性能下降。当然，这个还有具体的表结构的设计有关，最终导致的问题都是内存限制。</p>\n<p>因此，对于分库分表，需要结合实际需求，不宜过度设计，在项目一开始不采用分库与分表设计，而是随着业务的增长，在无法继续优化的情况下，再考虑分库与分表提高系统的性能。对此，阿里巴巴《Java 开发手册》补充到：如果预计三年后的数据量根本达不到这个级别，请不要在创建表时就分库分表。</p>\n<p>至于MySQL单表多大进行分库分表，应当根据机器资源进行评估。</p>\n<h2 id=\"35大表查询慢怎么优化\">35、大表查询慢怎么优化</h2>\n<p>某个表有近千万数据，查询比较慢，如何优化？</p>\n<p>当MySQL单表记录数过大时，数据库的性能会明显下降，一些常见的优化措施如下：</p>\n<ul>\n<li><strong>合理建立索引</strong>。在合适的字段上建立索引，例如在WHERE和ORDER BY命令上涉及的列建立索引，可根据EXPLAIN来查看是否用了索引还是全表扫描</li>\n<li><strong>索引优化，SQL优化</strong>。索引要符合最左匹配原则等，参考：<a href=\"https://topjavaer.cn/database/mysql.html#什么是覆盖索引\" ref=\"nofollow\" target=\"_blank\">https://topjavaer.cn/database/mysql.html#什么是覆盖索引open in new window</a><span><svg xmlns=\"http://www.w3.org/2000/svg\" class=\"inline ml-1\" style=\"color: #aaa;\" aria-hidden=\"true\" focusable=\"false\" x=\"0px\" y=\"0px\" viewBox=\"0 0 100 100\" width=\"15\" height=\"15\" class=\"icon outbound\"><path fill=\"currentColor\" d=\"M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z\"></path> <polygon fill=\"currentColor\" points=\"45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9\"></polygon></svg> <span class=\"sr-only\"></span></span></li>\n<li><strong>建立分区</strong>。对关键字段建立水平分区，比如时间字段，若查询条件往往通过时间范围来进行查询，能提升不少性能</li>\n<li><strong>利用缓存</strong>。利用Redis等缓存热点数据，提高查询效率</li>\n<li><strong>限定数据的范围</strong>。比如：用户在查询历史信息的时候，可以控制在一个月的时间范围内</li>\n<li><strong>读写分离</strong>。经典的数据库拆分方案，主库负责写，从库负责读</li>\n<li>通过<strong>分库分表</strong>的方式进行优化，主要有垂直拆分和水平拆分</li>\n<li><strong>数据异构到es</strong></li>\n<li><strong>冷热数据分离</strong>。几个月之前不常用的数据放到冷库中，最新的数据或比较新的数据放到热库中</li>\n</ul>\n<blockquote>\n<ul>\n<li>\n<p>通过 explain 执行结果，查看 sql 是否走索引，如果不走索引，考虑增加索引。</p>\n</li>\n<li>\n<p>可以通过建立联合索引，实现覆盖索引优化，减少回表，使用联合索引符合最左匹配原则，不然会索引失效</p>\n</li>\n<li>\n<p>避免索引失效，比如不要用左模糊匹配、函数计算、表达式计算等等。</p>\n</li>\n<li>\n<p>联表查询最好要以小表驱动大表，并且被驱动表的字段要有索引，当然最好通过冗余字段的设计，避免联表查询。</p>\n</li>\n<li>\n<p>针对 limit n,y 深分页的查询优化，可以把Limit查询转换成某个位置的查询：select * from tb_sku where id&gt;20000 limit 10，该方案适用于主键自增的表，</p>\n</li>\n<li>\n<p>将字段多的表分解成多个表，有些字段使用频率高，有些低，数据量大时，会由于使用频率低的存在而变慢，可以考虑分开</p>\n</li>\n</ul>\n</blockquote>\n<h2 id=\"36说说count1count和count字段名的区别\">36、说说count(1)、count(*)和count(字段名)的区别</h2>\n<blockquote>\n<ul>\n<li>count(1)、 count(*)、 count(主键字段)在执行的时候，如果表里存在二级索引，优化器就会选择二级索引进行扫描。</li>\n<li>所以，如果要执行 count(1)、 count(*)、 count(主键字段) 时，尽量在数据表上建立二级索引，这样优化器会自动采用 key_len 最小的二级索引进行扫描，相比于扫描主键索引效率会高一些。</li>\n<li>再来，就是不要使用 count(字段) 来统计记录个数，因为它的效率是最差的，会采用全表扫描的方式来统计。如果你非要统计表中该字段不为 NULL 的记录个数，建议给这个字段建立一个二级索引。</li>\n</ul>\n</blockquote>\n<p>嗯，先说说count(1) and count(字段名)的区别。</p>\n<p>两者的主要区别是</p>\n<ol>\n<li>count(1) 会统计表中的所有的记录数，包含字段为null 的记录。</li>\n<li>count(字段名) 会统计该字段在表中出现的次数，忽略字段为null 的情况。即不统计字段为null 的记录。</li>\n</ol>\n<p>接下来看看三者之间的区别。</p>\n<p>执行效果上：</p>\n<ul>\n<li>count(*)包括了所有的列，相当于行数，在统计结果的时候，<strong>不会忽略列值为NULL</strong></li>\n<li>count(1)包括了忽略所有列，用1代表代码行，在统计结果的时候，<strong>不会忽略列值为NULL</strong></li>\n<li>count(字段名)只包括列名那一列，在统计结果的时候，会忽略列值为空（这里的空不是只空字符串或者0，而是表示null）的计数，<strong>即某个字段值为NULL时，不统计</strong>。</li>\n</ul>\n<p>执行效率上：</p>\n<ul>\n<li>列名为主键，count(字段名)会比count(1)快</li>\n<li>列名不为主键，count(1)会比count(列名)快</li>\n<li>如果表多个列并且没有主键，则 count(1) 的执行效率优于 count(*)</li>\n<li>如果有主键，则 select count(主键)的执行效率是最优的</li>\n<li>如果表只有一个字段，则 select count(*)最优。</li>\n</ul>\n<p><strong>COUNT(<code>*</code>)是SQL92定义的标准统计行数的语法，效率高，MySQL对它进行了很多优化，MyISAM中会直接把表的总行数单独记录下来供COUNT(*)查询，而InnoDB则会在扫表的时候选择最小的索引来降低成本</strong>。</p>\n<h2 id=\"37mysql中datetime-和-timestamp有什么区别\">37、MySQL中DATETIME 和 TIMESTAMP有什么区别</h2>\n<p>嗯，<code>TIMESTAMP</code>和<code>DATETIME</code>都可以用来存储时间，它们主要有以下区别：</p>\n<p>1.表示范围</p>\n<ul>\n<li>DATETIME：1000-01-01 00:00:00.000000 到 9999-12-31 23:59:59.999999</li>\n<li>TIMESTAMP：'1970-01-01 00:00:01.000000' UTC 到 '2038-01-09 03:14:07.999999' UTC</li>\n</ul>\n<p><code>TIMESTAMP</code>支持的时间范围比<code>DATATIME</code>要小，容易出现超出的情况。</p>\n<p>2.空间占用</p>\n<ul>\n<li>TIMESTAMP ：占 4 个字节</li>\n<li>DATETIME：在 MySQL 5.6.4 之前，占 8 个字节 ，之后版本，占 5 个字节</li>\n</ul>\n<p>3.存入时间是否会自动转换</p>\n<p><code>TIMESTAMP</code>类型在默认情况下，insert、update 数据时，<code>TIMESTAMP</code>列会自动以当前时间（<code>CURRENT_TIMESTAMP</code>）填充/更新。<code>DATETIME</code>则不会做任何转换，也不会检测时区，你给什么数据，它存什么数据。</p>\n<p>4.<code>TIMESTAMP</code>比较受时区timezone的影响以及MYSQL版本和服务器的SQL MODE的影响。因为<code>TIMESTAMP</code>存的是时间戳，在不同的时区得出的时间不一致。</p>\n<p>5.如果存进NULL，两者实际存储的值不同。</p>\n<ul>\n<li>TIMESTAMP：会自动存储当前时间 now() 。</li>\n<li>DATETIME：不会自动存储当前时间，会直接存入 NULL 值。</li>\n</ul>\n<h2 id=\"38说说为什么不建议用外键\">38、说说为什么不建议用外键</h2>\n<p>外键是一种约束，这个约束的存在，会保证表间数据的关系始终完整。外键的存在，并非全然没有优点。</p>\n<p>外键可以保证数据的完整性和一致性，级联操作方便。而且使用外键可以将数据完整性判断托付给了数据库完成，减少了程序的代码量。</p>\n<p>虽然外键能够保证数据的完整性，但是会给系统带来很多缺陷。</p>\n<p>1、并发问题。在使用外键的情况下，每次修改数据都需要去另外一个表检查数据，需要获取额外的锁。若是在高并发大流量事务场景，使用外键更容易造成死锁。</p>\n<p>2、扩展性问题。比如从<code>MySQL</code>迁移到<code>Oracle</code>，外键依赖于数据库本身的特性，做迁移可能不方便。</p>\n<p>3、不利于分库分表。在水平拆分和分库的情况下，外键是无法生效的。将数据间关系的维护，放入应用程序中，为将来的分库分表省去很多的麻烦。</p>\n<h2 id=\"39使用自增主键有什么好处\">39、使用自增主键有什么好处</h2>\n<p>自增主键可以让主键索引尽量地保持递增顺序插入，避免了页分裂，因此索引更紧凑，在查询的时候，效率也就更高。</p>\n<h2 id=\"40自增主键保存在什么地方\">40、自增主键保存在什么地方</h2>\n<p>不同的引擎对于自增值的保存策略不同：</p>\n<ul>\n<li><strong>MyISAM引擎的自增值保存在数据文件中</strong>。</li>\n<li>在MySQL8.0以前，InnoDB引擎的自增值是存在内存中。MySQL重启之后内存中的这个值就丢失了，每次重启后第一次打开表的时候，会找自增值的最大值max(id)，然后将最大值加1作为这个表的自增值；MySQL8.0版本会将自增值的变更记录在redo log中，重启时依靠redo log恢复。</li>\n</ul>\n<h2 id=\"41自增主键一定是连续的吗\">41、自增主键一定是连续的吗</h2>\n<p>不一定，有几种情况会导致自增主键不连续。</p>\n<p>1、<strong>唯一键冲突导致自增主键不连续</strong>。当我们向一个自增主键的InnoDB表中插入数据的时候，如果违反表中定义的唯一索引的唯一约束，会导致插入数据失败。此时表的自增主键的键值是会向后加1滚动的。下次再次插入数据的时候，就不能再使用上次因插入数据失败而滚动生成的键值了，必须使用新滚动生成的键值。</p>\n<p>2、<strong>事务回滚导致自增主键不连续</strong>。当我们向一个自增主键的InnoDB表中插入数据的时候，如果显式开启了事务，然后因为某种原因最后回滚了事务，此时表的自增值也会发生滚动，而接下里新插入的数据，也将不能使用滚动过的自增值，而是需要重新申请一个新的自增值。</p>\n<p>3、<strong>批量插入导致自增值不连续</strong>。MySQL有一个批量申请自增id的策略：</p>\n<ul>\n<li>语句执行过程中，第一次申请自增id，分配1个自增id</li>\n<li>1个用完以后，第二次申请，会分配2个自增id</li>\n<li>2个用完以后，第三次申请，会分配4个自增id</li>\n<li>依次类推，每次申请都是上一次的两倍（最后一次申请不一定全部使用）</li>\n</ul>\n<p>如果下一个事务再次插入数据的时候，则会基于上一个事务申请后的自增值基础上再申请。此时就出现自增值不连续的情况出现。</p>\n<p>4、<strong>自增步长不是1</strong>，也会导致自增主键不连续。</p>\n<h2 id=\"42innodb的自增值为什么不能回收利用\">42、InnoDB的自增值为什么不能回收利用</h2>\n<p><strong>主要为了提升插入数据的效率和并行度</strong>。</p>\n<p>假设有两个并行执行的事务，在申请自增值的时候，为了避免两个事务申请到相同的自增 id，肯定要加锁，然后顺序申请。</p>\n<p>假设事务 A 申请到了 id=2， 事务 B 申请到 id=3，那么这时候表 t 的自增值是 4，之后继续执行。</p>\n<p>事务 B 正确提交了，但事务 A 出现了唯一键冲突。</p>\n<p>如果允许事务 A 把自增 id 回退，也就是把表 t 的当前自增值改回 2，那么就会出现这样的情况：表里面已经有 id=3 的行，而当前的自增 id 值是 2。</p>\n<p>接下来，继续执行的其他事务就会申请到 id=2，然后再申请到 id=3。这时，就会出现插入语句报错“主键冲突”。</p>\n<p>而为了解决这个主键冲突，有两种方法：</p>\n<ul>\n<li>每次申请 id 之前，先判断表里面是否已经存在这个 id。如果存在，就跳过这个 id。但是，这个方法的成本很高。因为，本来申请 id 是一个很快的操作，现在还要再去主键索引树上判断 id 是否存在。</li>\n<li>把自增 id 的锁范围扩大，必须等到一个事务执行完成并提交，下一个事务才能再申请自增 id。这个方法的问题，就是锁的粒度太大，系统并发能力大大下降。</li>\n</ul>\n<p>可见，这两个方法都会导致性能问题。</p>\n<p>因此，InnoDB 放弃了“允许自增 id 回退”这个设计，语句执行失败也不回退自增 id。</p>\n<h2 id=\"43mysql数据如何同步到redis缓存\">43、MySQL数据如何同步到Redis缓存</h2>\n<p>有两种方案：</p>\n<p>1、通过MySQL自动同步刷新Redis，<strong>MySQL触发器+UDF函数</strong>实现。</p>\n<blockquote>\n<p>UDF函数：常见的函数类型，可以操作单个数据行，且产生一个数据行作为输出，大多数函数为这一类。</p>\n</blockquote>\n<p>过程大致如下：</p>\n<ol>\n<li>在MySQL中对要操作的数据设置触发器Trigger，监听操作</li>\n<li>客户端向MySQL中写入数据时，触发器会被触发，触发之后调用MySQL的UDF函数</li>\n<li>UDF函数可以把数据写入到Redis中，从而达到同步的效果</li>\n</ol>\n<p>2、<strong>解析MySQL的binlog</strong>，实现将数据库中的数据同步到Redis。可以通过canal实现。canal是阿里巴巴旗下的一款开源项目，基于数据库增量日志解析，提供增量数据订阅&amp;消费。</p>\n<p>canal的原理如下：</p>\n<ol>\n<li>canal模拟mysql slave的交互协议，伪装自己为mysql slave，向mysql master发送dump协议</li>\n<li>mysql master收到dump请求，开始推送binary log给canal</li>\n<li>canal解析binary log对象（原始为byte流），将数据同步写入Redis。</li>\n</ol>\n<h2 id=\"44为什么阿里java手册禁止使用存储过程\">44、为什么阿里Java手册禁止使用存储过程</h2>\n<p>先看看什么是存储过程。</p>\n<p>存储过程是在大型数据库系统中，一组为了完成特定功能的SQL 语句集，它存储在数据库中，一次编译后永久有效，用户通过指定存储过程的名字并给出参数（如果该存储过程带有参数）来执行它。</p>\n<p>存储过程主要有以下几个缺点。</p>\n<ol>\n<li><strong>存储过程难以调试</strong>。存储过程的开发一直缺少有效的 IDE 环境。SQL 本身经常很长，调试时要把句子拆开分别独立执行，非常麻烦。</li>\n<li><strong>移植性差</strong>。存储过程的移植困难，一般业务系统总会不可避免地用到数据库独有的特性和语法，更换数据库时这部分代码就需要重写，成本较高。</li>\n<li><strong>管理困难</strong>。存储过程的目录是扁平的，而不是文件系统那样的树形结构，脚本少的时候还好办，一旦多起来，目录就会陷入混乱。</li>\n<li>存储过程是<strong>只优化一次</strong>，有的时候随着数据量的增加或者数据结构的变化，原来存储过程选择的执行计划也许并不是最优的了，所以这个时候需要手动干预或者重新编译了。</li>\n</ol>\n<h2 id=\"45mysql-update-是锁行还是锁表\">45、MySQL update 是锁行还是锁表？</h2>\n<p>首先，InnoDB行锁是通过给索引上的索引项加锁来实现的，只有通过索引条件检索数据，InnoDB才使用行级锁，否则，InnoDB将使用表锁。</p>\n<ol>\n<li>当执行update语句时，where中的过滤条件列，如果用到索引，就是锁行；如果无法用索引，就是锁表。</li>\n<li>如果两个update语句同时执行，第一个先执行触发行锁，但是第二个没有索引触发表锁，因为有个行锁住了，所以还是会等待行锁释放，才能锁表。</li>\n<li>当执行insert或者delete语句时，锁行。</li>\n</ol>\n<h2 id=\"46selectfor-update会锁表还是锁行\">46、select...for update会锁表还是锁行？</h2>\n<p>如果查询条件用了索引/主键，那么<code>select ... for update</code>就会加行锁。</p>\n<p>如果是普通字段(没有索引/主键)，那么<code>select ..... for update</code>就会加表锁。</p>\n<h2 id=\"47mysql的binlog有几种格式分别有什么区别\">47、MySQL的binlog有几种格式？分别有什么区别？</h2>\n<p>有三种格式，<strong>statement，row和mixed</strong>。</p>\n<ul>\n<li>statement：<strong>每一条会修改数据的sql都会记录在binlog中</strong>。不需要记录每一行的变化，减少了binlog日志量，节约了IO，提高性能。由于sql的执行是有上下文的，因此<strong>在保存的时候需要保存相关的信息</strong>，同时还有一些使用了函数之类的语句无法被记录复制。</li>\n<li>row：<strong>不记录sql语句上下文相关信息，仅保存哪条记录被修改</strong>。记录单元为每一行的改动，由于很多操作，会导致大量行的改动(比如alter table)，因此这种模式的文件保存的信息太多，日志量太大。</li>\n<li>mixed：一种折中的方案，普通操作使用statement记录，当无法使用statement的时候使用row。</li>\n</ul>\n<h2 id=\"48阿里手册为什么禁止使用-count列名或-count常量来替代-count\">48、阿里手册为什么禁止使用 count(列名)或 count(常量)来替代 count(*)</h2>\n<p>先看下这几种方式的区别。</p>\n<p>count(主键id)：InnoDB引擎会遍历整张表，把每一行id值都取出来，返给server层。server层拿到id后，判断是不可能为空的，就按行累加，不再对每个值进行NULL判断。</p>\n<p>count(常量)：InnoDB引擎会遍历整张表，但不取值。server层对于返回的每一行，放一个常量进去，判断是不可能为空的，按行累加，不再对每个值进行NULL判断。count(常量)比count(主键id)执行的要快，因为从引擎放回id会涉及解析数据行，以及拷贝字段值的操作。</p>\n<p>count(字段)：全表扫描，分情况讨论。</p>\n<p>1、如果参数字段定义NOT NULL，判断是不可能为空的，按行累加，不再对每个值进行NULL判断。 2、如果参数字段定义允许为NULL，那么执行的时候，判断可能是NULL，还要把值取出来再判断一下，不是NULL才累加。</p>\n<p>count(*)：统计所有的列，相当于行数，统计结果中会包含字段值为null的列；</p>\n<p><strong>COUNT(<code>*</code>)是SQL92定义的标准统计行数的语法，效率高，MySQL对它进行了很多优化，MyISAM中会直接把表的总行数单独记录下来供COUNT(*)查询，而InnoDB则会在扫表的时候选择最小的索引来降低成本</strong>。</p>\n<p>所以，建议使用COUNT(*)查询表的行数！</p>\n<h2 id=\"49存储md5值应该用varchar还是用char\">49、存储MD5值应该用VARCHAR还是用CHAR？</h2>\n<p>首先说说CHAR和VARCHAR的区别：</p>\n<p>1、存储长度：</p>\n<p><strong>CHAR类型的长度是固定的</strong></p>\n<p>当我们当定义CHAR(10)，输入的值是&quot;abc&quot;，但是它占用的空间一样是10个字节，会包含7个空字节。当输入的字符长度超过指定的数时，CHAR会截取超出的字符。而且，当存储为CHAR的时候，MySQL会自动删除输入字符串末尾的空格。</p>\n<p><strong>VARCHAR的长度是可变的</strong></p>\n<p>比如VARCHAR(10)，然后输入abc三个字符，那么实际存储大小为3个字节。</p>\n<p>除此之外，<strong>VARCHAR还会保留1个或2个额外的字节来记录字符串的实际长度</strong>。如果定义的最大长度小于等于255个字节，那么，就会预留1个字节；如果定义的最大长度大于255个字节，那么就会预留2个字节。</p>\n<p>2、存储效率</p>\n<p><strong>CHAR类型每次修改后的数据长度不变，效率更高</strong>。</p>\n<p>VARCHAR每次修改的数据要更新数据长度，效率更低。</p>\n<p>3、存储空间</p>\n<p>CHAR存储空间是<strong>初始的预计长度字符串再加上一个记录字符串长度的字节</strong>，可能会存在多余的空间。</p>\n<p>VARCHAR存储空间的时候是<strong>实际字符串再加上一个记录字符串长度的字节</strong>，占用空间较小。</p>\n<p>根据以上的分析，由于<strong>MD5是一个定长的值，所以MD5值适合使用CHAR存储</strong>。对于固定长度的非常短的列，CHAR比VARCHAR效率也更高。</p>\n","createTime":"2024-05-07 22:20:34","categoryId":36,"categoryName":"MySQL","readNum":83,"tags":[{"id":63,"name":"mysql","articlesTotal":null}],"preArticle":{"articleId":48,"articleTitle":"JVM"},"nextArticle":{"articleId":45,"articleTitle":"Redis"},"totalWords":22922,"readTime":"约 76 分钟","updateTime":"2024-06-03 20:17:54"}} =================================== 
2024-08-04 17:10:59.382 [http-nio-8088-exec-6] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 17:11:02.988 [http-nio-8088-exec-4] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 17:11:02.988 [http-nio-8088-exec-4] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 17:11:02.989 [http-nio-8088-exec-4] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [前台获取博客详情], 入参: , 请求类: BlogSettingsController, 请求方法: findDetail =================================== 
2024-08-04 17:11:02.989 [http-nio-8088-exec-2] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 17:11:02.989 [http-nio-8088-exec-2] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 17:11:02.991 [http-nio-8088-exec-2] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [获取文章详情], 入参: {"articleId":51}, 请求类: ArticleController, 请求方法: findArticleDetail =================================== 
2024-08-04 17:11:02.992 [http-nio-8088-exec-4] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [前台获取博客详情], 耗时: 3ms, 出参: {"success":true,"message":null,"errorCode":null,"data":{"logo":"http://110.41.141.141:9000/weblog/weblog/9853f8be13cb4f7fae00e3f5233dd688.png","name":"WebLog","author":"Jacob","introduction":"求知若饥，虚心若愚","avatar":"http://110.41.141.141:9000/weblog/weblog/cf0958d87787449fb05aae4cc84015c6.jpg","githubHomepage":"https://github.com/jdw-art","csdnHomepage":"https://www.csdn.net/?spm=1010.2135.3001.4476","giteeHomepage":"","zhihuHomepage":"https://www.zhihu.com/people/54-10-50-93"}} =================================== 
2024-08-04 17:11:02.993 [http-nio-8088-exec-4] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 17:11:03.107 [http-nio-8088-exec-2] INFO  c.j.weblog.event.subscriber.ReadArticleSubscriber - ==> threadName: http-nio-8088-exec-2
2024-08-04 17:11:03.107 [http-nio-8088-exec-2] INFO  c.j.weblog.event.subscriber.ReadArticleSubscriber - ==> 文章阅读事件消费成功，articleId: 51
2024-08-04 17:11:03.173 [http-nio-8088-exec-2] INFO  c.j.weblog.event.subscriber.ReadArticleSubscriber - ==> 文章阅读量 +1 操作成功，articleId: 51
2024-08-04 17:11:03.178 [http-nio-8088-exec-2] INFO  c.j.weblog.event.subscriber.ReadArticleSubscriber - ==> 当日文章 PV 访问量 +1 操作成功，date: 2024-08-04
2024-08-04 17:11:03.180 [http-nio-8088-exec-2] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [获取文章详情], 耗时: 187ms, 出参: {"success":true,"message":null,"errorCode":null,"data":{"title":"RedisIncrement","content":"<h2 id=\"1什么是-redis\">1、什么是 Redis</h2>\n<p>Redis 是一种基于内存的数据库，对数据的读写操作都是在内存中完成，因此<strong>读写速度非常快</strong>，常用于<strong>缓存，消息队列、分布式锁等场景</strong>。</p>\n<p>Redis 提供了多种数据类型来支持不同的业务场景，比如 String(字符串)、Hash(哈希)、 List (列表)、Set(集合)、Zset(有序集合)、Bitmaps（位图）、HyperLogLog（基数统计）、GEO（地理信息）、Stream（流），并且对数据类型的操作都是<strong>原子性</strong>的，因为执行命令由单线程负责的，不存在并发竞争的问题。</p>\n<p>除此之外，Redis 还支持<strong>事务 、持久化、Lua 脚本、多种集群方案（主从复制模式、哨兵模式、切片机群模式）、发布/订阅模式，内存淘汰机制、过期删除机制</strong>等等。</p>\n<h2 id=\"2redis-和-memcached-的区别\">2、Redis 和 Memcached 的区别</h2>\n<p>Redis 与 Memcached <strong>共同点</strong>：</p>\n<ol>\n<li>都是基于内存的数据库，一般都用来当做缓存使用。</li>\n<li>都有过期策略。</li>\n<li>两者的性能都非常高。</li>\n</ol>\n<p>Redis 与 Memcached <strong>区别</strong>：</p>\n<ul>\n<li>Redis 支持的数据类型更丰富（String、Hash、List、Set、ZSet），而 Memcached 只支持最简单的 key-value 数据类型；</li>\n<li>Redis 支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用，而 Memcached 没有持久化功能，数据全部存在内存之中，Memcached 重启或者挂掉后，数据就没了；</li>\n<li>Redis 原生支持集群模式，Memcached 没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据；</li>\n<li>Redis 支持发布订阅模型、Lua 脚本、事务等功能，而 Memcached 不支持；</li>\n</ul>\n<h2 id=\"3redis为什么这么快\">3、Redis为什么这么快？</h2>\n<ul>\n<li>Redis 的大部分操作<strong>都在内存中完成</strong>，并且采用了高效的数据结构，因此 Redis 瓶颈可能是机器的内存或者网络带宽，而并非 CPU，既然 CPU 不是瓶颈，那么自然就采用单线程的解决方案了；</li>\n<li>Redis 采用单线程模型可以<strong>避免了多线程之间的竞争</strong>，省去了多线程切换带来的时间和性能上的开销，而且也不会导致死锁问题。</li>\n<li>Redis 采用了 <strong>I/O 多路复用机制</strong>处理大量的客户端 Socket 请求，IO 多路复用机制是指一个线程处理多个 IO 流，就是我们经常听到的 select/epoll 机制。简单来说，在 Redis 只运行单线程的情况下，该机制允许内核中，同时存在多个监听 Socket 和已连接 Socket。内核会一直监听这些 Socket 上的连接请求或数据请求。一旦有请求到达，就会交给 Redis 线程处理，这就实现了一个 Redis 线程处理多个 IO 流的效果。</li>\n<li><strong>持久化机制</strong>：尽管Redis主要是一个内存<a href=\"https://cloud.tencent.com/solution/database?from_column=20065&from=20065\" ref=\"nofollow\" target=\"_blank\">数据库</a><span><svg xmlns=\"http://www.w3.org/2000/svg\" class=\"inline ml-1\" style=\"color: #aaa;\" aria-hidden=\"true\" focusable=\"false\" x=\"0px\" y=\"0px\" viewBox=\"0 0 100 100\" width=\"15\" height=\"15\" class=\"icon outbound\"><path fill=\"currentColor\" d=\"M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z\"></path> <polygon fill=\"currentColor\" points=\"45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9\"></polygon></svg> <span class=\"sr-only\"></span></span>，但它也支持将数据持久化到磁盘。Redis提供了两种持久化选项：AOF日志和RDB快照。这些机制确保了即使在服务器重启时，数据也不会丢失，从而满足了一些需要持久化数据的应用场景。</li>\n</ul>\n<h2 id=\"4redis数据结构\">4、Redis数据结构</h2>\n<h3 id=\"41redis-数据类型以及使用场景\">4.1、Redis 数据类型以及使用场景</h3>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/63981a5e635a4ec5a4400481fa1f77d8.png\">\n</p>\n<ul>\n<li>String 类型的应用场景：缓存对象、常规计数、分布式锁、共享 session 信息等。</li>\n<li>List 类型的应用场景：消息队列（但是有两个问题：1. 生产者需要自行实现全局唯一 ID；2. 不能以消费组形式消费数据）等。</li>\n<li>Hash 类型：缓存对象、购物车等。</li>\n<li>Set 类型：聚合计算（并集、交集、差集）场景，比如点赞、共同关注、抽奖活动等。</li>\n<li>Zset 类型：排序场景，比如排行榜、电话和姓名排序等。</li>\n</ul>\n<h3 id=\"42五种常见的-redis-数据类型是怎么实现\">4.2、五种常见的 Redis 数据类型是怎么实现</h3>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/839ae96d896a404fbc13fb0662704596.png\">\n</p>\n<blockquote>\n<p>String 类型内部实现</p>\n</blockquote>\n<p>String 类型的底层的数据结构实现主要是 SDS（简单动态字符串）。 SDS 和我们认识的 C 字符串不太一样，之所以没有使用 C 语言的字符串表示，因为 SDS 相比于 C 的原生字符串：</p>\n<ul>\n<li><strong>SDS 不仅可以保存文本数据，还可以保存二进制数据</strong>。因为 SDS 使用 len 属性的值而不是空字符来判断字符串是否结束，并且 SDS 的所有 API 都会以处理二进制的方式来处理 SDS 存放在 buf[] 数组里的数据。所以 SDS 不光能存放文本数据，而且能保存图片、音频、视频、压缩文件这样的二进制数据。</li>\n<li><strong>SDS 获取字符串长度的时间复杂度是 O(1)</strong>。因为 C 语言的字符串并不记录自身长度，所以获取长度的复杂度为 O(n)；而 SDS 结构里用 len 属性记录了字符串长度，所以复杂度为 O(1)。</li>\n<li><strong>Redis 的 SDS API 是安全的，拼接字符串不会造成缓冲区溢出</strong>。因为 SDS 在拼接字符串之前会检查 SDS 空间是否满足要求，如果空间不够会自动扩容，所以不会导致缓冲区溢出的问题。</li>\n</ul>\n<blockquote>\n<p>List 类型内部实现</p>\n</blockquote>\n<p>List 类型的底层数据结构是由<strong>双向链表或压缩列表</strong>实现的：</p>\n<ul>\n<li>如果列表的元素个数小于 512 个（默认值，可由 list-max-ziplist-entries 配置），列表每个元素的值都小于 64 字节（默认值，可由 list-max-ziplist-value 配置），Redis 会使用<strong>压缩列表</strong>作为 List 类型的底层数据结构；</li>\n<li>如果列表的元素不满足上面的条件，Redis 会使用<strong>双向链表</strong>作为 List 类型的底层数据结构；</li>\n</ul>\n<p>但是<strong>在 Redis 3.2 版本之后，List 数据类型底层数据结构就只由 quicklist 实现了，替代了双向链表和压缩列表</strong>。</p>\n<blockquote>\n<p>Hash 类型内部实现</p>\n</blockquote>\n<p>Hash 类型的底层数据结构是由<strong>压缩列表或哈希表</strong>实现的：</p>\n<ul>\n<li>如果哈希类型元素个数小于 512 个（默认值，可由 hash-max-ziplist-entries 配置），所有值小于 64 字节（默认值，可由 hash-max-ziplist-value 配置）的话，Redis 会使用<strong>压缩列表</strong>作为 Hash 类型的底层数据结构；</li>\n<li>如果哈希类型元素不满足上面条件，Redis 会使用<strong>哈希表</strong>作为 Hash 类型的底层数据结构。</li>\n</ul>\n<p><strong>在 Redis 7.0 中，压缩列表数据结构已经废弃了，交由 listpack 数据结构来实现了</strong>。</p>\n<blockquote>\n<p>Set 类型内部实现</p>\n</blockquote>\n<p>Set 类型的底层数据结构是由<strong>哈希表或整数集合</strong>实现的：</p>\n<ul>\n<li>如果集合中的元素都是整数且元素个数小于 512 （默认值，set-maxintset-entries配置）个，Redis 会使用<strong>整数集合</strong>作为 Set 类型的底层数据结构；</li>\n<li>如果集合中的元素不满足上面条件，则 Redis 使用<strong>哈希表</strong>作为 Set 类型的底层数据结构。</li>\n</ul>\n<blockquote>\n<p>ZSet 类型内部实现</p>\n</blockquote>\n<p>Zset 类型的底层数据结构是由<strong>压缩列表或跳表</strong>实现的：</p>\n<ul>\n<li>如果有序集合的元素个数小于 128 个，并且每个元素的值小于 64 字节时，Redis 会使用<strong>压缩列表</strong>作为 Zset 类型的底层数据结构；</li>\n<li>如果有序集合的元素不满足上面的条件，Redis 会使用<strong>跳表</strong>作为 Zset 类型的底层数据结构；</li>\n</ul>\n<h2 id=\"5redis-的单线程模型\">5、Redis 的单线程模型</h2>\n<p><strong>Redis 单线程指的是「接收客户端请求-&gt;解析请求 -&gt;进行数据读写等操作-&gt;发送数据给客户端」这个过程是由一个线程（主线程）来完成的</strong>，这也是我们常说 Redis 是单线程的原因。</p>\n<p>但是，<strong>Redis 程序并不是单线程的</strong>，Redis 在启动的时候，是会<strong>启动后台线程</strong>（BIO）的：</p>\n<ul>\n<li><strong>Redis 在 2.6 版本</strong>，会启动 2 个后台线程，分别处理关闭文件、AOF 刷盘这两个任务；</li>\n<li><strong>Redis 在 4.0 版本之后</strong>，新增了一个新的后台线程，用来异步释放 Redis 内存，也就是 lazyfree 线程。</li>\n</ul>\n<p>之所以 Redis 为「关闭文件、AOF 刷盘、释放内存」这些任务创建单独的线程来处理，是因为这些任务的操作都是很耗时的，如果把这些任务都放在主线程来处理，那么 Redis 主线程就很容易发生阻塞，这样就无法处理后续的请求了。</p>\n<h3 id=\"51redis的单线程模型的工作流程\">5.1、Redis的单线程模型的工作流程</h3>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/6c664d650d7c4711b871504692f27b3a.png\">\n</p>\n<p>图中的蓝色部分是一个事件循环，是由主线程负责的，可以看到网络 I/O 和命令处理都是单线程。 Redis 初始化的时候，会做下面这几件事情：</p>\n<ul>\n<li>首先，调用 epoll_create() 创建一个 epoll 对象和调用 socket() 创建一个服务端 socket</li>\n<li>然后，调用 bind() 绑定端口和调用 listen() 监听该 socket；</li>\n<li>然后，将调用 epoll_ctl() 将 listen socket 加入到 epoll，同时注册「连接事件」处理函数。</li>\n</ul>\n<p>初始化完后，主线程就进入到一个<strong>事件循环函数</strong>，主要会做以下事情：</p>\n<ul>\n<li>首先，先调用<strong>处理发送队列函数</strong>，看是发送队列里是否有任务，如果有发送任务，则通过 write 函数将客户端发送缓存区里的数据发送出去，如果这一轮数据没有发送完，就会注册写事件处理函数，等待 epoll_wait 发现可写后再处理 。</li>\n<li>接着，调用 epoll_wait 函数等待事件的到来：\n<ul>\n<li>如果是<strong>连接事件</strong>到来，则会调用<strong>连接事件处理函数</strong>，该函数会做这些事情：调用 accpet 获取已连接的 socket -&gt; 调用 epoll_ctl 将已连接的 socket 加入到 epoll -&gt; 注册「读事件」处理函数；</li>\n<li>如果是<strong>读事件</strong>到来，则会调用<strong>读事件处理函数</strong>，该函数会做这些事情：调用 read 获取客户端发送的数据 -&gt; 解析命令 -&gt; 处理命令 -&gt; 将客户端对象添加到发送队列 -&gt; 将执行结果写到发送缓存区等待发送；</li>\n<li>如果是<strong>写事件</strong>到来，则会调用<strong>写事件处理函数</strong>，该函数会做这些事情：通过 write 函数将客户端发送缓存区里的数据发送出去，如果这一轮数据没有发送完，就会继续注册写事件处理函数，等待 epoll_wait 发现可写后再处理 。</li>\n</ul>\n</li>\n</ul>\n<blockquote>\n<p>redis单线程指的是，接收客户端请求、解析请求、执行数据读写操作、将数据发送给客户端这一过程只有一个线程来完成，但是redis程序并不是单线程的，redis启动后，会启动几个后台线程，在redis2.6之前， 会启动两个线程用于关闭文件和AOF刷盘，在redis4.0之后，多了一个用于异步释放redis内存的线程，之所以为关闭文件、AOF刷盘、释放内存分配一个独立的线程，是因为这些任务都是耗时任务，如果把这些耗时任务都交给主线程执行，很有可能会阻塞主线程。</p>\n<p>redis的单线程模型的工作流程，首先会进行初始化阶段，在初始化阶段中，首先会调用epoll_create函数创建epoll对象，并调用socket方法创建服务端socket，然后调用bind方法绑定端口，并调用listen方法监听该socket，最后调用epoll_ctl方法将listen socket对象假如到epoll对象中，并注册写事件处理函数</p>\n<p>执行完初始化阶段后，主线程进入事件循环函数</p>\n<ul>\n<li>首先调用处理发送队列函数，调用write方法检查发送队列中是否有发送任务，如果有，则将客户端发送缓冲区中的数据发送出去，如果一轮数据未发送完，则注册写事件处理函数，等待epoll_wait发现为可写后再进行处理</li>\n<li>接着调用epoll_wait方法等待事件的到来</li>\n<li>如果是连接事件的到来，则调用连接事件处理函数，调用accepte方法接收已连接的socket，并调用epoll_ctl将socket加入到epoll对象中，接着注册读事件处理函数</li>\n<li>如果是读事件的到来，则调用读事件处理函数，调用read方法读取客户端发送来的数据，解析命令，执行命令，将客户端对象添加到发送队列，并将执行结果写入到客户端发送缓冲区中等待发送</li>\n<li>如果是写事件的到来，则调用写事件处理函数，调用write方法检查发送队列中是否有发送任务，如果有则将客户端发送缓冲区中的数据发送出去，如果一轮数据发送未完成，则继续注册写事件，等待epoll_wait发现为可见后继续执行</li>\n</ul>\n</blockquote>\n<h3 id=\"52redis-60-之前为什么使用单线程\">5.2、Redis 6.0 之前为什么使用单线程</h3>\n<p><strong>CPU 并不是制约 Redis 性能表现的瓶颈所在</strong>，更多情况下是受到内存大小和网络I/O的限制，所以 Redis 核心网络模型使用单线程并没有什么问题，如果你想要使用服务的多核CPU，可以在一台服务器上启动多个节点或者采用分片集群的方式。</p>\n<p>使用了单线程后，可维护性高，多线程模型虽然在某些方面表现优异，但是它却引入了程序执行顺序的不确定性，带来了并发读写的一系列问题，<strong>增加了系统复杂度、同时可能存在线程切换、甚至加锁解锁、死锁造成的性能损耗</strong>。</p>\n<h3 id=\"53redis-60-之后为什么引入了多线程\">5.3、Redis 6.0 之后为什么引入了多线程</h3>\n<p>虽然 Redis 的主要工作（网络 I/O 和执行命令）一直是单线程模型，但是<strong>在 Redis 6.0 版本之后，也采用了多个 I/O 线程来处理网络请求</strong>，<strong>这是因为随着网络硬件的性能提升，Redis 的性能瓶颈有时会出现在网络 I/O 的处理上</strong>。所以为了提高网络 I/O 的并行度，Redis 6.0 对于网络 I/O 采用多线程来处理。<strong>但是对于命令的执行，Redis 仍然使用单线程来处理</strong>。</p>\n<h2 id=\"6redis-持久化\">6、Redis 持久化</h2>\n<p>Redis 的读写操作都是在内存中，所以 Redis 性能才会高，但是当 Redis 重启后，内存中的数据就会丢失，那为了保证内存中的数据不会丢失，Redis 实现了数据持久化的机制，这个机制会把数据存储到磁盘，这样在 Redis 重启就能够从磁盘中恢复原有的数据。</p>\n<p>Redis 共有三种数据持久化的方式：</p>\n<ul>\n<li><strong>AOF 日志</strong>：Redis 在执行完一条写操作命令后，就会把该命令以追加的方式写入到一个文件里，然后 Redis 重启时，会读取该文件记录的命令，然后逐一执行命令的方式来进行数据恢复。</li>\n<li><strong>RDB 快照</strong>：将某一时刻的内存数据，以二进制的方式写入磁盘；</li>\n<li><strong>混合持久化方式</strong>：Redis 4.0 新增的方式，集成了 AOF 和 RBD 的优点；</li>\n</ul>\n<h3 id=\"61aof-日志\">6.1、AOF 日志</h3>\n<p>Redis 在执行完一条写操作命令后，就会把该命令以追加的方式写入到一个文件里，然后 Redis 重启时，会读取该文件记录的命令，然后逐一执行命令的方式来进行数据恢复。</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/3e7e5f6be8e241a1ad5bd6c8ea4102cd.png\">\n</p>\n<h4 id=\"611为什么先执行命令再把数据写入日志\">6.1.1、为什么先执行命令，再把数据写入日志</h4>\n<p>Reids 是先执行写操作命令后，才将该命令记录到 AOF 日志里的，这么做其实有两个好处。</p>\n<ul>\n<li><strong>避免额外的检查开销</strong>：因为如果先将写操作命令记录到 AOF 日志里，再执行该命令的话，如果当前的命令语法有问题，那么如果不进行命令语法检查，该错误的命令记录到 AOF 日志里后，Redis 在使用日志恢复数据时，就可能会出错。</li>\n<li><strong>不会阻塞当前写操作命令的执行</strong>：因为当写操作命令执行成功后，才会将命令记录到 AOF 日志。</li>\n</ul>\n<p>当然，这样做也会带来风险：</p>\n<ul>\n<li><strong>数据可能会丢失：</strong> 执行写操作命令和记录日志是两个过程，那当 Redis 在还没来得及将命令写入到硬盘时，服务器发生宕机了，这个数据就会有丢失的风险。</li>\n<li><strong>可能阻塞其他操作：</strong> 由于写操作命令执行成功后才记录到 AOF 日志，所以不会阻塞当前命令的执行，但因为 AOF 日志也是在主线程中执行，所以当 Redis 把日志文件写入磁盘的时候，还是会阻塞后续的操作无法执行。</li>\n</ul>\n<h4 id=\"612aof-写回策略\">6.1.2、AOF 写回策略</h4>\n<p>Redis 写入 AOF 日志的过程：</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/b02aa9979bd04468bdc902e76b39e8f7.png\">\n</p>\n<ol>\n<li>Redis 执行完写操作命令后，会将命令追加到 server.aof_buf 缓冲区；</li>\n<li>然后通过 write() 系统调用，将 aof_buf 缓冲区的数据写入到 AOF 文件，此时数据并没有写入到硬盘，而是拷贝到了内核缓冲区 page cache，等待内核将数据写入硬盘；</li>\n<li>具体内核缓冲区的数据什么时候写入到硬盘，由内核决定。</li>\n</ol>\n<p>Redis 提供了 3 种写回硬盘的策略，控制的就是上面说的第三步的过程。 在 Redis.conf 配置文件中的 appendfsync 配置项可以有以下 3 种参数可填：</p>\n<ul>\n<li><strong>Always</strong>，这个单词的意思是「总是」，所以它的意思是每次写操作命令执行完后，同步将 AOF 日志数据写回硬盘；</li>\n<li><strong>Everysec</strong>，这个单词的意思是「每秒」，所以它的意思是每次写操作命令执行完后，先将命令写入到 AOF 文件的内核缓冲区，然后每隔一秒将缓冲区里的内容写回到硬盘；</li>\n<li><strong>No</strong>，意味着不由 Redis 控制写回硬盘的时机，转交给操作系统控制写回的时机，也就是每次写操作命令执行完后，先将命令写入到 AOF 文件的内核缓冲区，再由操作系统决定何时将缓冲区内容写回硬盘。</li>\n</ul>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/b38fafeffba14d1290e41a32ce50fadd.png\">\n</p>\n<h4 id=\"613aof-日志过大会触发什么机制\">6.1.3、AOF 日志过大，会触发什么机制</h4>\n<p>AOF 日志是一个文件，随着执行的写操作命令越来越多，文件的大小会越来越大。 如果当 AOF 日志文件过大就会带来性能问题，比如重启 Redis 后，需要读 AOF 文件的内容以恢复数据，如果文件过大，整个恢复的过程就会很慢。</p>\n<p>所以，Redis 为了避免 AOF 文件越写越大，提供了 <strong>AOF 重写机制</strong>，当 AOF 文件的大小超过所设定的阈值后，Redis 就会启用 AOF 重写机制，来压缩 AOF 文件。</p>\n<p>AOF 重写机制是在重写时，读取当前数据库中的所有键值对，然后将每一个键值对用一条命令记录到「新的 AOF 文件」，重写工作完成后，就会将新的 AOF 文件覆盖现有的 AOF 文件，这就相当于压缩了 AOF 文件，使得 AOF 文件体积变小了。</p>\n<h4 id=\"614重写-aof-日志的过程\">6.1.4、重写 AOF 日志的过程</h4>\n<p>Redis 的<strong>重写 AOF 过程是由后台子进程 <em>bgrewriteaof</em>来完成的</strong>。\n<img src=\"http://110.41.141.141:9000/weblog/weblog/a6c938cc8a414fd7a528979dadf9ef6b.png\">\n</p>\n<p>在 bgrewriteaof 子进程执行 AOF 重写期间，主进程需要执行以下三个工作:</p>\n<ul>\n<li>执行客户端发来的命令；</li>\n<li>将执行后的写命令追加到 「AOF 缓冲区」；</li>\n<li>将执行后的写命令追加到 「AOF 重写缓冲区」；</li>\n</ul>\n<p>当子进程完成 AOF 重写工作（<em>扫描数据库中所有数据，逐一把内存数据的键值对转换成一条命令，再将命令记录到重写日志</em>）后，会向主进程发送一条信号，信号是进程间通讯的一种方式，且是异步的。</p>\n<p>主进程收到该信号后，会调用一个信号处理函数，该函数主要做以下工作：</p>\n<ul>\n<li>将 AOF 重写缓冲区中的所有内容追加到新的 AOF 的文件中，使得新旧两个 AOF 文件所保存的数据库状态一致；</li>\n<li>新的 AOF 的文件进行改名，覆盖现有的 AOF 文件。</li>\n</ul>\n<p>信号函数执行完后，主进程就可以继续像往常一样处理命令了。</p>\n<h3 id=\"62rdb-快照\">6.2、RDB 快照</h3>\n<p>因为 AOF 日志记录的是操作命令，不是实际的数据，所以用 AOF 方法做故障恢复时，需要全量把日志都执行一遍，一旦 AOF 日志非常多，势必会造成 Redis 的恢复操作缓慢。</p>\n<p>为了解决这个问题，Redis 增加了 RDB 快照。RDB 快照就是记录某一个瞬间的内存数据，记录的是实际数据，因此在 Redis 恢复数据时， RDB 恢复数据的效率会比 AOF 高些，因为直接将 RDB 文件读入内存就可以，不需要像 AOF 那样还需要额外执行操作命令的步骤才能恢复数据。</p>\n<h4 id=\"621rdb-做快照时会阻塞线程吗\">6.2.1、RDB 做快照时会阻塞线程吗</h4>\n<p>Redis 提供了两个命令来生成 RDB 文件，分别是 save 和 bgsave，他们的区别就在于是否在「主线程」里执行：</p>\n<ul>\n<li>执行了 save 命令，就会在主线程生成 RDB 文件，由于和执行操作命令在同一个线程，所以如果写入 RDB 文件的时间太长，<strong>会阻塞主线程</strong>；</li>\n<li>执行了 bgsave 命令，会创建一个子进程来生成 RDB 文件，这样可以<strong>避免主线程的阻塞</strong>；</li>\n</ul>\n<blockquote>\n<p>Redis 的快照是<strong>全量快照</strong>，也就是说每次执行快照，都是把内存中的「所有数据」都记录到磁盘中。所以执行快照是一个比较重的操作，如果频率太频繁，可能会对 Redis 性能产生影响。如果频率太低，服务器故障时，丢失的数据会更多。</p>\n</blockquote>\n<h4 id=\"622rdb-在执行快照的时候数据能修改吗\">6.2.2、RDB 在执行快照的时候，数据能修改吗</h4>\n<p>执行 bgsave 过程中，Redis 依然<strong>可以继续处理操作命令</strong>的，也就是数据是能被修改的，关键的技术就在于<strong>写时复制技术（Copy-On-Write, COW）。</strong></p>\n<p>执行 bgsave 命令的时候，会通过 fork() 创建子进程，此时子进程和父进程是共享同一片内存数据的，因为创建子进程的时候，会复制父进程的页表，但是页表指向的物理内存还是一个，此时如果主线程执行读操作，则主线程和 bgsave 子进程互相不影响。如果主线程执行写操作，则被修改的数据会复制一份副本，然后 bgsave 子进程会把该副本数据写入 RDB 文件，在这个过程中，主线程仍然可以直接修改原来的数据。</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/fae8c9a881dc47e9838a6830768fa137.png\">\n</p>\n<h3 id=\"63混合持久化\">6.3、混合持久化</h3>\n<p>RDB 优点是数据恢复速度快，但是快照的频率不好把握。频率太低，丢失的数据就会比较多，频率太高，就会影响性能。AOF 优点是丢失数据少，但是数据恢复不快。</p>\n<p>为了集成了两者的优点， Redis 4.0 提出了<strong>混合使用 AOF 日志和内存快照</strong>，也叫混合持久化，既保证了 Redis 重启速度，又降低数据丢失风险。</p>\n<p>混合持久化工作在 <strong>AOF 日志重写过程</strong>，当开启了混合持久化时，在 AOF 重写日志时，fork 出来的重写子进程会先将与主线程共享的内存数据以 RDB 方式写入到 AOF 文件，然后主线程处理的操作命令会被记录在重写缓冲区里，重写缓冲区里的增量命令会以 AOF 方式写入到 AOF 文件，写入完成后通知主进程将新的含有 RDB 格式和 AOF 格式的 AOF 文件替换旧的的 AOF 文件。</p>\n<p>也就是说，使用了混合持久化，AOF 文件的<strong>前半部分是 RDB 格式的全量数据，后半部分是 AOF 格式的增量数据</strong>。</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/590758a18ec04a6a94d31391643c8aaa.png\">\n</p>\n<p>这样的好处在于，重启 Redis 加载数据的时候，由于前半部分是 RDB 内容，这样<strong>加载的时候速度会很快</strong>。</p>\n<p>加载完 RDB 的内容后，才会加载后半部分的 AOF 内容，这里的内容是 Redis 后台子进程重写 AOF 期间，主线程处理的操作命令，可以使得<strong>数据更少的丢失</strong>。</p>\n<p><strong>混合持久化优点：</strong></p>\n<ul>\n<li>混合持久化结合了 RDB 和 AOF 持久化的优点，开头为 RDB 的格式，使得 Redis 可以更快的启动，同时结合 AOF 的优点，有减低了大量数据丢失的风险。</li>\n</ul>\n<p><strong>混合持久化缺点：</strong></p>\n<ul>\n<li>AOF 文件中添加了 RDB 格式的内容，使得 AOF 文件的可读性变得很差；</li>\n<li>兼容性差，如果开启混合持久化，那么此混合持久化 AOF 文件，就不能用在 Redis 4.0 之前版本了。</li>\n</ul>\n<h2 id=\"7redis-集群\">7、Redis 集群</h2>\n<h3 id=\"71redis-如何实现服务高可用\">7.1、Redis 如何实现服务高可用</h3>\n<p>要想设计一个高可用的 Redis 服务，一定要从 Redis 的多服务节点来考虑，比如 Redis 的主从复制、哨兵模式、切片集群。</p>\n<h4 id=\"711主从复制\">7.1.1、主从复制</h4>\n<p>主从复制是 Redis 高可用服务的最基础的保证，实现方案就是将从前的一台 Redis 服务器，同步数据到多台从 Redis 服务器上，即一主多从的模式，且主从服务器之间采用的是「读写分离」的方式。</p>\n<p>主服务器可以进行读写操作，当发生写操作时自动将写操作同步给从服务器，而从服务器一般是只读，并接受主服务器同步过来写操作命令，然后执行这条命令。也就是说，所有的数据修改只在主服务器上进行，然后将最新的数据同步给从服务器，这样就使得主从服务器的数据是一致的。</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/5a19d67227e14334a9b4f23dc6f31fda.png\">\n</p>\n<blockquote>\n<p>注意，主从服务器之间的命令复制是<strong>异步</strong>进行的。具体来说，在主从服务器命令传播阶段，主服务器收到新的写命令后，会发送给从服务器。但是，主服务器并不会等到从服务器实际执行完命令后，再把结果返回给客户端，而是主服务器自己在本地执行完命令后，就会向客户端返回结果了。如果从服务器还没有执行主服务器同步过来的命令，主从服务器间的数据就不一致了。所以，无法实现强一致性保证（主从数据时时刻刻保持一致），数据不一致是难以避免的。</p>\n</blockquote>\n<h4 id=\"712哨兵模式\">7.1.2、哨兵模式</h4>\n<p>在使用 Redis 主从服务的时候，会有一个问题，就是当 Redis 的主从服务器出现故障宕机时，需要手动进行恢复。为了解决这个问题，Redis 增加了哨兵模式（<strong>Redis Sentinel</strong>），因为哨兵模式做到了可以监控主从服务器，并且提供<strong>主从节点故障转移的功能。</strong></p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/b5c450318fa947aeacb67543705a8b89.png\">\n</p>\n<h4 id=\"713切片集群模式\">7.1.3、切片集群模式</h4>\n<p>当 Redis 缓存数据量大到一台服务器无法缓存时，就需要使用 <strong>Redis 切片集群</strong>（Redis Cluster ）方案，它将数据分布在不同的服务器上，以此来降低系统对单主节点的依赖，从而提高 Redis 服务的读写性能。</p>\n<p>Redis Cluster 方案采用哈希槽（Hash Slot），来处理数据和节点之间的映射关系。在 Redis Cluster 方案中，<strong>一个切片集群共有 16384 个哈希槽</strong>，这些哈希槽类似于数据分区，每个键值对都会根据它的 key，被映射到一个哈希槽中，具体执行过程分为两大步：</p>\n<ul>\n<li>根据键值对的 key，按照 <a href=\"https://en.wikipedia.org/wiki/Cyclic_redundancy_check\" ref=\"nofollow\" target=\"_blank\">CRC16 算法 (opens new window)</a><span><svg xmlns=\"http://www.w3.org/2000/svg\" class=\"inline ml-1\" style=\"color: #aaa;\" aria-hidden=\"true\" focusable=\"false\" x=\"0px\" y=\"0px\" viewBox=\"0 0 100 100\" width=\"15\" height=\"15\" class=\"icon outbound\"><path fill=\"currentColor\" d=\"M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z\"></path> <polygon fill=\"currentColor\" points=\"45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9\"></polygon></svg> <span class=\"sr-only\"></span></span>计算一个 16 bit 的值。</li>\n<li>再用 16bit 值对 16384 取模，得到 0~16383 范围内的模数，每个模数代表一个相应编号的哈希槽。</li>\n</ul>\n<p>接下来的问题就是，这些哈希槽怎么被映射到具体的 Redis 节点上的呢？有两种方案：</p>\n<ul>\n<li><strong>平均分配：</strong> 在使用 cluster create 命令创建 Redis 集群时，Redis 会自动把所有哈希槽平均分布到集群节点上。比如集群中有 9 个节点，则每个节点上槽的个数为 16384/9 个。</li>\n<li><strong>手动分配：</strong> 可以使用 cluster meet 命令手动建立节点间的连接，组成集群，再使用 cluster addslots 命令，指定每个节点上的哈希槽个数。</li>\n</ul>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/654046e286454340ad06066363422073.png\">\n</p>\n<h3 id=\"72集群脑裂\">7.2、集群脑裂</h3>\n<p>在 Redis 主从架构中，部署方式一般是「一主多从」，主节点提供写操作，从节点提供读操作。 如果主节点的网络突然发生了问题，它与所有的从节点都失联了，但是此时的主节点和客户端的网络是正常的，这个客户端并不知道 Redis 内部已经出现了问题，还在照样的向这个失联的主节点写数据（过程A），此时这些数据被旧主节点缓存到了缓冲区里，因为主从节点之间的网络问题，这些数据都是无法同步给从节点的。</p>\n<p>这时，哨兵也发现主节点失联了，它就认为主节点挂了（但实际上主节点正常运行，只是网络出问题了），于是哨兵就会在「从节点」中选举出一个 leader 作为主节点，这时集群就有两个主节点了 —— <strong>脑裂出现了</strong>。</p>\n<blockquote>\n<p>由于网络问题，集群节点之间失去联系。主从数据不同步；重新平衡选举，产生两个主服务。等网络恢复，旧主节点会降级为从节点，再与新主节点进行同步复制的时候，由于会从节点会清空自己的缓冲区，所以导致之前客户端写入的数据丢失了。</p>\n</blockquote>\n<p><strong>解决方案</strong>：</p>\n<p>当主节点发现从节点下线或者通信超时的总数量小于阈值时，那么禁止主节点进行写数据，直接把错误返回给客户端。</p>\n<p>在 Redis 的配置文件中有两个参数我们可以设置：</p>\n<ul>\n<li>min-slaves-to-write x，主节点必须要有至少 x 个从节点连接，如果小于这个数，主节点会禁止写数据。</li>\n<li>min-slaves-max-lag x，主从数据复制和同步的延迟不能超过 x 秒，如果超过，主节点会禁止写数据。</li>\n</ul>\n<p>我们可以把 min-slaves-to-write 和 min-slaves-max-lag 这两个配置项搭配起来使用，分别给它们设置一定的阈值，假设为 N 和 T。</p>\n<p>这两个配置项组合后的要求是，主库连接的从库中至少有 N 个从库，和主库进行数据复制时的 ACK 消息延迟不能超过 T 秒，否则，主库就不会再接收客户端的写请求了。</p>\n<p>即使原主库是假故障，它在假故障期间也无法响应哨兵心跳，也不能和从库进行同步，自然也就无法和从库进行 ACK 确认了。这样一来，min-slaves-to-write 和 min-slaves-max-lag 的组合要求就无法得到满足，<strong>原主库就会被限制接收客户端写请求，客户端也就不能在原主库中写入新数据了</strong>。</p>\n<p><strong>等到新主库上线时，就只有新主库能接收和处理客户端请求，此时，新写的数据会被直接写到新主库中。而原主库会被哨兵降为从库，即使它的数据被清空了，也不会有新数据丢失。</strong></p>\n<h2 id=\"8redis-过期删除与内存淘汰\">8、Redis 过期删除与内存淘汰</h2>\n<h3 id=\"81过期删除策略\">8.1、过期删除策略</h3>\n<p>Redis 是可以对 key 设置过期时间的，因此需要有相应的机制将已过期的键值对删除，而做这个工作的就是过期键值删除策略。</p>\n<p>每当我们对一个 key 设置了过期时间时，Redis 会把该 key 带上过期时间存储到一个<strong>过期字典</strong>（expires dict）中，也就是说「过期字典」保存了数据库中所有 key 的过期时间。</p>\n<p>当我们查询一个 key 时，Redis 首先检查该 key 是否存在于过期字典中：</p>\n<ul>\n<li>如果不在，则正常读取键值；</li>\n<li>如果存在，则会获取该 key 的过期时间，然后与当前系统时间进行比对，如果比系统时间大，那就没有过期，否则判定该 key 已过期。</li>\n</ul>\n<p>Redis 使用的过期删除策略是「<strong>惰性删除+定期删除</strong>」这两种策略配和使用。</p>\n<h4 id=\"811惰性删除策略\">8.1.1、惰性删除策略</h4>\n<p>惰性删除策略的做法是，<strong>不主动删除过期键，每次从数据库访问 key 时，都检测 key 是否过期，如果过期则删除该 key。</strong></p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/6c88cdc07b334d54892cee0b197817b5.png\">\n</p>\n<p>惰性删除策略的<strong>优点</strong>：</p>\n<ul>\n<li>因为每次访问时，才会检查 key 是否过期，所以此策略只会使用很少的系统资源，因此，惰性删除策略对 CPU 时间最友好。</li>\n</ul>\n<p>惰性删除策略的<strong>缺点</strong>：</p>\n<ul>\n<li>如果一个 key 已经过期，而这个 key 又仍然保留在数据库中，那么只要这个过期 key 一直没有被访问，它所占用的内存就不会释放，造成了一定的内存空间浪费。所以，惰性删除策略对内存不友好。</li>\n</ul>\n<h4 id=\"812定期删除策略\">8.1.2、定期删除策略</h4>\n<p>定期删除策略的做法是，<strong>每隔一段时间「随机」从数据库中取出一定数量的 key 进行检查，并删除其中的过期key。</strong></p>\n<p>Redis 的定期删除的流程：</p>\n<ol>\n<li>从过期字典中随机抽取 20 个 key；</li>\n<li>检查这 20 个 key 是否过期，并删除已过期的 key；</li>\n<li>如果本轮检查的已过期 key 的数量，超过 5 个（20/4），也就是「已过期 key 的数量」占比「随机抽取 key 的数量」大于 25%，则继续重复步骤 1；如果已过期的 key 比例小于 25%，则停止继续删除过期 key，然后等待下一轮再检查。</li>\n</ol>\n<p>可以看到，定期删除是一个循环的流程。那 Redis 为了保证定期删除不会出现循环过度，导致线程卡死现象，为此增加了定期删除循环流程的时间上限，默认不会超过 25ms。</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/6d78ed93891d40d2ab6129f12167e4ca.png\">\n</p>\n<p>定期删除策略的<strong>优点</strong>：</p>\n<ul>\n<li>通过限制删除操作执行的时长和频率，来减少删除操作对 CPU 的影响，同时也能删除一部分过期的数据减少了过期键对空间的无效占用。</li>\n</ul>\n<p>定期删除策略的<strong>缺点</strong>：</p>\n<ul>\n<li>难以确定删除操作执行的时长和频率。如果执行的太频繁，就会对 CPU 不友好；如果执行的太少，那又和惰性删除一样了，过期 key 占用的内存不会及时得到释放。</li>\n</ul>\n<p>可以看到，惰性删除策略和定期删除策略都有各自的优点，所以 <strong>Redis 选择「惰性删除+定期删除」这两种策略配和使用</strong>，以求在合理使用 CPU 时间和避免内存浪费之间取得平衡。</p>\n<h3 id=\"82redis-持久化时对过期键会如何处理\">8.2、Redis 持久化时，对过期键会如何处理</h3>\n<p>Redis 持久化文件有两种格式：RDB（Redis Database）和 AOF（Append Only File），下面我们分别来看过期键在这两种格式中的呈现状态。</p>\n<p>RDB 文件分为两个阶段，RDB 文件生成阶段和加载阶段。</p>\n<ul>\n<li>\n<p><strong>RDB 文件生成阶段</strong>：从内存状态持久化成 RDB（文件）的时候，会对 key 进行过期检查，<strong>过期的键「不会」被保存到新的 RDB 文件中</strong>，因此 Redis 中的过期键不会对生成新 RDB 文件产生任何影响。</p>\n</li>\n<li>\n<p>RDB 加载阶段：RDB 加载阶段时，要看服务器是主服务器还是从服务器，分别对应以下两种情况：</p>\n<ul>\n<li><strong>如果 Redis 是「主服务器」运行模式的话，在载入 RDB 文件时，程序会对文件中保存的键进行检查，过期键「不会」被载入到数据库中</strong>。所以过期键不会对载入 RDB 文件的主服务器造成影响；</li>\n<li><strong>如果 Redis 是「从服务器」运行模式的话，在载入 RDB 文件时，不论键是否过期都会被载入到数据库中</strong>。但由于主从服务器在进行数据同步时，从服务器的数据会被清空。所以一般来说，过期键对载入 RDB 文件的从服务器也不会造成影响。</li>\n</ul>\n</li>\n</ul>\n<p>AOF 文件分为两个阶段，AOF 文件写入阶段和 AOF 重写阶段。</p>\n<ul>\n<li><strong>AOF 文件写入阶段</strong>：当 Redis 以 AOF 模式持久化时，<strong>如果数据库某个过期键还没被删除，那么 AOF 文件会保留此过期键，当此过期键被删除后，Redis 会向 AOF 文件追加一条 DEL 命令来显式地删除该键值</strong>。</li>\n<li><strong>AOF 重写阶段</strong>：执行 AOF 重写时，会对 Redis 中的键值对进行检查，<strong>已过期的键不会被保存到重写后的 AOF 文件中</strong>，因此不会对 AOF 重写造成任何影响。</li>\n</ul>\n<h3 id=\"83redis-主从模式中对过期键会如何处理\">8.3、Redis 主从模式中，对过期键会如何处理</h3>\n<p>当 Redis 运行在主从模式下时，<strong>从库不会进行过期扫描，从库对过期的处理是被动的</strong>。也就是即使从库中的 key 过期了，如果有客户端访问从库时，依然可以得到 key 对应的值，像未过期的键值对一样返回。</p>\n<p>从库的过期键处理依靠主服务器控制，<strong>主库在 key 到期时，会在 AOF 文件里增加一条 del 指令，同步到所有的从库</strong>，从库通过执行这条 del 指令来删除过期的 key。</p>\n<h3 id=\"84内存淘汰策略\">8.4、内存淘汰策略</h3>\n<p>在 Redis 的运行内存达到了某个阀值，就会触发<strong>内存淘汰机制</strong>，这个阀值就是我们设置的最大运行内存，此值在 Redis 的配置文件中可以找到，配置项为 maxmemory。</p>\n<p>Redis 内存淘汰策略共有八种，这八种策略大体分为「不进行数据淘汰」和「进行数据淘汰」两类策略。</p>\n<p><em><strong>1、不进行数据淘汰的策略</strong></em></p>\n<p><strong>noeviction</strong>（Redis3.0之后，默认的内存淘汰策略） ：它表示当运行内存超过最大设置内存时，不淘汰任何数据，而是不再提供服务，直接返回错误。</p>\n<p><em><strong>2、进行数据淘汰的策略</strong></em></p>\n<p>针对「进行数据淘汰」这一类策略，又可以细分为「在设置了过期时间的数据中进行淘汰」和「在所有数据范围内进行淘汰」这两类策略。 在设置了过期时间的数据中进行淘汰：</p>\n<ul>\n<li><strong>volatile-random</strong>：随机淘汰设置了过期时间的任意键值；</li>\n<li><strong>volatile-ttl</strong>：优先淘汰更早过期的键值。</li>\n<li><strong>volatile-lru</strong>（Redis3.0 之前，默认的内存淘汰策略）：淘汰所有设置了过期时间的键值中，最久未使用的键值；</li>\n<li><strong>volatile-lfu</strong>（Redis 4.0 后新增的内存淘汰策略）：淘汰所有设置了过期时间的键值中，最少使用的键值；</li>\n</ul>\n<p>在所有数据范围内进行淘汰：</p>\n<ul>\n<li><strong>allkeys-random</strong>：随机淘汰任意键值;</li>\n<li><strong>allkeys-lru</strong>：淘汰整个键值中最久未使用的键值；</li>\n<li><strong>allkeys-lfu</strong>（Redis 4.0 后新增的内存淘汰策略）：淘汰整个键值中最少使用的键值。</li>\n</ul>\n<h3 id=\"85lru-算法和-lfu-算法\">8.5、LRU 算法和 LFU 算法</h3>\n<blockquote>\n<p>什么是 LRU 算法？</p>\n</blockquote>\n<p><strong>LRU</strong> 全称是 Least Recently Used 翻译为<strong>最近最少使用</strong>，会选择淘汰最近最少使用的数据。</p>\n<p>传统 LRU 算法的实现是基于「链表」结构，链表中的元素按照操作顺序从前往后排列，最新操作的键会被移动到表头，当需要内存淘汰时，只需要删除链表尾部的元素即可，因为链表尾部的元素就代表最久未被使用的元素。</p>\n<p>Redis 并没有使用这样的方式实现 LRU 算法，因为传统的 LRU 算法存在两个问题：</p>\n<ul>\n<li>需要用链表管理所有的缓存数据，这会带来额外的空间开销；</li>\n<li>当有数据被访问时，需要在链表上把该数据移动到头端，如果有大量数据被访问，就会带来很多链表移动操作，会很耗时，进而会降低 Redis 缓存性能。</li>\n</ul>\n<blockquote>\n<p>Redis 是如何实现 LRU 算法的？</p>\n</blockquote>\n<p><em>Redis 实现的是一种<strong>近似 LRU 算法</strong>，目的是为了更好的节约内存，它的<strong>实现方式是在 Redis 的对象结构体中添加一个额外的字段，用于记录此数据的最后一次访问时间</strong>。</em></p>\n<p><em>当 Redis 进行内存淘汰时，会使用<strong>随机采样的方式来淘汰数据</strong>，它是随机取 5 个值（此值可配置），然后<strong>淘汰最久没有使用的那个</strong>。</em></p>\n<p>Redis 实现的 LRU 算法的优点：</p>\n<ul>\n<li>不用为所有的数据维护一个大链表，节省了空间占用；</li>\n<li>不用在每次数据访问时都移动链表项，提升了缓存的性能；</li>\n</ul>\n<p>但是 LRU 算法有一个问题，<strong>无法解决缓存污染问题</strong>，比如应用一次读取了大量的数据，而这些数据只会被读取这一次，那么这些数据会留存在 Redis 缓存中很长一段时间，造成缓存污染。</p>\n<p>因此，在 Redis 4.0 之后引入了 LFU 算法来解决这个问题。</p>\n<blockquote>\n<p>什么是 LFU 算法？</p>\n</blockquote>\n<p>LFU 全称是 Least Frequently Used 翻译为<strong>最近最不常用的</strong>，LFU 算法是根据数据访问次数来淘汰数据的，它的核心思想是“如果数据过去被访问多次，那么将来被访问的频率也更高”。</p>\n<p>所以， LFU 算法会记录每个数据的访问次数。当一个数据被再次访问时，就会增加该数据的访问次数。这样就解决了偶尔被访问一次之后，数据留存在缓存中很长一段时间的问题，相比于 LRU 算法也更合理一些。</p>\n<blockquote>\n<p>Redis 是如何实现 LFU 算法的？</p>\n</blockquote>\n<p>LFU 算法相比于 LRU 算法的实现，多记录了「数据的访问频次」的信息。Redis 对象的结构如下：</p>\n<pre><code class=\"language-c\">typedef struct redisObject {\n    ...\n      \n    // 24 bits，用于记录对象的访问信息\n    unsigned lru:24;  \n    ...\n} robj;\n</code></pre>\n<p>Redis 对象头中的 lru 字段，在 LRU 算法下和 LFU 算法下使用方式并不相同。</p>\n<p><strong>在 LRU 算法中</strong>，Redis 对象头的 24 bits 的 lru 字段是用来记录 key 的访问时间戳，因此在 LRU 模式下，Redis可以根据对象头中的 lru 字段记录的值，来比较最后一次 key 的访问时间长，从而淘汰最久未被使用的 key。</p>\n<p><strong>在 LFU 算法中</strong>，Redis对象头的 24 bits 的 lru 字段被分成两段来存储，高 16bit 存储 ldt(Last Decrement Time)，用来记录 key 的访问时间戳；低 8bit 存储 logc(Logistic Counter)，用来记录 key 的访问频次。</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/154e5f81b95e46f3b4498c9e7465bb52.png\">\n</p>\n<h2 id=\"9redis-缓存设计\">9、Redis 缓存设计</h2>\n<h3 id=\"91缓存雪崩\">9.1、缓存雪崩</h3>\n<p>当大量缓存数据在同一时间过期（失效）时，如果此时有大量的用户请求，都无法在 Redis 中处理，于是全部请求都直接访问数据库，从而导致数据库的压力骤增，严重的会造成数据库宕机，从而形成一系列连锁反应，造成整个系统崩溃，这就是缓存雪崩的问题。</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/cf32e5a26bb54739a54ed074590aa588.png\">\n</p>\n<p>对于缓存雪崩问题，我们可以采用两种方案解决。</p>\n<ul>\n<li><strong>将缓存失效时间随机打散：</strong> 我们可以在原有的失效时间基础上增加一个随机值（比如 1 到 10 分钟）这样每个缓存的过期时间都不重复了，也就降低了缓存集体失效的概率。</li>\n<li><strong>互斥锁方案</strong>（Redis 中使用 setNX 方法设置一个状态位，表示这是一种锁定状态），保证同一时间只有一个业务线程请求缓存，未能获取互斥锁的请求，要么等待锁释放后重新读取缓存，要么就返回空值或者默认值。</li>\n<li><strong>设置缓存不过期：</strong> 我们可以通过后台服务来更新缓存数据，不给热点数据设置过期时间，由后台异步更新缓存，或者在热点数据准备要过期前，提前通知后台线程更新缓存以及重新设置过期时间,从而避免因为缓存失效造成的缓存雪崩，也可以在一定程度上避免缓存并发问题。</li>\n</ul>\n<h3 id=\"92缓存击穿\">9.2、缓存击穿</h3>\n<p>如果缓存中的<strong>某个热点数据过期</strong>了，此时大量的请求访问了该热点数据，就无法从缓存中读取，直接访问数据库，数据库很容易就被高并发的请求冲垮，这就是<strong>缓存击穿</strong>的问题。</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/ebcbae64b54d4b618fa1c555caa0ee0c.png\">\n</p>\n<p>可以发现缓存击穿跟缓存雪崩很相似，你可以认为缓存击穿是缓存雪崩的一个子集。 应对缓存击穿可以采取前面说到两种方案：</p>\n<ul>\n<li>互斥锁方案（Redis 中使用 setNX 方法设置一个状态位，表示这是一种锁定状态），保证同一时间只有一个业务线程请求缓存，未能获取互斥锁的请求，要么等待锁释放后重新读取缓存，要么就返回空值或者默认值。</li>\n<li>不给热点数据设置过期时间，由后台异步更新缓存，或者在热点数据准备要过期前，提前通知后台线程更新缓存以及重新设置过期时间；</li>\n</ul>\n<h3 id=\"93缓存穿透\">9.3、缓存穿透</h3>\n<p>当用户访问的数据，<strong>既不在缓存中，也不在数据库中</strong>，导致请求在访问缓存时，发现缓存缺失，再去访问数据库时，发现数据库中也没有要访问的数据，没办法构建缓存数据，来服务后续的请求。那么当有大量这样的请求到来时，数据库的压力骤增，这就是<strong>缓存穿透</strong>的问题。</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/7af294ec079c441fb29d745df711af89.png\">\n</p>\n<p>缓存穿透的发生一般有这两种情况：</p>\n<ul>\n<li>业务误操作，缓存中的数据和数据库中的数据都被误删除了，所以导致缓存和数据库中都没有数据；</li>\n<li>黑客恶意攻击，故意大量访问某些读取不存在数据的业务；</li>\n</ul>\n<p>应对缓存穿透的方案，常见的方案有三种。</p>\n<ul>\n<li><strong>非法请求的限制</strong>：在 API 入口处我们要判断请求参数是否合理，请求参数是否含有非法值、请求字段是否存在，如果判断出是恶意请求就直接返回错误，避免进一步访问缓存和数据库。</li>\n<li><strong>设置空值或者默认值</strong>：当我们线上业务发现缓存穿透的现象时，可以针对查询的数据，在缓存中设置一个空值或者默认值，这样后续请求就可以从缓存中读取到空值或者默认值，返回给应用，而不会继续查询数据库。</li>\n<li><strong>使用布隆过滤器快速判断数据是否存在，避免通过查询数据库来判断数据是否存在</strong>：我们可以在写入数据库数据时，使用布隆过滤器做个标记，然后在用户请求到来时，业务线程确认缓存失效后，可以通过查询布隆过滤器快速判断数据是否存在，如果不存在，就不用通过查询数据库来判断数据是否存在，即使发生了缓存穿透，大量请求只会查询 Redis 和布隆过滤器，而不会查询数据库，保证了数据库能正常运行，Redis 自身也是支持布隆过滤器的。</li>\n</ul>\n<h3 id=\"94布隆过滤器\">9.4、布隆过滤器</h3>\n<p>布隆过滤器由「初始值都为 0 的位图数组」和「 N 个哈希函数」两部分组成。当我们在写入数据库数据时，在布隆过滤器里做个标记，这样下次查询数据是否在数据库时，只需要查询布隆过滤器，如果查询到数据没有被标记，说明不在数据库中。</p>\n<p>布隆过滤器会通过 3 个操作完成标记：</p>\n<ul>\n<li>第一步，使用 N 个哈希函数分别对数据做哈希计算，得到 N 个哈希值；</li>\n<li>第二步，将第一步得到的 N 个哈希值对位图数组的长度取模，得到每个哈希值在位图数组的对应位置。</li>\n<li>第三步，将每个哈希值在位图数组的对应位置的值设置为 1；</li>\n</ul>\n<p>举个例子，假设有一个位图数组长度为 8，哈希函数 3 个的布隆过滤器。</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/4493388e10ca4153a3feda8291e9ab76.png\">\n</p>\n<p>在数据库写入数据 x 后，把数据 x 标记在布隆过滤器时，数据 x 会被 3 个哈希函数分别计算出 3 个哈希值，然后在对这 3 个哈希值对 8 取模，假设取模的结果为 1、4、6，然后把位图数组的第 1、4、6 位置的值设置为 1。<strong>当应用要查询数据 x 是否数据库时，通过布隆过滤器只要查到位图数组的第 1、4、6 位置的值是否全为 1，只要有一个为 0，就认为数据 x 不在数据库中</strong>。</p>\n<p>布隆过滤器由于是基于哈希函数实现查找的，高效查找的同时<strong>存在哈希冲突的可能性</strong>，比如数据 x 和数据 y 可能都落在第 1、4、6 位置，而事实上，可能数据库中并不存在数据 y，存在误判的情况。</p>\n<p>所以，<strong>查询布隆过滤器说数据存在，并不一定证明数据库中存在这个数据，但是查询到数据不存在，数据库中一定就不存在这个数据</strong>。</p>\n<h3 id=\"95如何设计一个缓存策略可以动态缓存热点数据\">9.5、如何设计一个缓存策略，可以动态缓存热点数据</h3>\n<p>由于数据存储受限，系统并不是将所有数据都需要存放到缓存中的，而<strong>只是将其中一部分热点数据缓存起来</strong>，所以我们要设计一个热点数据动态缓存的策略。</p>\n<p>热点数据动态缓存的策略总体思路：<strong>通过数据最新访问时间来做排名，并过滤掉不常访问的数据，只留下经常访问的数据</strong>。</p>\n<p>以电商平台场景中的例子，现在要求只缓存用户经常访问的 Top 1000 的商品。具体细节如下：</p>\n<ul>\n<li>先通过缓存系统做一个排序队列（比如存放 1000 个商品），系统会根据商品的访问时间，更新队列信息，越是最近访问的商品排名越靠前；</li>\n<li>同时系统会定期过滤掉队列中排名最后的 200 个商品，然后再从数据库中随机读取出 200 个商品加入队列中；</li>\n<li>这样当请求每次到达的时候，会先从队列中获取商品 ID，如果命中，就根据 ID 再从另一个缓存数据结构中读取实际的商品信息，并返回。</li>\n</ul>\n<p>在 Redis 中可以用 zadd 方法和 zrange 方法来完成排序队列和获取 200 个商品的操作。</p>\n<h3 id=\"96缓存更新策略\">9.6、缓存更新策略</h3>\n<p>常见的缓存更新策略共有3种：</p>\n<ul>\n<li>Cache Aside（旁路缓存）策略；</li>\n<li>Read/Write Through（读穿 / 写穿）策略；</li>\n<li>Write Back（写回）策略；</li>\n</ul>\n<p>实际开发中，Redis 和 MySQL 的更新策略用的是 Cache Aside，另外两种策略应用不了。</p>\n<h4 id=\"961cache-aside旁路缓存策略\">9.6.1、Cache Aside（旁路缓存）策略</h4>\n<p>Cache Aside（旁路缓存）策略是最常用的，应用程序直接与「数据库、缓存」交互，并负责对缓存的维护，该策略又可以细分为「读策略」和「写策略」。</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/6eb163a1763842fe98a4e01f16c6c287.png\">\n</p>\n<p><strong>写策略的步骤：</strong></p>\n<ul>\n<li>先更新数据库中的数据，再删除缓存中的数据。</li>\n</ul>\n<p><strong>读策略的步骤：</strong></p>\n<ul>\n<li>如果读取的数据命中了缓存，则直接返回数据；</li>\n<li>如果读取的数据没有命中缓存，则从数据库中读取数据，然后将数据写入到缓存，并且返回给用户。</li>\n</ul>\n<p>注意，写策略的步骤的顺序不能倒过来，即<strong>不能先删除缓存再更新数据库</strong>，原因是在「读+写」并发的时候，会出现缓存和数据库的数据不一致性的问题。</p>\n<p>举个例子，假设某个用户的年龄是 20，请求 A 要更新用户年龄为 21，所以它会删除缓存中的内容。这时，另一个请求 B 要读取这个用户的年龄，它查询缓存发现未命中后，会从数据库中读取到年龄为 20，并且写入到缓存中，然后请求 A 继续更改数据库，将用户的年龄更新为 21。</p>\n<blockquote>\n<p><em><strong>先删除缓存再更新数据库</strong></em>：请求A先执行删除缓存的操作，随后请求B向缓存中查询数据但是未命中缓存，转向数据库查询数据并将数据写入到缓存中，之后请求A向数据库中更新数据，这样就会出现缓存和数据库中数据不一致的情况。</p>\n</blockquote>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/276ebb2ec21d467a898863d5276916a1.png\">\n</p>\n<p>最终，该用户年龄在缓存中是 20（旧值），在数据库中是 21（新值），缓存和数据库的数据不一致。</p>\n<p><strong>为什么「先更新数据库再删除缓存」不会有数据不一致的问题？</strong></p>\n<p>继续用「读 + 写」请求的并发的场景来分析。</p>\n<p>假如某个用户数据在缓存中不存在，请求 A 读取数据时从数据库中查询到年龄为 20，在未写入缓存中时另一个请求 B 更新数据。它更新数据库中的年龄为 21，并且清空缓存。这时请求 A 把从数据库中读到的年龄为 20 的数据写入到缓存中。</p>\n<blockquote>\n<p><em><strong>先更新数据库再删除缓存</strong></em>：请求A向缓存中查询数据但是未命中缓存，转向数据库查询数据，在将数据写入到缓存之前，此时请求B向数据库中更新数据并删除缓存中的数据，这一操作结束后，请求A才向缓存中写入数据，此时就会出现缓存和数据库中数据不一致的情况。</p>\n</blockquote>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/fce0d6b43b9d4616937bba464932fecf.png\">\n</p>\n<p>最终，该用户年龄在缓存中是 20（旧值），在数据库中是 21（新值），缓存和数据库数据不一致。 从上面的理论上分析，先更新数据库，再删除缓存也是会出现数据不一致性的问题，<strong>但是在实际中，这个问题出现的概率并不高</strong>。</p>\n<p><strong>因为缓存的写入通常要远远快于数据库的写入</strong>，所以在实际中很难出现请求 B 已经更新了数据库并且删除了缓存，请求 A 才更新完缓存的情况。而一旦请求 A 早于请求 B 删除缓存之前更新了缓存，那么接下来的请求就会因为缓存不命中而从数据库中重新读取数据，所以不会出现这种不一致的情况。</p>\n<p><strong>Cache Aside 策略适合读多写少的场景，不适合写多的场景</strong>，因为当写入比较频繁时，缓存中的数据会被频繁地清理，这样会对缓存的命中率有一些影响。如果业务对缓存命中率有严格的要求，那么可以考虑两种解决方案：</p>\n<ul>\n<li>一种做法是在更新数据时也更新缓存，只是在更新缓存前先加一个分布式锁，因为这样在同一时间只允许一个线程更新缓存，就不会产生并发问题了。当然这么做对于写入的性能会有一些影响；</li>\n<li>另一种做法同样也是在更新数据时更新缓存，只是给缓存加一个较短的过期时间，这样即使出现缓存不一致的情况，缓存的数据也会很快过期，对业务的影响也是可以接受。</li>\n</ul>\n<h4 id=\"962readwrite-through读穿--写穿策略\">9.6.2、Read/Write Through（读穿 / 写穿）策略</h4>\n<p>Read/Write Through（读穿 / 写穿）策略原则是应用程序只和缓存交互，不再和数据库交互，而是由缓存和数据库交互，相当于更新数据库的操作由缓存自己代理了。</p>\n<p><em><strong>1、Read Through 策略</strong></em></p>\n<p>先查询缓存中数据是否存在，如果存在则直接返回，如果不存在，则由缓存组件负责从数据库查询数据，并将结果写入到缓存组件，最后缓存组件将数据返回给应用。</p>\n<p><em><strong>2、Write Through 策略</strong></em></p>\n<p>当有数据更新的时候，先查询要写入的数据在缓存中是否已经存在：</p>\n<ul>\n<li>如果缓存中数据已经存在，则更新缓存中的数据，并且由缓存组件同步更新到数据库中，然后缓存组件告知应用程序更新完成。</li>\n<li>如果缓存中数据不存在，直接更新数据库，然后返回；</li>\n</ul>\n<h4 id=\"963write-back写回策略\">9.6.3、Write Back（写回）策略</h4>\n<p>Write Back（写回）策略在更新数据的时候，只更新缓存，同时将缓存数据设置为脏的，然后立马返回，并不会更新数据库。对于数据库的更新，会通过批量异步更新的方式进行。</p>\n<p>实际上，Write Back（写回）策略也不能应用到我们常用的数据库和缓存的场景中，因为 Redis 并没有异步更新数据库的功能。</p>\n<p>Write Back 是计算机体系结构中的设计，比如 CPU 的缓存、操作系统中文件系统的缓存都采用了 Write Back（写回）策略。</p>\n<p><strong>Write Back 策略特别适合写多的场景</strong>，因为发生写操作的时候， 只需要更新缓存，就立马返回了。比如，写文件的时候，实际上是写入到文件系统的缓存就返回了，并不会写磁盘。</p>\n<p><strong>但是带来的问题是，数据不是强一致性的，而且会有数据丢失的风险</strong>，因为缓存一般使用内存，而内存是非持久化的，所以一旦缓存机器掉电，就会造成原本缓存中的脏数据丢失。所以你会发现系统在掉电之后，之前写入的文件会有部分丢失，就是因为 Page Cache 还没有来得及刷盘造成的。</p>\n<h3 id=\"97缓存一致性\">9.7、缓存一致性</h3>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/24d2e400cc5f4e469d84df7657fc99cd.png\">\n</p>\n<p><strong>由于引入了缓存，那么在数据更新时，不仅要更新数据库，而且要更新缓存，这两个更新操作存在前后的问题</strong>：</p>\n<ul>\n<li>先更新数据库，再更新缓存；</li>\n<li>先更新缓存，再更新数据库；</li>\n</ul>\n<h4 id=\"971先更新数据库再更新缓存\">9.7.1、先更新数据库，再更新缓存</h4>\n<p>举个例子，比如「请求 A 」和「请求 B 」两个请求，同时更新「同一条」数据，则可能出现这样的顺序：</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/3840ab40610a4625b5543e57db1e25c6.png\">\n</p>\n<p>A 请求先将数据库的数据更新为 1，然后在更新缓存前，请求 B 将数据库的数据更新为 2，紧接着也把缓存更新为 2，然后 A 请求更新缓存为 1。</p>\n<p>此时，数据库中的数据是 2，而缓存中的数据却是 1，<strong>出现了缓存和数据库中的数据不一致的现象</strong>。</p>\n<h4 id=\"972先更新缓存再更新数据库\">9.7.2、先更新缓存，再更新数据库</h4>\n<p>那换成「<strong>先更新缓存，再更新数据库</strong>」这个方案，还会有问题吗？</p>\n<p>依然还是存在并发的问题，分析思路也是一样。</p>\n<p>假设「请求 A 」和「请求 B 」两个请求，同时更新「同一条」数据，则可能出现这样的顺序：</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/f8cc60ff6832470ba2e7f97d4a6422c0.png\">\n</p>\n<p>A 请求先将缓存的数据更新为 1，然后在更新数据库前，B 请求来了， 将缓存的数据更新为 2，紧接着把数据库更新为 2，然后 A 请求将数据库的数据更新为 1。</p>\n<p>此时，数据库中的数据是 1，而缓存中的数据却是 2，<strong>出现了缓存和数据库中的数据不一致的现象</strong>。</p>\n<p>所以，<strong>无论是「先更新数据库，再更新缓存」，还是「先更新缓存，再更新数据库」，这两个方案都存在并发问题，当两个请求并发更新同一条数据的时候，可能会出现缓存和数据库中的数据不一致的现象</strong>。</p>\n<h4 id=\"973先更新数据库还是先删除缓存cache-aside-策略\">9.7.3、先更新数据库，还是先删除缓存（Cache Aside 策略）</h4>\n<p>在更新数据时，<strong>不更新缓存，而是删除缓存中的数据。然后，到读取数据时，发现缓存中没了数据之后，再从数据库中读取数据，更新到缓存中。</strong></p>\n<p><strong>写策略的步骤：</strong></p>\n<ul>\n<li>更新数据库中的数据；</li>\n<li>删除缓存中的数据。</li>\n</ul>\n<p><strong>读策略的步骤：</strong></p>\n<ul>\n<li>如果读取的数据命中了缓存，则直接返回数据；</li>\n<li>如果读取的数据没有命中缓存，则从数据库中读取数据，然后将数据写入到缓存，并且返回给用户。</li>\n</ul>\n<p>在写策略中，<strong>先删除缓存，再更新数据库，在「读 + 写」并发的时候，还是会出现缓存和数据库的数据不一致的问题</strong>。</p>\n<p>先更新数据库，再删除缓存也是会出现数据不一致性的问题，<strong>但是在实际中，这个问题出现的概率并不高</strong>。</p>\n<p><strong>因为缓存的写入通常要远远快于数据库的写入</strong>，所以在实际中很难出现请求 B 已经更新了数据库并且删除了缓存，请求 A 才更新完缓存的情况。</p>\n<p>而一旦请求 A 早于请求 B 删除缓存之前更新了缓存，那么接下来的请求就会因为缓存不命中而从数据库中重新读取数据，所以不会出现这种不一致的情况。</p>\n<p>所以，<strong>「先更新数据库 + 再删除缓存」的方案，是可以保证数据一致性的</strong>。同时给缓存数据加上了「<strong>过期时间</strong>」，就算在这期间存在缓存数据不一致，有过期时间来兜底，这样也能达到最终一致。</p>\n<h2 id=\"10怎么实现一个分布式锁\">10、怎么实现一个分布式锁</h2>\n<p>分布式锁是用于分布式环境下并发控制的一种机制，用于控制某个资源在同一时刻只能被一个应用所使用。如下图所示：</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/27314ac154c349df95ca4cad56f0e2b7.png\">\n</p>\n<p>Redis 本身可以被多个客户端共享访问，正好就是一个共享存储系统，可以用来保存分布式锁，而且 Redis 的读写性能高，可以应对高并发的锁操作场景。Redis 的 SET 命令有个 NX 参数可以实现「key不存在才插入」，所以可以用它来实现分布式锁：</p>\n<ul>\n<li>如果 key 不存在，则显示插入成功，可以用来表示加锁成功；</li>\n<li>如果 key 存在，则会显示插入失败，可以用来表示加锁失败。</li>\n</ul>\n<p>基于 Redis 节点实现分布式锁时，对于加锁操作，我们需要满足三个条件。</p>\n<ul>\n<li>加锁包括了读取锁变量、检查锁变量值和设置锁变量值三个操作，但需要以原子操作的方式完成，所以，我们使用 SET 命令带上 NX 选项来实现加锁；</li>\n<li>锁变量需要设置过期时间，以免客户端拿到锁后发生异常，导致锁一直无法释放，所以，我们在 SET 命令执行时加上 EX/PX 选项，设置其过期时间；</li>\n<li>锁变量的值需要能区分来自不同客户端的加锁操作，以免在释放锁时，出现误释放操作，所以，我们使用 SET 命令设置锁变量值时，每个客户端设置的值是一个唯一值，用于标识客户端；</li>\n</ul>\n<p>满足这三个条件的分布式命令如下：</p>\n<pre><code>SET lock_key unique_value NX PX 10000\n</code></pre>\n<ul>\n<li>lock_key 就是 key 键；</li>\n<li>unique_value 是客户端生成的唯一的标识，区分来自不同客户端的锁操作；</li>\n<li>NX 代表只在 lock_key 不存在时，才对 lock_key 进行设置操作；</li>\n<li>PX 10000 表示设置 lock_key 的过期时间为 10s，这是为了避免客户端发生异常而无法释放锁。</li>\n</ul>\n<p>而解锁的过程就是将 lock_key 键删除（del lock_key），但不能乱删，要保证执行操作的客户端就是加锁的客户端。所以，解锁的时候，我们要先判断锁的 unique_value 是否为加锁客户端，是的话，才将 lock_key 键删除。可以看到，解锁是有两个操作，这时就需要 Lua 脚本来保证解锁的原子性，因为 Redis 在执行 Lua 脚本时，可以以原子性的方式执行，保证了锁释放操作的原子性。</p>\n<pre><code>// 释放锁时，先比较 unique_value 是否相等，避免锁的误释放\nif redis.call(&quot;get&quot;,KEYS[1]) == ARGV[1] then\n    return redis.call(&quot;del&quot;,KEYS[1])\nelse\n    return 0\nend\n</code></pre>\n<p>这样一来，就通过使用 SET 命令和 Lua 脚本在 Redis 单节点上完成了分布式锁的加锁和解锁。</p>\n<h2 id=\"11redis-如何实现延迟队列\">11、Redis 如何实现延迟队列</h2>\n<p>延迟队列是指把当前要做的事情，往后推迟一段时间再做。延迟队列的常见使用场景有以下几种：</p>\n<ul>\n<li>在淘宝、京东等购物平台上下单，超过一定时间未付款，订单会自动取消；</li>\n<li>打车的时候，在规定时间没有车主接单，平台会取消你的单并提醒你暂时没有车主接单；</li>\n<li>点外卖的时候，如果商家在10分钟还没接单，就会自动取消订单；</li>\n</ul>\n<p>在 Redis 可以使用有序集合（ZSet）的方式来实现延迟消息队列的，ZSet 有一个 Score 属性可以用来存储延迟执行的时间。使用 zadd score1 value1 命令就可以一直往内存中生产消息。再利用 zrange by socre 查询符合条件的所有待处理的任务， 通过循环执行队列任务即可。</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/6395ed7619574dd5bf839a59beeebccb.png\">\n</p>\n<h2 id=\"12redis-的大-key-如何处理\">12、Redis 的大 key 如何处理</h2>\n<h3 id=\"121redis-大-key\">12.1、Redis 大 key</h3>\n<p>大 key 并不是指 key 的值很大，而是 key 对应的 value 很大。</p>\n<p>一般而言，下面这两种情况被称为大 key：</p>\n<ul>\n<li>String 类型的值大于 10 KB；</li>\n<li>Hash、List、Set、ZSet 类型的元素的个数超过 5000个；</li>\n</ul>\n<h3 id=\"122大-key-会造成什么问题\">12.2、大 key 会造成什么问题</h3>\n<p>大 key 会带来以下四种影响：</p>\n<ul>\n<li><strong>客户端超时阻塞</strong>。由于 Redis 执行命令是单线程处理，然后在操作大 key 时会比较耗时，那么就会阻塞 Redis，从客户端这一视角看，就是很久很久都没有响应。</li>\n<li><strong>引发网络阻塞</strong>。每次获取大 key 产生的网络流量较大，如果一个 key 的大小是 1 MB，每秒访问量为 1000，那么每秒会产生 1000MB 的流量，这对于普通千兆网卡的服务器来说是灾难性的。</li>\n<li><strong>阻塞工作线程</strong>。如果使用 del 删除大 key 时，会阻塞工作线程，这样就没办法处理后续的命令。</li>\n<li><strong>内存分布不均</strong>。集群模型在 slot 分片均匀情况下，会出现数据和查询倾斜情况，部分有大 key 的 Redis 节点占用内存多，QPS 也会比较大。</li>\n</ul>\n<h3 id=\"123如何找到大-key\">12.3、如何找到大 key</h3>\n<p><em><strong>1、redis-cli --bigkeys 查找大key</strong></em></p>\n<pre><code class=\"language-bash\">redis-cli -h 127.0.0.1 -p6379 -a &quot;password&quot; -- bigkeys\n</code></pre>\n<p>使用的时候注意事项：</p>\n<ul>\n<li>最好选择在从节点上执行该命令。因为主节点上执行时，会阻塞主节点；</li>\n<li>如果没有从节点，那么可以选择在 Redis 实例业务压力的低峰阶段进行扫描查询，以免影响到实例的正常运行；或者可以使用 -i 参数控制扫描间隔，避免长时间扫描降低 Redis 实例的性能。</li>\n</ul>\n<p>该方式的不足之处：</p>\n<ul>\n<li>这个方法只能返回每种类型中最大的那个 bigkey，无法得到大小排在前 N 位的 bigkey；</li>\n<li>对于集合类型来说，这个方法只统计集合元素个数的多少，而不是实际占用的内存量。但是，一个集合中的元素个数多，并不一定占用的内存就多。因为，有可能每个元素占用的内存很小，这样的话，即使元素个数有很多，总内存开销也不大；</li>\n</ul>\n<p><em><strong>2、使用 SCAN 命令查找大 key</strong></em></p>\n<p>使用 SCAN 命令对数据库扫描，然后用 TYPE 命令获取返回的每一个 key 的类型。</p>\n<p>对于 String 类型，可以直接使用 STRLEN 命令获取字符串的长度，也就是占用的内存空间字节数。</p>\n<p>对于集合类型来说，有两种方法可以获得它占用的内存大小：</p>\n<ul>\n<li>如果能够预先从业务层知道集合元素的平均大小，那么，可以使用下面的命令获取集合元素的个数，然后乘以集合元素的平均大小，这样就能获得集合占用的内存大小了。List 类型：<code>LLEN</code> 命令；Hash 类型：<code>HLEN</code> 命令；Set 类型：<code>SCARD</code> 命令；Sorted Set 类型：<code>ZCARD</code> 命令；</li>\n<li>如果不能提前知道写入集合的元素大小，可以使用 <code>MEMORY USAGE</code> 命令（需要 Redis 4.0 及以上版本），查询一个键值对占用的内存空间。</li>\n</ul>\n<p><em><strong>3、使用 RdbTools 工具查找大 key</strong></em></p>\n<p>使用 RdbTools 第三方开源工具，可以用来解析 Redis 快照（RDB）文件，找到其中的大 key。</p>\n<p>比如，下面这条命令，将大于 10 kb 的  key  输出到一个表格文件。</p>\n<pre><code class=\"language-shell\">rdb dump.rdb -c memory --bytes 10240 -f redis.csv\n</code></pre>\n<h3 id=\"124如何删除大-key\">12.4、如何删除大 key</h3>\n<ul>\n<li>分批次删除</li>\n<li>异步删除（Redis 4.0版本以上）</li>\n</ul>\n<p>对于<strong>删除大 Hash</strong>，使用 <code>hscan</code> 命令，每次获取 100 个字段，再用 <code>hdel</code> 命令，每次删除 1 个字段。</p>\n<p>Python代码：</p>\n<pre><code class=\"language-python\">def del_large_hash():\n  r = redis.StrictRedis(host='redis-host1', port=6379)\n    large_hash_key =&quot;xxx&quot; #要删除的大hash键名\n    cursor = '0'\n    while cursor != 0:\n        # 使用 hscan 命令，每次获取 100 个字段\n        cursor, data = r.hscan(large_hash_key, cursor=cursor, count=100)\n        for item in data.items():\n                # 再用 hdel 命令，每次删除1个字段\n                r.hdel(large_hash_key, item[0])\n</code></pre>\n<p>对于<strong>删除大 List</strong>，通过 <code>ltrim</code> 命令，每次删除少量元素。</p>\n<p>Python代码：</p>\n<pre><code class=\"language-python\">def del_large_list():\n  r = redis.StrictRedis(host='redis-host1', port=6379)\n  large_list_key = 'xxx'  #要删除的大list的键名\n  while r.llen(large_list_key)&gt;0:\n      #每次只删除最右100个元素\n      r.ltrim(large_list_key, 0, -101) \n</code></pre>\n<p>对于<strong>删除大 Set</strong>，使用 <code>sscan</code> 命令，每次扫描集合中 100 个元素，再用 <code>srem</code> 命令每次删除一个键。</p>\n<p>Python代码：</p>\n<pre><code class=\"language-python\">def del_large_set():\n  r = redis.StrictRedis(host='redis-host1', port=6379)\n  large_set_key = 'xxx'   # 要删除的大set的键名\n  cursor = '0'\n  while cursor != 0:\n    # 使用 sscan 命令，每次扫描集合中 100 个元素\n    cursor, data = r.sscan(large_set_key, cursor=cursor, count=100)\n    for item in data:\n      # 再用 srem 命令每次删除一个键\n      r.srem(large_size_key, item)\n</code></pre>\n<p>对于<strong>删除大 ZSet</strong>，使用 <code>zremrangebyrank</code> 命令，每次删除 top 100个元素。</p>\n<p>Python代码：</p>\n<pre><code class=\"language-python\">def del_large_sortedset():\n  r = redis.StrictRedis(host='large_sortedset_key', port=6379)\n  large_sortedset_key='xxx'\n  while r.zcard(large_sortedset_key)&gt;0:\n    # 使用 zremrangebyrank 命令，每次删除 top 100个元素\n    r.zremrangebyrank(large_sortedset_key,0,99) \n</code></pre>\n<p><em><strong>2、异步删除</strong></em></p>\n<p>从 Redis 4.0 版本开始，可以采用<strong>异步删除</strong>法，<strong>用 unlink 命令代替 del 来删除</strong>。</p>\n<p>这样 Redis 会将这个 key 放入到一个异步线程中进行删除，这样不会阻塞主线程。</p>\n<p>除了主动调用 unlink 命令实现异步删除之外，我们还可以通过配置参数，达到某些条件的时候自动进行异步删除。</p>\n<p>主要有 4 种场景，默认都是关闭的：</p>\n<pre><code class=\"language-text\">lazyfree-lazy-eviction no\nlazyfree-lazy-expire no\nlazyfree-lazy-server-del\nnoslave-lazy-flush no\n</code></pre>\n<p>它们代表的含义如下：</p>\n<ul>\n<li>lazyfree-lazy-eviction：表示当 Redis 运行内存超过 maxmeory 时，是否开启 lazy free 机制删除；</li>\n<li>lazyfree-lazy-expire：表示设置了过期时间的键值，当过期之后是否开启 lazy free 机制删除；</li>\n<li>lazyfree-lazy-server-del：有些指令在处理已存在的键时，会带有一个隐式的 del 键的操作，比如 rename 命令，当目标键已存在，Redis 会先删除目标键，如果这些目标键是一个 big key，就会造成阻塞删除的问题，此配置表示在这种场景中是否开启 lazy free 机制删除；</li>\n<li>slave-lazy-flush：针对 slave (从节点) 进行全量数据同步，slave 在加载 master 的 RDB 文件前，会运行 flushall 来清理自己的数据，它表示此时是否开启 lazy free 机制删除。</li>\n</ul>\n<p>建议开启其中的 lazyfree-lazy-eviction、lazyfree-lazy-expire、lazyfree-lazy-server-del 等配置，这样就可以有效的提高主线程的执行效率。</p>\n<h2 id=\"13redis-管道\">13、Redis 管道</h2>\n<p>管道技术（Pipeline）是客户端提供的一种批处理技术，用于一次处理多个 Redis 命令，从而提高整个交互的性能。</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/e310ae39300a4001a5e31e4a59b4563c.png\">\n</p>\n<p>使用<strong>管道技术可以解决多个命令执行时的网络等待</strong>，它是把多个命令整合到一起发送给服务器端处理之后统一返回给客户端，这样就免去了每条命令执行后都要等待的情况，从而有效地提高了程序的执行效率。</p>\n<p>但使用管道技术也要注意避免发送的命令过大，或管道内的数据太多而导致的网络阻塞。要注意的是，管道技术本质上是客户端提供的功能，而非 Redis 服务器端的功能。</p>\n<h2 id=\"14redis事务\">14、Redis事务</h2>\n<p>Redis 是单进程程序，并且它保证在执行事务时，不会对事务进行中断，事务可以运行直到执行完所有事务队列中的命令为止。因此，Redis 的事务是总是带有隔离性的。Redis单条命令是原子性执行的，但事务不保证原子性，且没有回滚。事务中任意命令执行失败，其余的命令仍会被执行。</p>\n<h2 id=\"15redlock\">15、RedLock</h2>\n<p>为了保证集群环境下分布式锁的可靠性，Redis 官方已经设计了一个分布式锁算法 Redlock（红锁）。</p>\n<p>它是基于<strong>多个 Redis 节点</strong>的分布式锁，即使有节点发生了故障，锁变量仍然是存在的，客户端还是可以完成锁操作。官方推荐是至少部署 5 个 Redis 节点，而且都是主节点，它们之间没有任何关系，都是一个个孤立的节点。</p>\n<p>Redlock 算法的基本思路，<strong>是让客户端和多个独立的 Redis 节点依次请求申请加锁，如果客户端能够和半数以上的节点成功地完成加锁操作，那么我们就认为，客户端成功地获得分布式锁，否则加锁失败</strong>。</p>\n<p>这样一来，即使有某个 Redis 节点发生故障，因为锁的数据在其他节点上也有保存，所以客户端仍然可以正常地进行锁操作，锁的数据也不会丢失。</p>\n<p>Redlock 算法加锁三个过程：</p>\n<ul>\n<li>第一步是，客户端获取当前时间（t1）。</li>\n<li>第二步是，客户端按顺序依次向 N 个 Redis 节点执行加锁操作：\n<ul>\n<li>加锁操作使用 SET 命令，带上 NX，EX/PX 选项，以及带上客户端的唯一标识。</li>\n<li>如果某个 Redis 节点发生故障了，为了保证在这种情况下，Redlock 算法能够继续运行，我们需要给「加锁操作」设置一个超时时间（不是对「锁」设置超时时间，而是对「加锁操作」设置超时时间），加锁操作的超时时间需要远远地小于锁的过期时间，一般也就是设置为几十毫秒。</li>\n</ul>\n</li>\n<li>第三步是，一旦客户端从超过半数（大于等于 N/2+1）的 Redis 节点上成功获取到了锁，就再次获取当前时间（t2），然后计算计算整个加锁过程的总耗时（t2-t1）。如果 t2-t1 &lt; 锁的过期时间，此时，认为客户端加锁成功，否则认为加锁失败。</li>\n</ul>\n<p>可以看到，加锁成功要同时满足两个条件（<em>简述：如果有超过半数的 Redis 节点成功的获取到了锁，并且总耗时没有超过锁的有效时间，那么就是加锁成功</em>）：</p>\n<ul>\n<li>条件一：客户端从超过半数（大于等于 N/2+1）的 Redis 节点上成功获取到了锁；</li>\n<li>条件二：客户端从大多数节点获取锁的总耗时（t2-t1）小于锁设置的过期时间。</li>\n</ul>\n<p>加锁成功后，客户端需要重新计算这把锁的有效时间，计算的结果是「锁最初设置的过期时间」减去「客户端从大多数节点获取锁的总耗时（t2-t1）」。<em>如果计算的结果已经来不及完成共享数据的操作了，我们可以释放锁，以免出现还没完成数据操作，锁就过期了的情况</em>。</p>\n<p>加锁失败后，客户端向<strong>所有 Redis 节点发起释放锁的操作</strong>，释放锁的操作和在单节点上释放锁的操作一样，只要执行释放锁的 Lua 脚本就可以了。</p>\n","createTime":"2024-05-17 15:38:26","categoryId":34,"categoryName":"Redis","readNum":53,"tags":[{"id":47,"name":"java","articlesTotal":null},{"id":62,"name":"redis","articlesTotal":null}],"preArticle":{"articleId":53,"articleTitle":"计算机网络"},"nextArticle":{"articleId":50,"articleTitle":"SpringMVC"},"totalWords":20169,"readTime":"约 67 分钟","updateTime":"2024-05-22 09:38:56"}} =================================== 
2024-08-04 17:11:03.192 [http-nio-8088-exec-2] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 17:11:03.228 [http-nio-8088-exec-5] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 17:11:03.229 [http-nio-8088-exec-5] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 17:11:03.231 [http-nio-8088-exec-5] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [获取知识库文章上下页], 入参: {"id":1,"articleId":51}, 请求类: WikiController, 请求方法: findArticlePreNext =================================== 
2024-08-04 17:11:03.257 [http-nio-8088-exec-5] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [获取知识库文章上下页], 耗时: 26ms, 出参: {"success":true,"message":null,"errorCode":null,"data":{"preArticle":{"articleId":45,"articleTitle":"Redis"},"nextArticle":null}} =================================== 
2024-08-04 17:11:03.257 [http-nio-8088-exec-5] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 21:28:07.356 [http-nio-8088-exec-8] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 21:28:07.356 [http-nio-8088-exec-1] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 21:28:07.356 [http-nio-8088-exec-10] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 21:28:07.680 [http-nio-8088-exec-1] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 21:28:07.680 [http-nio-8088-exec-8] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 21:28:07.680 [http-nio-8088-exec-10] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 21:28:09.216 [http-nio-8088-exec-8] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [前台获取博客详情], 入参: , 请求类: BlogSettingsController, 请求方法: findDetail =================================== 
2024-08-04 21:28:10.383 [http-nio-8088-exec-1] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [获取知识库文章上下页], 入参: {"id":1,"articleId":47}, 请求类: WikiController, 请求方法: findArticlePreNext =================================== 
2024-08-04 21:28:10.383 [http-nio-8088-exec-10] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [获取文章详情], 入参: {"articleId":47}, 请求类: ArticleController, 请求方法: findArticleDetail =================================== 
2024-08-04 21:28:13.906 [http-nio-8088-exec-8] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [前台获取博客详情], 耗时: 4713ms, 出参: {"success":true,"message":null,"errorCode":null,"data":{"logo":"http://110.41.141.141:9000/weblog/weblog/9853f8be13cb4f7fae00e3f5233dd688.png","name":"WebLog","author":"Jacob","introduction":"求知若饥，虚心若愚","avatar":"http://110.41.141.141:9000/weblog/weblog/cf0958d87787449fb05aae4cc84015c6.jpg","githubHomepage":"https://github.com/jdw-art","csdnHomepage":"https://www.csdn.net/?spm=1010.2135.3001.4476","giteeHomepage":"","zhihuHomepage":"https://www.zhihu.com/people/54-10-50-93"}} =================================== 
2024-08-04 21:28:14.313 [http-nio-8088-exec-1] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [获取知识库文章上下页], 耗时: 4059ms, 出参: {"success":true,"message":null,"errorCode":null,"data":{"preArticle":null,"nextArticle":{"articleId":42,"articleTitle":"MongoDB"}}} =================================== 
2024-08-04 21:28:15.990 [http-nio-8088-exec-10] INFO  c.j.weblog.event.subscriber.ReadArticleSubscriber - ==> threadName: http-nio-8088-exec-10
2024-08-04 21:28:16.017 [http-nio-8088-exec-10] INFO  c.j.weblog.event.subscriber.ReadArticleSubscriber - ==> 文章阅读事件消费成功，articleId: 47
2024-08-04 21:28:16.451 [http-nio-8088-exec-10] INFO  c.j.weblog.event.subscriber.ReadArticleSubscriber - ==> 文章阅读量 +1 操作成功，articleId: 47
2024-08-04 21:28:16.822 [http-nio-8088-exec-8] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 21:28:16.822 [http-nio-8088-exec-1] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 21:28:17.037 [http-nio-8088-exec-10] INFO  c.j.weblog.event.subscriber.ReadArticleSubscriber - ==> 当日文章 PV 访问量 +1 操作成功，date: 2024-08-04
2024-08-04 21:28:17.465 [http-nio-8088-exec-10] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [获取文章详情], 耗时: 6785ms, 出参: {"success":true,"message":null,"errorCode":null,"data":{"title":"MySQL","content":"<h2 id=\"1事务的四大特性\">1、事务的四大特性</h2>\n<p>事务特性ACID：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability）。</p>\n<ul>\n<li><strong>原子性</strong>：事务包含的所有操作要么全部成功，要么全部失败回滚。</li>\n<li><strong>一致性</strong>：一个事务在执行前和执行后都必须处于一致性状态。如a和b账户共1000块，两人之间转账无论成功还是失败之后，两个账户的总和都必须是1000块。</li>\n<li><strong>隔离性</strong>：和隔离级别相关，如<code>read committed</code>，<strong>一个事务只能读取到已经提交的修改</strong>。</li>\n<li><strong>持久性</strong>：一个事务一旦被提交了，那么对于数据库中的数据的改变将是永久性的，即使在数据库系统遇到故障的情况下也不会丢失提交事务的操作。</li>\n</ul>\n<h2 id=\"2数据库的三大范式\">2、数据库的三大范式</h2>\n<ol>\n<li>\n<p><strong>第一范式1NF</strong></p>\n<p><strong>确保数据库字段的原子性。</strong> 数据库中的字段不可再分</p>\n<p>比如字段 userInfo : 广东省 10086' ，依照第一范式必须拆分成 userInfo : 广东省 userTel : 10086\n两个字段。</p>\n</li>\n<li>\n<p><strong>第二范式2NF</strong></p>\n<p><strong>首先要满足第一范式，另外还要包含两个内容，一是表必须有主键，二是非主键必须完全依赖于主键，而不能依赖于主键的一部分。</strong></p>\n<p>举个例子。假定选课关系表为student_course (student_no, student_name, age, course_name,\ngrade, credit)，主键为(student_no, course_name)。其中学分完全依赖于课程名称，姓名年龄完全依\n赖学号，不符合第二范式，会导致数据冗余（学生选n门课，姓名年龄有n条记录）、插入异常（插入一\n门新课，因为没有学号，无法保存新课记录）等问题。</p>\n<p>应该拆分成三个表：学生： student (stuent_no, student_name, 年龄)；课程：\ncourse (course_name, credit)；选课关系： student_course_relation (student_no, course_name,\ngrade)。</p>\n</li>\n<li>\n<p><strong>第三范式3NF</strong></p>\n<p><strong>首先要满足第二范式，另外非主键列必须直接依赖于主键，不能存在传递依赖。即不能存在：非主键列A依赖于非主键列B，非主键列B依赖于主键的情况。</strong></p>\n<p>假定学生关系表为Student(student_no, student_name, age, academy_id, academy_telephone)，主\n键为&quot;学号&quot;，其中学院id依赖于学号，而学院地点和学院电话依赖于学院id，存在传递依赖，不符合第三\n范式。</p>\n<p>可以把学生关系表分为如下两个表：学生：(student_no, student_name, age, academy_id)；学院：\n(academy_id, academy_telephone)。</p>\n</li>\n</ol>\n<p><strong>2NF和3NF的区别：</strong></p>\n<ul>\n<li>2NF依据是非主键列是否完全依赖主键，还是依赖于主键的一部分。</li>\n<li>3NF依据是非主键列是直接依赖于主键，还是直接依赖于非主键。</li>\n</ul>\n<p><strong>第一范式：<strong>1NF是对属性的</strong>原子性约束</strong>，要求属性具有原子性，不可再分解；\n<strong>第二范式：<strong>2NF是对记录的</strong>惟一性约束</strong>，要求记录有惟一标识，即实体的惟一性；\n<strong>第三范式：<strong>3NF是对</strong>字段冗余性的约束</strong>，即任何字段不能由其他字段派生出来，它要求字段没有冗余。</p>\n<blockquote>\n<p>没有冗余的数据库设计可以做到。但是，没有冗余的数据库未必是最好的数据库，有时为了提高运行效率，就必须降低范式标准，适当保留冗余数据。具体做法是：在概念数据模型设计时遵守第三范式，降低范式标准的工作放到物理数据模型设计时考虑。降低范式就是增加字段，允许冗余。</p>\n</blockquote>\n<h2 id=\"3事务隔离的级别有哪些\">3、事务隔离的级别有哪些</h2>\n<p>先解释一下脏读、不可重复读和幻读的概念：</p>\n<ul>\n<li><strong>脏读：</strong> 在一个事务处理过程中读取了另一个未提交事务中的数据。</li>\n<li><strong>不可重复读：</strong> 对于数据库中的某行记录，一个事务范围内多次查询却返回了不同的数据值，这是由于在查询间隔，另一个事务修改了数据并提交了。</li>\n<li><strong>幻读：</strong> 当某个事务在读取某个范围内的记录时，另一个事务又在该范围内插入了新的记录。对幻读的正确理解是，一个事务内的读取操作的结论不足以支撑之后业务的执行。假设事务要新增一条记录，主键为id，在新增之前执行了select，发现没有id为xxx的记录，但插入时却出现了主键冲突，这就属于幻读，读取不到的记录却发生了主键冲突，是因为记录实际上已经被其他的事务插入了，但当前事务不可见。</li>\n</ul>\n<p>不可重复读和脏读的区别：<strong>脏读时某一事务读取了另一个事务未提交的脏数据，而不可重复读则是读取了前一个事务提交的数据</strong>。</p>\n<p>事务隔离就是为了解决上面提到的脏读、不可重复读、幻读这几个问题。</p>\n<p>MySQL数据库提供了四种隔离级别：</p>\n<ul>\n<li><strong>Serializable（串行化）</strong>：通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。</li>\n<li><strong>Repeatable read（可重复读）</strong>：<strong>MySQL的默认事务隔离级别，它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行，解决了不可重复读的问题</strong>。</li>\n<li><strong>Read committed（读已提交）</strong>：一个事务只能看见已经提交事务所作的改变，可避免脏读的发生。</li>\n<li><strong>Read uncommitted（读未提交）</strong>：所有事务都可以看见其他未提交事务的执行结果。</li>\n</ul>\n<p><strong>查看隔离级别：</strong></p>\n<pre><code class=\"language-sq\">select @@transaction_isolation;\n</code></pre>\n<p><strong>设置隔离级别：</strong></p>\n<pre><code class=\"language-sql\">set session transaction isolation level read uncommitted;\n</code></pre>\n<h2 id=\"4生产环境数据库一般用的隔离级别\">4、生产环境数据库一般用的隔离级别</h2>\n<p>生产环境一般使用读已提交（RC）隔离级别。为什么不使用可重复读（RR）隔离级别？</p>\n<blockquote>\n<p>可重复读(Repeatable Read)，简称为RR 读已提交(Read Commited)，简称为RC</p>\n</blockquote>\n<ul>\n<li>在可重复读（RR）隔离级别下，存在<strong>间隙锁</strong>，导致出现死锁的几率比读已提交（RC）大得多。</li>\n<li>在可重复读（RR）隔离级别下，<strong>条件列未命中索引会锁表</strong>，而在读已提交（RC）隔离级别下，只锁行。也就是说，读以提交的并发性要比可重复读高。</li>\n<li>并且大部分场景下，不可重复读是可以接受的。毕竟数据都已经提交了，读出来本身没有太大问题。</li>\n</ul>\n<h2 id=\"5编码和字符集的关系\">5、编码和字符集的关系</h2>\n<p>我们平时可以在编辑器上输入各种中文英文字母，但这些都是给人读的，不是给计算机读的，其实<strong>计算机真正保存和传输数据都是以二进制0101的格式进行的</strong>。</p>\n<p>那么就需要有一个规则，把中文和英文字母转化为二进制。其中d对应十六进制下的64，它可以转换为01二进制的格式。于是字母和数字就这样一一对应起来了，这就是ASCII编码格式。</p>\n<p>它用<strong>一个字节</strong>，也就是<code>8位</code>来标识字符，基础符号有128个，扩展符号也是128个。也就只能表示下<strong>英文字母和数字</strong>。</p>\n<p>这明显不够用。于是，为了标识<strong>中文</strong>，出现了<strong>GB2312</strong>的编码格式。为了标识<strong>希腊语</strong>，出现了<strong>greek</strong>编码格式，为了标识<strong>俄语</strong>，整了<strong>cp866</strong>编码格式。</p>\n<p>为了统一它们，于是出现了<strong>Unicode编码格式</strong>，它用了2~4个字节来表示字符，这样理论上所有符号都能被收录进去，并且它还完全兼容ASCII的编码，也就是说，同样是字母d，在ASCII用64表示，在Unicode里还是用64来表示。</p>\n<p>但<strong>不同的地方是ASCII编码用1个字节来表示，而Unicode用则两个字节来表示。</strong></p>\n<p>同样都是字母d，unicode比ascii多使用了一个字节，如下：</p>\n<pre><code class=\"language-\\\">D   ASCII:           01100100\nD Unicode:  00000000 01100100\n</code></pre>\n<p>可以看到，上面的unicode编码，前面的都是0，其实用不上，但还占了个字节，有点浪费。如果我们能做到该隐藏时隐藏，这样就能省下不少空间，按这个思路，就是就有了<strong>UTF-8编码</strong>。</p>\n<p>总结一下，<strong>按照一定规则把符号和二进制码对应起来，这就是编码。而把n多这种已经编码的字符聚在一起，就是我们常说的字符集</strong>。</p>\n<p>比如utf-8字符集就是所有utf-8编码格式的字符的合集。</p>\n<p>想看下mysql支持哪些字符集。可以执行 <code>show charset;</code></p>\n<h2 id=\"6utf8和utf8mb4的区别\">6、utf8和utf8mb4的区别</h2>\n<p>上面提到utf-8是在unicode的基础上做的优化，既然unicode有办法表示所有字符，那utf-8也一样可以表示所有字符，为了避免混淆，我在后面叫它<strong>大utf8</strong>。</p>\n<p>mysql支持的字符集中有utf8和utf8mb4。</p>\n<p>先说<strong>utf8mb4</strong>编码，mb4就是<strong>most bytes 4</strong>的意思，从上图最右边的<code>Maxlen</code>可以看到，它最大支持用<strong>4个字节</strong>来表示字符，它几乎可以用来表示目前已知的所有的字符。</p>\n<p>再说mysql字符集里的<strong>utf8</strong>，它是数据库的<strong>默认字符集</strong>。但注意，<strong>此utf8非彼utf8</strong>，我们叫它<strong>小utf8</strong>字符集。为什么这么说，因为从Maxlen可以看出，它最多支持用3个字节去表示字符，按utf8mb4的命名方式，准确点应该叫它<strong>utf8mb3</strong>。</p>\n<p>utf8 就像是阉割版的utf8mb4，只支持部分字符。比如<code>emoji</code>表情，它就不支持。</p>\n<p>而mysql支持的字符集里，第三列，<strong>collation</strong>，它是指<strong>字符集的比较规则</strong>。</p>\n<p>比如，&quot;debug&quot;和&quot;Debug&quot;是同一个单词，但它们大小写不同，该不该判为同一个单词呢。</p>\n<p>这时候就需要用到collation了。</p>\n<p>通过<code>SHOW COLLATION WHERE Charset = 'utf8mb4';</code>可以查看到<code>utf8mb4</code>下支持什么比较规则。</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/e89f7eae13cd4f3a933a6dffd7b73bdb.png\">\n</p>\n<p>如果<code>collation = utf8mb4_general_ci</code>，是指使用utf8mb4字符集的前提下，<strong>挨个字符进行比较</strong>（<code>general</code>），并且不区分大小写（<code>_ci，case insensitice</code>）。</p>\n<p>这种情况下，&quot;debug&quot;和&quot;Debug&quot;是同一个单词。</p>\n<p>如果改成<code>collation=utf8mb4_bin</code>，就是指<strong>挨个比较二进制位大小</strong>。</p>\n<p>于是&quot;debug&quot;和&quot;Debug&quot;就不是同一个单词。</p>\n<p><strong>那utf8mb4对比utf8有什么劣势吗？</strong></p>\n<p>我们知道数据库表里，字段类型如果是<code>char(2)</code>的话，里面的<code>2</code>是指<strong>字符个数</strong>，也就是说<strong>不管这张表用的是什么编码的字符集</strong>，都能放上2个字符。</p>\n<p>而char又是<strong>固定长度</strong>，为了能放下2个utf8mb4的字符，char会默认保留<code>2*4（maxlen=4）= 8</code>个字节的空间。</p>\n<p>如果是utf8mb3，则会默认保留 <code>2 * 3 (maxlen=3) = 6</code>个字节的空间。也就是说，在这种情况下，<strong>utf8mb4会比utf8mb3多使用一些空间。</strong></p>\n<p><strong>对于固定长度的char类型的字符串，utf8mb4会比utf8多使用一些空间</strong>。</p>\n<h2 id=\"7索引\">7、索引</h2>\n<h3 id=\"71索引的定义\">7.1、索引的定义</h3>\n<p>索引是存储引擎用于提高数据库表的访问速度的一种数据结构。</p>\n<h3 id=\"72索引的优缺点\">7.2、索引的优缺点</h3>\n<p><strong>优点：</strong></p>\n<ul>\n<li><strong>加快数据查找速度。</strong></li>\n<li>为用来排序或者是分组的字段添加索引，可以<strong>加快分组和排序的速度。</strong></li>\n<li><strong>加快表与表之间的连接。</strong></li>\n</ul>\n<p><strong>缺点：</strong></p>\n<ul>\n<li>建立索引需要<strong>占用物理空间</strong>。</li>\n<li>会<strong>降低表的增删改的效率</strong>，因为每次对表记录进行增删改，需要进行动态维护索引，导致增删改的时间变长。</li>\n</ul>\n<h3 id=\"73索引的作用\">7.3、索引的作用</h3>\n<p>数据是存储在磁盘上的，查询数据时，如果没有索引，就需要加载所有的数据到内存上，一次进行检索，读取磁盘次数较多。有了索引，就不需要加载全部数据了，因为B+树的高度一般在2-4层，最多只需要读取2-4次磁盘，查询速度大大提升。</p>\n<h3 id=\"74什么情况下需要建立索引\">7.4、什么情况下需要建立索引</h3>\n<ol>\n<li>经常用于<strong>查询</strong>的字段</li>\n<li>经常用于<strong>连接</strong>的字段建立索引，可以加快连接的速度。</li>\n<li>经常需要<strong>排序</strong>的字段建立索引，因为索引已经排好序，可以加快排序查询速度。</li>\n</ol>\n<h3 id=\"75什么情况下不建立索引\">7.5、什么情况下不建立索引</h3>\n<ol>\n<li><strong>where条件中用不到的字段</strong>不适合建立索引。</li>\n<li><strong>表记录较少</strong>。比如只有几百条数据，没必要加索引。</li>\n<li>需要<strong>经常增删改</strong>，需要评估是否适合加索引。</li>\n<li><strong>参与列计算的列</strong>不适合建立索引。</li>\n<li><strong>区分度不高的字段</strong>不适合建立索引，如性别，只有男/女/未知三个值。加了索引，查询效率也不会提高。</li>\n</ol>\n<h3 id=\"76索引的数据结构\">7.6、索引的数据结构</h3>\n<p>索引的数据结构主要有B+树和哈希表，对应的索引分别是<strong>B+树索引和哈希索引</strong>。InnoDB引擎的索引类型有B+树索引和哈希索引，默认的索引类型为B+树索引。</p>\n<p><strong>B+树索引</strong></p>\n<p>B+ 树是基于B 树和叶子节点顺序访问指针进行实现，它具有B树的平衡性，并且通过顺序访问指针来提高区间查询的性能。</p>\n<p>在 B+ 树中，节点中的 <code>key</code> 从左到右递增排列，如果某个指针的左右相邻 <code>key</code> 分别是 keyi 和 keyi+1，则该指针指向节点的所有 <code>key</code> 大于等于 keyi 且小于等于 keyi+1。</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/2d2a398bc5e94876a66d4e8359570895.png\">\n</p>\n<p>进行查找操作时，首先在根节点进行二分查找，找到key 所在的指针，然后递归地在指针所指向的节点进行查找。直到查找到叶子节点，然后在叶子节点上进行二分查找，找出key 所对应的数据项。</p>\n<p>MySQL 数据库使用最多的索引类型是BTREE 索引，底层基于B+树数据结构来实现。</p>\n<pre><code class=\"language-cmd\">mysql&gt; show index from blog\\G;\n*************************** 1. row ***************************\n        Table: blog\n   Non_unique: 0\n     Key_name: PRIMARY\n Seq_in_index: 1\n  Column_name: blog_id\n    Collation: A\n  Cardinality: 4\n     Sub_part: NULL\n       Packed: NULL\n         Null:\n   Index_type: BTREE\n      Comment:\nIndex_comment:\n      Visible: YES\n   Expression: NULL\n\n</code></pre>\n<p><strong>哈希索引</strong></p>\n<p>哈希索引是基于哈希表实现的，对于每一行数据，存储引擎会对索引列进行哈希计算得到哈希码，并且哈希算法要尽量保证不同的列值计算出的哈希码值是不同的，将哈希码的值作为哈希表的key值，将指向数据行的指针作为哈希表的value值。这样查找一个数据的时间复杂度就是O(1)，一般多用于精确查找。</p>\n<h3 id=\"77hash索引和b树索引的区别\">7.7、Hash索引和B+树索引的区别</h3>\n<ul>\n<li>哈希索引<strong>不支持排序</strong>，因为哈希表是无序的。</li>\n<li>哈希索引<strong>不支持范围查找</strong>。</li>\n<li>哈希索引<strong>不支持模糊查询</strong>以及多列索引的最左前缀匹配。</li>\n<li>因为哈希表会存在哈希冲突，所以<strong>哈希索引的性能是不稳定的，而B+树索引的性能相对稳定</strong>，每次查询都是从根节点到叶子节点。</li>\n</ul>\n<h3 id=\"78为什么b树比b树更适合实现数据库索引\">7.8、为什么B+树比B树更适合实现数据库索引</h3>\n<ul>\n<li>由于<strong>B+树的数据都存储在叶子节点中</strong>，叶子节点均为索引，方便扫库，只需要扫一遍叶子节点即可，但是B树因为其分支同样存储着数据，我们要找到具体的数据，需要进行一次中序遍历按序来扫，所以B+树更适合在区间查询的情况，而在数据库中基于范围的查询是非常频繁的，所以通常使用B+树用于数据库索引。</li>\n<li><strong>B+树的节点只存储索引key值，具体信息的地址存在于叶子节点的地址中</strong>。这就使<strong>以页为单位的索引中可以存放更多的节点</strong>。减少更多的I/O支出。</li>\n<li><strong>B+树的查询效率更加稳定，任何关键字的查找必须走一条从根结点到叶子结点的路</strong>。所有关键字查询的路径长度相同，导致每一个数据的查询效率相当。</li>\n</ul>\n<hr />\n<ul>\n<li>B+ 树的非叶子节点不存放实际的记录数据，仅存放索引，因此数据量相同的情况下，相比存储即存索引又存记录的 B 树，B+树的非叶子节点可以存放更多的索引，因此 B+ 树可以比 B 树更「矮胖」，查询底层节点的磁盘 I/O次数会更少。</li>\n<li>B+ 树有大量的冗余节点（所有非叶子节点都是冗余索引），这些冗余索引让 B+ 树在插入、删除的效率都更高，比如删除根节点的时候，不会像 B 树那样会发生复杂的树的变化；</li>\n<li>B+ 树叶子节点之间用链表连接了起来，有利于范围查询，而 B 树要实现范围查询，因此只能通过树的遍历来完成范围查询，这会涉及多个节点的磁盘 I/O 操作，范围查询效率不如 B+ 树。</li>\n</ul>\n<h3 id=\"79索引的分类\">7.9、索引的分类</h3>\n<ol>\n<li>\n<p><strong>主键索引</strong>：名为primary的唯一非空索引，不允许有空值。</p>\n</li>\n<li>\n<p><strong>唯一索引</strong>：索引列的值必须是唯一的，允许有空值。主键索引和唯一索引的区别在于：唯一索引字段可以为null且可以存在多个null值，而主键索引字段不可以为null。唯一索引的用途：唯一标识数据库中的每条记录，主要用来防止重复数据插入。创建唯一索引的SQL语句为：</p>\n<pre><code class=\"language-sql\">ALTER TABLE table_name\nADD CONSTRAINT constraint_name UNIQUE KEY(column_1,column_2,...);\n</code></pre>\n</li>\n<li>\n<p><strong>组合索引</strong>：在表中的多个字段组合上创建的索引，只有在查询条件中使用了这些字段的左边字段时，索引才会被使用，使用组合索引时需遵循最左前缀原则。</p>\n</li>\n<li>\n<p><strong>全文索引</strong>：只能在CHAR 、VARCHAR 和TEXT 类型字段上使用全文索引。</p>\n</li>\n<li>\n<p><strong>普通索引</strong>：普通索引是最基本的索引，它没有任何限制，值可以为空。</p>\n</li>\n</ol>\n<h3 id=\"710什么是最左匹配原则\">7.10、什么是最左匹配原则</h3>\n<p><strong>如果 SQL 语句中用到了组合索引中的最左边的索引，那么这条 SQL 语句就可以利用这个组合索引去进行匹配。当遇到范围查询(<code>&gt;</code>、<code>&lt;</code>、<code>between</code>、<code>like</code>)就会停止匹配，后面的字段不会用到索引</strong>。</p>\n<p>对<code>(a,b,c)</code>建立索引，查询条件使用 a/ab/abc 会走索引，使用 bc 不会走索引。</p>\n<p>对<code>(a,b,c,d)</code>建立索引，查询条件为<code>a = 1 and b = 2 and c &gt; 3 and d = 4</code>，那么a、b和c三个字段能用到索引，而d无法使用索引。因为遇到了范围查询。</p>\n<p>如下图，对(a, b) 建立索引，a 在索引树中是全局有序的，而 b 是全局无序，局部有序（当a相等时，会根据b进行排序）。直接执行<code>b = 2</code>这种查询条件无法使用索引。</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/fe96a1d64f5a4eaf9181803fe8fc1294.png\">\n</p>\n<p>当a的值确定的时候，b是有序的。例如<code>a = 1</code>时，b值为1，2是有序的状态。当<code>a = 2</code>时候，b的值为1，4也是有序状态。 当执行<code>a = 1 and b = 2</code>时a和b字段能用到索引。而执行<code>a &gt; 1 and b = 2</code>时，a字段能用到索引，b字段用不到索引。因为a的值此时是一个范围，不是固定的，在这个范围内b值不是有序的，因此b字段无法使用索引。</p>\n<h3 id=\"711什么是聚集索引\">7.11、什么是聚集索引</h3>\n<p>InnoDB使用<strong>表的主键构造主键索引树</strong>，同时叶子节点中存放的即为整张表的记录数据。聚集索引叶子节点的存储是逻辑上连续的，使用双向链表连接，叶子节点按照主键的顺序排序，因此对于主键的排序查找和范围查找速度比较快。如果表中没有显示指定主键，则会<strong>选择表中的第一个不允许为NULL 的唯一索引</strong>。如果没有主键也没有合适的唯一索引，那么InnoDB 内部会生成一个隐藏的主键作为聚集索引，这个隐藏的主键长度为6个字节，它的值会随着数据的插入自增。</p>\n<h3 id=\"712什么是覆盖索引\">7.12、什么是覆盖索引</h3>\n<p><em>InnoDB 的数据是按「数据页」为单位来读写的，默认数据页大小为 16 KB。每个数据页之间通过双向链表的形式组织起来，物理上不连续，但是逻辑上连续。</em></p>\n<p><em>数据页内包含用户记录，每个记录之间用单向链表的方式组织起来，为了加快在数据页内高效查询记录，设计了一个页目录，页目录存储各个槽（分组），且主键值是有序的，于是可以通过二分查找法的方式进行检索从而提高效率。</em></p>\n<p><em>为了高效查询记录所在的数据页，InnoDB 采用 b+ 树作为索引，每个节点都是一个数据页。</em></p>\n<p><em>如果叶子节点存储的是实际数据的就是聚簇索引，一个表只能有一个聚簇索引；如果叶子节点存储的不是实际数据，而是主键值则就是二级索引，一个表中可以有多个二级索引。</em></p>\n<p><em>在使用二级索引进行查找数据时，如果查询的数据能在二级索引找到，那么就是「索引覆盖」操作，如果查询的数据不在二级索引里，就需要先在二级索引找到主键值，需要去聚簇索引中获得数据行，这个过程就叫作「回表」。</em></p>\n<p><strong>覆盖索引（covering index ，或称为索引覆盖）即从非主键索引中就能查到的记录，而不需要查询主键索引中的记录，避免了回表的产生减少了树的搜索次数，显著提升性能</strong>。不是所有类型的索引都可以成为覆盖索引。覆盖索引要存储索引列的值，而哈希索引、全文索引不存储索引列的值，所以<strong>MySQL使用b+树索引做覆盖索引</strong>。</p>\n<p>对于使用了覆盖索引的查询，在查询前面使用<code>explain</code>，输出的extra列会显示为<code>using index</code>。</p>\n<p>比如<code>user_like</code> 用户点赞表，组合索引为<code>(user_id, blog_id)</code>，<code>user_id</code>和<code>blog_id</code>都不为<code>null</code>。</p>\n<pre><code class=\"language-sql\">explain select blog_id from user_like where user_id = 13;\n</code></pre>\n<p><code>explain</code>结果的<code>Extra</code>列为<code>Using index</code>，查询的列被索引覆盖，并且where筛选条件符合最左前缀原则，通过<strong>索引查找</strong>就能直接找到符合条件的数据，不需要回表查询数据。</p>\n<pre><code class=\"language-sql\">explain select user_id from user_like where blog_id = 1;\n</code></pre>\n<p><code>explain</code>结果的<code>Extra</code>列为<code>Using where; Using index</code>， 查询的列被索引覆盖，where筛选条件不符合最左前缀原则，无法通过索引查找找到符合条件的数据，但可以通过<strong>索引扫描</strong>找到符合条件的数据，也不需要回表查询数据。</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/01d86030cd5e4303bb22af9d6167c413.png\">\n</p>\n<blockquote>\n<p><em>如果某个查询语句使用了二级索引，但是查询的数据不是主键值，这时在二级索引找到主键值后，需要去聚簇索引中获得数据行，这个过程就叫作「回表」，也就是说要查两个 B+ 树才能查到数据。不过，当查询的数据是主键值时，因为只在二级索引就能查询到，不用再去聚簇索引查，这个过程就叫作「索引覆盖」，也就是只需要查一个 B+ 树就能找到数据。</em></p>\n</blockquote>\n<h3 id=\"713索引的设计原则\">7.13、索引的设计原则</h3>\n<ul>\n<li>\n<p>对于经常作为查询条件的字段，应该建立索引，以提高查询速度</p>\n</li>\n<li>\n<p>为经常需要排序、分组和联合操作的字段建立索引</p>\n</li>\n<li>\n<p>索引列的<strong>区分度越高</strong>，索引的效果越好。比如使用性别这种区分度很低的列作为索引，效果就会很差。</p>\n</li>\n<li>\n<p>避免给&quot;大字段&quot;建立索引。尽量使用数据量小的字段作为索引。因为<code>MySQL</code>在维护索引的时候是会将字段值一起维护的，那这样必然会导致索引占用更多的空间，另外在排序的时候需要花费更多的时间去对比。</p>\n</li>\n<li>\n<p>尽量使用<strong>短索引</strong>，对于较长的字符串进行索引时应该指定一个较短的前缀长度，因为较小的索引涉及到的磁盘I/O较少，查询速度更快。</p>\n</li>\n<li>\n<p>索引不是越多越好，每个索引都需要额外的物理空间，维护也需要花费时间。</p>\n</li>\n<li>\n<p>频繁增删改的字段不要建立索引。假设某个字段频繁修改，那就意味着需要频繁的重建索引，这必然影响MySQL的性能</p>\n</li>\n<li>\n<p>利用<strong>最左前缀原则</strong>。</p>\n</li>\n<li>\n<p>自增主键可以让主键索引尽量地保持递增顺序插入，避免了页分裂，因此索引更紧凑，在查询的时候，效率也就更高。</p>\n</li>\n</ul>\n<h3 id=\"714索引什么时候会失效\">7.14、索引什么时候会失效</h3>\n<p>导致索引失效的情况：</p>\n<ul>\n<li>对于组合索引，不是使用组合索引最左边的字段，则不会使用索引</li>\n<li>以%开头的like查询如<code>%abc</code>，无法使用索引；非%开头的like查询如<code>abc%</code>，相当于范围查询，会使用索引</li>\n<li>查询条件中列类型是字符串，没有使用引号，可能会因为类型不同发生隐式转换，使索引失效</li>\n<li>判断索引列是否不等于某个值时</li>\n<li>对索引列进行运算</li>\n<li>查询条件使用<code>or</code>连接，也会导致索引失效</li>\n</ul>\n<h3 id=\"715什么是前缀索引\">7.15、什么是前缀索引</h3>\n<p>有时需要在很长的字符列上创建索引，这会造成索引特别大且慢。使用前缀索引可以避免这个问题。</p>\n<p><strong>前缀索引是指对文本或者字符串的前几个字符建立索引</strong>，这样索引的长度更短，查询速度更快。</p>\n<p>创建前缀索引的关键在于选择足够长的前缀以<strong>保证较高的索引选择性</strong>。索引选择性越高查询效率就越高，因为选择性高的索引可以让MySQL在查找时过滤掉更多的数据行。</p>\n<p>建立前缀索引的方式：</p>\n<pre><code class=\"language-sql\">// email列创建前缀索引\nALTER TABLE table_name ADD KEY(column_name(prefix_length));\n</code></pre>\n<h2 id=\"8常见的存储引擎有哪些\">8、常见的存储引擎有哪些</h2>\n<p>MySQL中常用的四种存储引擎分别是： <strong>MyISAM</strong>、<strong>InnoDB</strong>、<strong>MEMORY</strong>、<strong>ARCHIVE</strong>。MySQL 5.5版本后默认的存储引擎为<code>InnoDB</code>。</p>\n<p><strong>InnoDB存储引擎</strong></p>\n<p>InnoDB是MySQL<strong>默认的事务型存储引擎</strong>，使用最广泛，基于聚簇索引建立的。InnoDB内部做了很多优化，如能够自动在内存中创建自适应hash索引，以加速读操作。</p>\n<p><strong>优点</strong>：支持事务和崩溃修复能力；引入了行级锁和外键约束。</p>\n<p><strong>缺点</strong>：占用的数据空间相对较大。</p>\n<p><strong>适用场景</strong>：需要事务支持，并且有较高的并发读写频率。</p>\n<p><strong>MyISAM存储引擎</strong></p>\n<p>数据以紧密格式存储。对于只读数据，或者表比较小、可以容忍修复操作，可以使用MyISAM引擎。MyISAM会将表存储在两个文件中，数据文件<code>.MYD</code>和索引文件<code>.MYI</code>。</p>\n<p><strong>优点</strong>：访问速度快。</p>\n<p><strong>缺点</strong>：MyISAM不支持事务和行级锁，不支持崩溃后的安全恢复，也不支持外键。</p>\n<p><strong>适用场景</strong>：对事务完整性没有要求；表的数据都是只读的。</p>\n<p><strong>MEMORY存储引擎</strong></p>\n<p>MEMORY引擎将数据全部放在内存中，访问速度较快，但是一旦系统奔溃的话，数据都会丢失。</p>\n<p>MEMORY引擎默认使用哈希索引，将键的哈希值和指向数据行的指针保存在哈希索引中。</p>\n<p><strong>优点</strong>：访问速度较快。</p>\n<p><strong>缺点</strong>：</p>\n<ol>\n<li>哈希索引数据不是按照索引值顺序存储，无法用于排序。</li>\n<li>不支持部分索引匹配查找，因为哈希索引是使用索引列的全部内容来计算哈希值的。</li>\n<li>只支持等值比较，不支持范围查询。</li>\n<li>当出现哈希冲突时，存储引擎需要遍历链表中所有的行指针，逐行进行比较，直到找到符合条件的行。</li>\n</ol>\n<p><strong>ARCHIVE存储引擎</strong></p>\n<p>ARCHIVE存储引擎非常适合存储大量独立的、作为历史记录的数据。ARCHIVE提供了压缩功能，拥有高效的插入速度，但是这种引擎不支持索引，所以查询性能较差。</p>\n<h2 id=\"9myisam和innodb的区别\">9、MyISAM和InnoDB的区别</h2>\n<ul>\n<li>\n<p><strong>存储结构的区别</strong>。每个MyISAM在磁盘上存储成三个文件。文件的名字以表的名字开始，扩展名指出文件类型。 .frm文件存储表定义。数据文件的扩展名为.MYD (MYData)。索引文件的扩展名是.MYI (MYIndex)。InnoDB所有的表都保存在同一个数据文件中（也可能是多个文件，或者是独立的表空间文件），InnoDB表的大小只受限于操作系统文件的大小，一般为2GB。</p>\n</li>\n<li>\n<p><strong>存储空间的区别</strong>。MyISAM支持支持三种不同的存储格式：静态表(默认，但是注意数据末尾不能有空格，会被去掉)、动态表、压缩表。当表在创建之后并导入数据之后，不会再进行修改操作，可以使用压缩表，极大的减少磁盘的空间占用。InnoDB需要更多的内存和存储，它会在主内存中建立其专用的缓冲池用于高速缓冲数据和索引。</p>\n</li>\n<li>\n<p><strong>可移植性、备份及恢复</strong>。MyISAM数据是以文件的形式存储，所以在跨平台的数据转移中会很方便。在备份和恢复时可单独针对某个表进行操作。对于InnoDB，可行的方案是拷贝数据文件、备份 binlog，或者用mysqldump，在数据量达到几十G的时候就相对麻烦了。</p>\n</li>\n<li>\n<p><strong>是否支持行级锁</strong>。MyISAM 只支持表级锁，用户在操作myisam表时，select，update，delete，insert语句都会给表自动加锁，如果加锁以后的表满足insert并发的情况下，可以在表的尾部插入新的数据。而InnoDB 支持行级锁和表级锁，默认为行级锁。行锁大幅度提高了多用户并发操作的性能。</p>\n</li>\n<li>\n<p><strong>是否支持事务和崩溃后的安全恢复</strong>。 MyISAM 不提供事务支持。而InnoDB 提供事务支持，具有事务、回滚和崩溃修复能力。</p>\n</li>\n<li>\n<p><strong>是否支持外键</strong>。MyISAM不支持，而InnoDB支持。</p>\n</li>\n<li>\n<p><strong>是否支持MVCC</strong>。MyISAM不支持，InnoDB支持。应对高并发事务，MVCC比单纯的加锁更高效。</p>\n</li>\n<li>\n<p><strong>是否支持聚集索引</strong>。MyISAM不支持聚集索引，InnoDB支持聚集索引。</p>\n</li>\n<li>\n<p><strong>全文索引</strong>。MyISAM支持 FULLTEXT类型的全文索引。InnoDB不支持FULLTEXT类型的全文索引，但是innodb可以使用sphinx插件支持全文索引，并且效果更好。</p>\n</li>\n<li>\n<p><strong>表主键</strong>。MyISAM允许没有任何索引和主键的表存在，索引都是保存行的地址。对于InnoDB，如果没有设定主键或者非空唯一索引，就会自动生成一个6字节的主键(用户不可见)。</p>\n</li>\n<li>\n<p><strong>表的行数</strong>。MyISAM保存有表的总行数，如果<code>select count(*) from table</code>;会直接取出该值。InnoDB没有保存表的总行数，如果使用select count(*) from table；就会遍历整个表，消耗相当大，但是在加了where条件后，MyISAM和InnoDB处理的方式都一样。</p>\n</li>\n</ul>\n<hr />\n<ul>\n<li>\n<p><strong>存储结构的区别</strong>。每个MyISAM在磁盘上存储成三个文件（索引文件、数据文件和表结构文件）。InnoDB所有的表都保存在同一个数据文件中（也可能是多个文件，或者是独立的表空间文件）。</p>\n</li>\n<li>\n<p><strong>存储空间的区别</strong>。MyISAM支持三种不同的存储格式：静态表、动态表、压缩表。当表在创建之后并导入数据之后，不会再进行修改操作，可以使用压缩表，极大的减少磁盘的空间占用。InnoDB需要更多的内存和存储，它会在主内存中建立其专用的缓冲池用于高速缓冲数据和索引。</p>\n</li>\n<li>\n<p><strong>可移植性、备份及恢复</strong>。MyISAM数据是以文件的形式存储，所以在跨平台的数据转移中会很方便。对于InnoDB，可行的方案是拷贝数据文件、备份 binlog，或者用mysqldump，在数据量达到几十G的时候就相对麻烦了。</p>\n</li>\n<li>\n<p><strong>是否支持行级锁</strong>。MyISAM 只支持表级锁，而InnoDB 支持行级锁和表级锁，默认为行级锁。行锁大幅度提高了多用户并发操作的性能。</p>\n</li>\n<li>\n<p><strong>是否支持事务和崩溃后的安全恢复</strong>。 MyISAM 不提供事务支持。而InnoDB 提供事务支持，具有事务、回滚和崩溃修复能力。</p>\n</li>\n<li>\n<p><strong>是否支持外键</strong>。MyISAM不支持，而InnoDB支持。</p>\n</li>\n<li>\n<p><strong>是否支持MVCC</strong>。MyISAM不支持，InnoDB支持。应对高并发事务，MVCC比单纯的加锁更高效。</p>\n</li>\n<li>\n<p><strong>是否支持聚集索引</strong>。MyISAM不支持聚集索引，InnoDB支持聚集索引。</p>\n</li>\n<li>\n<p><strong>全文索引</strong>。MyISAM支持 FULLTEXT类型的全文索引。InnoDB不支持FULLTEXT类型的全文索引，但是innodb可以使用sphinx插件支持全文索引，并且效果更好。</p>\n</li>\n<li>\n<p><strong>表主键</strong>。MyISAM允许没有任何索引和主键的表存在，索引都是保存行的地址。对于InnoDB，如果没有设定主键或者非空唯一索引，就会自动生成一个6字节的主键(用户不可见)。</p>\n</li>\n<li>\n<p><strong>表的行数</strong>。MyISAM保存有表的总行数，如果<code>select count(*) from table</code>;会直接取出该值。InnoDB没有保存表的总行数，如果使用select count(*) from table；就会遍历整个表，消耗相当大，但是在加了where条件后，MyISAM和InnoDB处理的方式都一样。</p>\n</li>\n</ul>\n<h2 id=\"10mysql有哪些锁\">10、MySQL有哪些锁</h2>\n<p><strong>按锁粒度分类</strong>，有行级锁、表级锁和页级锁。</p>\n<ol>\n<li>行级锁是mysql中锁的粒度最细的一种锁。表示只针对当前操作的行进行加锁。行级锁能大大减少数据库操作的冲突，其加锁粒度最小，但加锁的开销也最大。行级锁的类型主要有三类：\n<ul>\n<li>Record Lock，记录锁，也就是仅仅把一条记录锁上；</li>\n<li>Gap Lock，间隙锁，锁定一个范围，但是不包含记录本身；</li>\n<li>Next-Key Lock：Record Lock + Gap Lock 的组合，锁定一个范围，并且锁定记录本身。</li>\n</ul>\n</li>\n<li>表级锁是mysql中锁定粒度最大的一种锁，表示对当前操作的整张表加锁，它实现简单，资源消耗较少，被大部分mysql引擎支持。最常使用的MyISAM与InnoDB都支持表级锁定。</li>\n<li>页级锁是 MySQL 中锁定粒度介于行级锁和表级锁中间的一种锁。表级锁速度快，但冲突多，行级冲突少，但速度慢。因此，采取了折衷的页级锁，一次锁定相邻的一组记录。</li>\n</ol>\n<p><strong>按锁级别分类</strong>，有共享锁、排他锁和意向锁。</p>\n<ol>\n<li>共享锁又称读锁，是读取操作创建的锁。其他用户可以并发读取数据，但任何事务都不能对数据进行修改（获取数据上的排他锁），直到已释放所有共享锁。</li>\n<li>排他锁又称写锁、独占锁，如果事务T对数据A加上排他锁后，则其他事务不能再对A加任何类型的封锁。获准排他锁的事务既能读数据，又能修改数据。</li>\n<li>意向锁是表级锁，其设计目的主要是为了在一个事务中揭示下一行将要被请求锁的类型。InnoDB 中的两个表锁：\n<ul>\n<li>意向共享锁（IS）：表示事务准备给数据行加入共享锁，也就是说一个数据行加共享锁前必须先取得该表的IS锁；</li>\n<li>意向排他锁（IX）：类似上面，表示事务准备给数据行加入排他锁，说明事务在一个数据行加排他锁前必须先取得该表的IX锁。</li>\n</ul>\n</li>\n</ol>\n<p>意向锁是 InnoDB 自动加的，不需要用户干预。</p>\n<p><strong>对于INSERT、UPDATE和DELETE，InnoDB 会自动给涉及的数据加排他锁；对于一般的SELECT语句，InnoDB 不会加任何锁</strong>，事务可以通过以下语句显式加共享锁或排他锁。</p>\n<p>共享锁：<code>SELECT … LOCK IN SHARE MODE;</code></p>\n<p>排他锁：<code>SELECT … FOR UPDATE;</code></p>\n<h2 id=\"11mvcc-实现原理\">11、MVCC 实现原理</h2>\n<p>我们需要了解两个知识：</p>\n<p>Read View 中四个字段作用；\n聚簇索引记录中两个跟事务有关的隐藏列；\n那 Read View 到底是个什么东西？</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/26e9aa37ce3440db94c8e15459fe2e53.png\">\n</p>\n<p>Read View 有四个重要的字段：</p>\n<ul>\n<li><em><strong>m_ids</strong></em> ：指的是在创建 Read View 时，当前数据库中「活跃事务」的事务 id 列表，注意是一个列表，“活跃事务”指的就是，启动了但还没提交的事务。</li>\n<li><em><strong>min_trx_id</strong></em> ：指的是在创建 Read View 时，当前数据库中「活跃事务」中事务 id 最小的事务，也就是 m_ids 的最小值。</li>\n<li><em><strong>max_trx_id</strong></em> ：这个并不是 m_ids 的最大值，而是创建 Read View 时当前数据库中应该给下一个事务的 id 值，也就是全局事务中最大的事务 id 值 + 1；</li>\n<li><em><strong>creator_trx_id</strong></em> ：指的是创建该 Read View 的事务的事务 id。\n知道了 Read View 的字段，我们还需要了解聚簇索引记录中的两个隐藏列。</li>\n</ul>\n<p>假设在账户余额表插入一条小林余额为 100 万的记录，然后我把这两个隐藏列也画出来，该记录的整个示意图如下：</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/30528a510be44807b7535d94eacd1304.png\">\n</p>\n<p>对于使用 InnoDB 存储引擎的数据库表，它的聚簇索引记录中都包含下面两个隐藏列：</p>\n<ul>\n<li><em><strong>trx_id</strong></em>，当一个事务对某条聚簇索引记录进行改动时，就会把该事务的事务 id 记录在 trx_id 隐藏列里；</li>\n<li><em><strong>roll_pointer</strong></em>，每次对某条聚簇索引记录进行改动时，都会把旧版本的记录写入到 undo 日志中，然后这个隐藏列是个指针，指向每一个旧版本记录，于是就可以通过它找到修改前的记录。\n在创建 Read View 后，我们可以将记录中的 trx_id 划分这三种情况：</li>\n</ul>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/8dc914e744dc4d9aa514ed1bd0fef737.png\">\n</p>\n<p>一个事务去访问记录的时候，除了自己的更新记录总是可见之外，还有这几种情况：</p>\n<ul>\n<li>如果记录的 trx_id 值小于 Read View 中的 min_trx_id 值，表示这个版本的记录是在创建 Read View 前已经提交的事务生成的，所以该版本的记录对当前事务可见。</li>\n<li>如果记录的 trx_id 值大于等于 Read View 中的 max_trx_id 值，表示这个版本的记录是在创建 Read View 后才启动的事务生成的，所以该版本的记录对当前事务不可见。</li>\n<li>如果记录的 trx_id 值在 Read View 的 min_trx_id 和 max_trx_id 之间，需要判断 trx_id 是否在 m_ids 列表中：\n<ul>\n<li>如果记录的 trx_id 在 m_ids 列表中，表示生成该版本记录的活跃事务依然活跃着（还没提交事务），所以该版本的记录对当前事务不可见。</li>\n<li>如果记录的 trx_id 不在 m_ids列表中，表示生成该版本记录的活跃事务已经被提交，所以该版本的记录对当前事务可见。</li>\n</ul>\n</li>\n</ul>\n<p><em><strong>这种通过「版本链」来控制并发事务访问同一个记录时的行为就叫 MVCC（多版本并发控制）</strong></em>。</p>\n<h2 id=\"12快照读和当前读\">12、快照读和当前读</h2>\n<p>表记录有两种读取方式。</p>\n<ul>\n<li>快照读：<strong>读取的是快照版本</strong>。普通的<code>SELECT</code>就是快照读。通过mvcc来进行并发控制的，不用加锁。</li>\n<li>当前读：<strong>读取的是最新版本</strong>。<code>UPDATE、DELETE、INSERT、SELECT … LOCK IN SHARE MODE、SELECT … FOR UPDATE</code>是当前读。</li>\n</ul>\n<p>快照读情况下，InnoDB通过<code>mvcc</code>机制避免了幻读现象。而<code>mvcc</code>机制无法避免当前读情况下出现的幻读现象。因为当前读每次读取的都是最新数据，这时如果两次查询中间有其它事务插入数据，就会产生幻读。</p>\n<p>下面举个例子说明下：</p>\n<p>1、首先，user表只有两条记录，具体如下：</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/a0f3abf69e0e4a45868300c8f96ae0ad.png\">\n</p>\n<p>2、事务a和事务b同时开启事务<code>start transaction</code>；</p>\n<p>3、事务a插入数据然后提交；</p>\n<pre><code class=\"language-sql\">insert into user(user_name, user_password, user_mail, user_state) values('tyson', 'a', 'a', 0);\n</code></pre>\n<p>4、事务b执行全表的update；</p>\n<pre><code class=\"language-sql\">update user set user_name = 'a';\n</code></pre>\n<p>5、事务b然后执行查询，查到了事务a中插入的数据。（下图左边是事务b，右边是事务a。事务开始之前只有两条记录，事务a插入一条数据之后，事务b查询出来是三条数据）</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/f98b40f48d1641018a638e91fda8a69d.png\">\n</p>\n<p>以上就是当前读出现的幻读现象。</p>\n<p><strong>那么MySQL是如何避免幻读？</strong></p>\n<ul>\n<li>针对快照读（普通 select 语句），是通过 MVCC 方式解决了幻读，因为可重复读隔离级别下，事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，即使中途有其他事务插入了一条数据，是查询不出来这条数据的，所以就很好了避免幻读问题。</li>\n<li>针对当前读（select ... for update 等语句），是通过 next-key lock（记录锁+间隙锁）方式解决了幻读，因为当执行 select ... for update 语句的时候，会加上 next-key lock，如果有其他事务在 next-key lock 锁范围内插入了一条记录，那么这个插入语句就会被阻塞，无法成功插入，所以就很好了避免幻读问题。</li>\n</ul>\n<p>next-key包括两部分：行锁和间隙锁。行锁是加在索引上的锁，间隙锁是加在索引之间的。</p>\n<p><code>Serializable</code>隔离级别也可以避免幻读，会锁住整张表，并发性极低，一般不会使用。</p>\n<h2 id=\"13共享锁和排他锁\">13、共享锁和排他锁</h2>\n<p>SELECT 的读取锁定主要分为两种方式：共享锁和排他锁。</p>\n<pre><code class=\"language-sql\">select * from table where id&lt;6 lock in share mode;--共享锁\nselect * from table where id&lt;6 for update;--排他锁\n</code></pre>\n<p>这两种方式主要的不同在于<code>LOCK IN SHARE MODE </code>多个事务同时更新同一个表单时很容易造成死锁。</p>\n<p>申请排他锁的前提是，没有线程对该结果集的任何行数据使用排它锁或者共享锁，否则申请会受到阻塞。在进行事务操作时，MySQL会对查询结果集的每行数据添加排它锁，其他线程对这些数据的更改或删除操作会被阻塞（只能读操作），直到该语句的事务被<code>commit</code>语句或<code>rollback</code>语句结束为止。</p>\n<p><code>SELECT... FOR UPDATE</code> 使用注意事项：</p>\n<ol>\n<li><code>for update</code> 仅适用于innodb，且必须在事务范围内才能生效。</li>\n<li>根据主键进行查询，查询条件为<code>like</code>或者不等于，主键字段产生<strong>表锁</strong>。</li>\n<li>根据非索引字段进行查询，会产生<strong>表锁</strong>。</li>\n</ol>\n<h2 id=\"14bin-logredo-logundo-log\">14、bin log/redo log/undo log</h2>\n<p>MySQL日志主要包括查询日志、慢查询日志、事务日志、错误日志、二进制日志等。其中比较重要的是 <code>bin log</code>（二进制日志）和 <code>redo log</code>（重做日志）和 <code>undo log</code>（回滚日志）。</p>\n<p><strong>bin log</strong></p>\n<p><code>bin log</code>是MySQL数据库级别的文件，记录对MySQL数据库执行修改的所有操作，不会记录select和show语句，主要用于恢复数据库和同步数据库。</p>\n<p><strong>redo log</strong></p>\n<p><code>redo log</code>是innodb引擎级别，用来记录innodb存储引擎的事务日志，不管事务是否提交都会记录下来，用于数据恢复。当数据库发生故障，innoDB存储引擎会使用<code>redo log</code>恢复到发生故障前的时刻，以此来保证数据的完整性。将参数<code>innodb_flush_log_at_tx_commit</code>设置为1，那么在执行commit时会将<code>redo log</code>同步写到磁盘。</p>\n<p><strong>undo log</strong></p>\n<p>除了记录<code>redo log</code>外，当进行数据修改时还会记录<code>undo log</code>，<code>undo log</code>用于数据的撤回操作，它保留了记录修改前的内容。通过<code>undo log</code>可以实现事务回滚，并且可以根据<code>undo log</code>回溯到某个特定的版本的数据，<strong>实现MVCC</strong>。</p>\n<blockquote>\n<ul>\n<li><em><strong>实现事务回滚</strong></em>，保障事务的原子性。事务处理过程中，如果出现了错误或者用户执 行了 ROLLBACK 语句，MySQL 可以利用 undo log 中的历史数据将数据恢复到事务开始之前的状态。</li>\n<li><em><strong>实现 MVCC（多版本并发控制）关键因素之一</strong></em>。MVCC 是通过 ReadView + undo log 实现的。undo log 为每条记录保存多份历史数据，MySQL 在执行快照读（普通 select 语句）的时候，会根据事务的 Read View 里的信息，顺着 undo log 的版本链找到满足其可见性的记录。</li>\n</ul>\n<p><strong>Buffer Poll：</strong> MySQL 的数据都是存在磁盘中的，那么我们要更新一条记录的时候，得先要从磁盘读取该记录，然后在内存中修改这条记录。修改完这条记录后将记录放入缓存中，下次有查询语句命中了这条记录，直接读取缓存中的记录，就不需要从磁盘获取数据了。</p>\n<ul>\n<li>当读取数据时，如果数据存在于 Buffer Pool 中，客户端就会直接读取 Buffer Pool 中的数据，否则再去磁盘中读取。</li>\n<li>当修改数据时，如果数据存在于 Buffer Pool 中，那直接修改 Buffer Pool 中数据所在的页，然后将其页设置为脏页（该页的内存数据和磁盘上的数据已经不一致），为了减少磁盘I/O，不会立即将脏页写入磁盘，后续由后台线程选择一个合适的时机将脏页写入到磁盘。</li>\n</ul>\n<p><strong>redo log：</strong> Buffer Pool 是提高了读写效率没错，但是问题来了，Buffer Pool 是基于内存的，而内存总是不可靠，万一断电重启，还没来得及落盘的脏页数据就会丢失。\n为了防止断电导致数据丢失的问题，当有一条记录需要更新的时候，InnoDB 引擎就会先更新内存（同时标记为脏页），然后将本次对这个页的修改以 redo log 的形式记录下来，这个时候更新就算完成了。</p>\n<p><strong>bin log：</strong> MySQL 在完成一条更新操作后，Server 层还会生成一条 binlog，等之后事务提交的时候，会将该事物执行过程中产生的所有 binlog 统一写 入 binlog 文件。\nbinlog 文件是记录了所有数据库表结构变更和表数据修改的日志，不会记录查询类的操作，比如 SELECT 和 SHOW 操作。</p>\n</blockquote>\n<h2 id=\"15bin-log和redo-log有什么区别\">15、bin log和redo log有什么区别</h2>\n<ul>\n<li>\n<p>binlog 是 MySQL 的 Server 层实现的日志，所有存储引擎都可以使用；redo log 是 Innodb 存储引擎实现的日志；</p>\n</li>\n<li>\n<p>binlog 是追加写，写满一个文件，就创建一个新的文件继续写，不会覆盖以前的日志，保存的是全量的日志。redo log 是循环写，日志空间大小是固定，全部写满就从头开始，保存未被刷入磁盘的脏页日志。</p>\n</li>\n<li>\n<p><code>bin log</code>是逻辑日志，记录的是SQL语句的原始逻辑；<code>redo log</code>是物理日志，记录的是在某个数据页上做了什么修改。</p>\n</li>\n</ul>\n<h2 id=\"16讲一下mysql架构\">16、讲一下MySQL架构</h2>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/3feecf7a27754947b91011da413669b4.png\">\n</p>\n<p>MySQL主要分为 Server 层和存储引擎层：</p>\n<ul>\n<li><strong>Server 层</strong>：主要包括连接器、查询缓存、分析器、优化器、执行器等，所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图，函数等，还有一个通用的日志模块 binglog 日志模块。</li>\n<li><strong>存储引擎</strong>： 主要负责数据的存储和读取。server 层通过api与存储引擎进行通信。</li>\n</ul>\n<blockquote>\n<p>Server 层负责建立连接、分析和执行 SQL。MySQL 大多数的核心功能模块都在这实现，主要包括连接器，查询缓存、解析器、预处理器、优化器、执行器等。另外，所有的内置函数（如日期、时间、数学和加密函数等）和所有跨存储引擎的功能（如存储过程、触发器、视图等。）都在 Server 层实现。</p>\n<p>存储引擎层负责数据的存储和提取。支持 InnoDB、MyISAM、Memory 等多个存储引擎，不同的存储引擎共用一个 Server 层。现在最常用的存储引擎是 InnoDB，从 MySQL 5.5 版本开始， InnoDB 成为了 MySQL 的默认存储引擎。我们常说的索引数据结构，就是由存储引擎层实现的，不同的存储引擎支持的索引类型也不相同，比如 InnoDB 支持索引类型是 B+树 ，且是默认使用，也就是说在数据表中创建的主键索引和二级索引默认使用的是 B+ 树索引。</p>\n</blockquote>\n<p><strong>Server 层基本组件</strong></p>\n<ul>\n<li><strong>连接器：</strong> 当客户端连接 MySQL 时，server层会对其进行身份认证和权限校验。</li>\n<li><strong>查询缓存:</strong> 执行查询语句的时候，会先查询缓存，先校验这个 sql 是否执行过，如果有缓存这个 sql，就会直接返回给客户端，如果没有命中，就会执行后续的操作。</li>\n<li><strong>分析器:</strong> 没有命中缓存的话，SQL 语句就会经过分析器，主要分为两步，词法分析和语法分析，先看 SQL 语句要做什么，再检查 SQL 语句语法是否正确。</li>\n<li><strong>优化器：</strong> 优化器对查询进行优化，包括重写查询、决定表的读写顺序以及选择合适的索引等，生成执行计划。</li>\n<li><strong>执行器：</strong> 首先执行前会校验该用户有没有权限，如果没有权限，就会返回错误信息，如果有权限，就会根据执行计划去调用引擎的接口，返回结果。</li>\n</ul>\n<h2 id=\"17分库分表\">17、分库分表</h2>\n<p>当单表的数据量达到1000W或100G以后，优化索引、添加从库等可能对数据库性能提升效果不明显，此时就要考虑对其进行切分了。切分的目的就在于减少数据库的负担，缩短查询的时间。</p>\n<p>数据切分可以分为两种方式：垂直划分和水平划分。</p>\n<p><strong>垂直划分</strong></p>\n<p>垂直划分数据库是根据业务进行划分，例如购物场景，可以将库中涉及商品、订单、用户的表分别划分出成一个库，通过降低单库的大小来提高性能。同样的，分表的情况就是将一个大表根据业务功能拆分成一个个子表，例如商品基本信息和商品描述，商品基本信息一般会展示在商品列表，商品描述在商品详情页，可以将商品基本信息和商品描述拆分成两张表。</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/a5ceca78b476467ab019021543fa4244.png\">\n</p>\n<p><strong>优点</strong>：行记录变小，数据页可以存放更多记录，在查询时减少I/O次数。</p>\n<p><strong>缺点</strong>：</p>\n<ul>\n<li>主键出现冗余，需要管理冗余列；</li>\n<li>会引起表连接JOIN操作，可以通过在业务服务器上进行join来减少数据库压力；</li>\n<li>依然存在单表数据量过大的问题。</li>\n</ul>\n<p><strong>水平划分</strong></p>\n<p>水平划分是根据一定规则，例如时间或id序列值等进行数据的拆分。比如根据年份来拆分不同的数据库。每个数据库结构一致，但是数据得以拆分，从而提升性能。\n<img src=\"http://110.41.141.141:9000/weblog/weblog/72decfe442834c83aa1c8c3480fb85ef.png\">\n</p>\n<p><strong>优点</strong>：单库（表）的数据量得以减少，提高性能；切分出的表结构相同，程序改动较少。</p>\n<p><strong>缺点</strong>：</p>\n<ul>\n<li>分片事务一致性难以解决</li>\n<li>跨节点<code>join</code>性能差，逻辑复杂</li>\n<li>数据分片在扩容时需要迁移</li>\n</ul>\n<h2 id=\"18什么是分区表\">18、什么是分区表</h2>\n<p>分区是把一张表的数据分成N多个区块。分区表是一个独立的逻辑表，但是底层由多个物理子表组成。</p>\n<p>当查询条件的数据分布在某一个分区的时候，查询引擎只会去某一个分区查询，而不是遍历整个表。在管理层面，如果需要删除某一个分区的数据，只需要删除对应的分区即可。</p>\n<p>分区一般都是放在单机里的，用的比较多的是时间范围分区，方便归档。只不过分库分表需要代码实现，分区则是mysql内部实现。分库分表和分区并不冲突，可以结合使用。</p>\n<h2 id=\"19分区表类型\">19、分区表类型</h2>\n<p><strong>range分区</strong>，按照范围分区。比如按照时间范围分区</p>\n<pre><code class=\"language-sql\">CREATE TABLE test_range_partition(\n       id INT auto_increment,\n       createdate DATETIME,\n       primary key (id,createdate)\n   ) \n   PARTITION BY RANGE (TO_DAYS(createdate) ) (\n      PARTITION p201801 VALUES LESS THAN ( TO_DAYS('20180201') ),\n      PARTITION p201802 VALUES LESS THAN ( TO_DAYS('20180301') ),\n      PARTITION p201803 VALUES LESS THAN ( TO_DAYS('20180401') ),\n      PARTITION p201804 VALUES LESS THAN ( TO_DAYS('20180501') ),\n      PARTITION p201805 VALUES LESS THAN ( TO_DAYS('20180601') ),\n      PARTITION p201806 VALUES LESS THAN ( TO_DAYS('20180701') ),\n      PARTITION p201807 VALUES LESS THAN ( TO_DAYS('20180801') ),\n      PARTITION p201808 VALUES LESS THAN ( TO_DAYS('20180901') ),\n      PARTITION p201809 VALUES LESS THAN ( TO_DAYS('20181001') ),\n      PARTITION p201810 VALUES LESS THAN ( TO_DAYS('20181101') ),\n      PARTITION p201811 VALUES LESS THAN ( TO_DAYS('20181201') ),\n      PARTITION p201812 VALUES LESS THAN ( TO_DAYS('20190101') )\n   );\n</code></pre>\n<p>在<code>/var/lib/mysql/data/</code>可以找到对应的数据文件，每个分区表都有一个使用#分隔命名的表文件：</p>\n<pre><code class=\"language-sql\">   -rw-r----- 1 MySQL MySQL    65 Mar 14 21:47 db.opt\n   -rw-r----- 1 MySQL MySQL  8598 Mar 14 21:50 test_range_partition.frm\n   -rw-r----- 1 MySQL MySQL 98304 Mar 14 21:50 test_range_partition#P#p201801.ibd\n   -rw-r----- 1 MySQL MySQL 98304 Mar 14 21:50 test_range_partition#P#p201802.ibd\n   -rw-r----- 1 MySQL MySQL 98304 Mar 14 21:50 test_range_partition#P#p201803.ibd\n...\n</code></pre>\n<p><strong>list分区</strong></p>\n<p>list分区和range分区相似，主要区别在于list是枚举值列表的集合，range是连续的区间值的集合。对于list分区，分区字段必须是已知的，如果插入的字段不在分区时的枚举值中，将无法插入。</p>\n<pre><code class=\"language-sql\">create table test_list_partiotion\n   (\n       id int auto_increment,\n       data_type tinyint,\n       primary key(id,data_type)\n   )partition by list(data_type)\n   (\n       partition p0 values in (0,1,2,3,4,5,6),\n       partition p1 values in (7,8,9,10,11,12),\n       partition p2 values in (13,14,15,16,17)\n   );\n</code></pre>\n<p><strong>hash分区</strong></p>\n<p>可以将数据均匀地分布到预先定义的分区中。</p>\n<pre><code class=\"language-sql\">create table test_hash_partiotion\n   (\n       id int auto_increment,\n       create_date datetime,\n       primary key(id,create_date)\n   )partition by hash(year(create_date)) partitions 10;\n</code></pre>\n<h2 id=\"20分区的问题\">20、分区的问题</h2>\n<ul>\n<li>\n<p>打开和锁住所有底层表的成本可能很高。当查询访问分区表时，MySQL 需要打开并锁住所有的底层表，这个操作在分区过滤之前发生，所以无法通过分区过滤来降低此开销，会影响到查询速度。可以通过批量操作来降低此类开销，比如批量插入、<code>LOAD DATA INFILE</code>和一次删除多行数据。</p>\n</li>\n<li>\n<p>维护分区的成本可能很高。例如重组分区，会先创建一个临时分区，然后将数据复制到其中，最后再删除原分区。</p>\n</li>\n<li>\n<p>所有分区必须使用相同的存储引擎。</p>\n</li>\n</ul>\n<h2 id=\"21查询语句执行流程\">21、查询语句执行流程</h2>\n<p>查询语句的执行流程如下：权限校验、查询缓存、分析器、优化器、权限校验、执行器、引擎。</p>\n<p>举个例子，查询语句如下：</p>\n<pre><code class=\"language-sql\">select * from user where id &gt; 1 and name = '大彬';\n</code></pre>\n<ul>\n<li>\n<p>首先检查权限，没有权限则返回错误；</p>\n</li>\n<li>\n<p>MySQL8.0以前会查询缓存，缓存命中则直接返回，没有则执行下一步；</p>\n</li>\n<li>\n<p>词法分析和语法分析。提取表名、查询条件，检查语法是否有错误；</p>\n</li>\n<li>\n<p>两种执行方案，先查 <code>id &gt; 1</code> 还是 <code>name = '大彬'</code>，优化器根据自己的优化算法选择执行效率最好的方案；</p>\n</li>\n<li>\n<p>校验权限，有权限就调用数据库引擎接口，返回引擎的执行结果。</p>\n</li>\n</ul>\n<h2 id=\"22更新语句执行过程\">22、更新语句执行过程</h2>\n<p>更新语句执行流程如下：分析器、权限校验、执行器、引擎、<code>redo log</code>（<code>prepare</code>状态）、<code>binlog</code>、<code>redo log</code>（<code>commit</code>状态）</p>\n<p>举个例子，更新语句如下：</p>\n<pre><code class=\"language-sql\">update user set name = '大彬' where id = 1;\n</code></pre>\n<ol>\n<li>先查询到 id 为1的记录，有缓存会使用缓存。</li>\n<li>拿到查询结果，将 name 更新为大彬，然后调用引擎接口，写入更新数据，innodb 引擎将数据保存在内存中，同时记录<code>redo log</code>，此时<code>redo log</code>进入 <code>prepare</code>状态。</li>\n<li>执行器收到通知后记录<code>binlog</code>，然后调用引擎接口，提交<code>redo log</code>为<code>commit</code>状态。</li>\n<li>更新完成。</li>\n</ol>\n<p>为什么记录完<code>redo log</code>，不直接提交，而是先进入<code>prepare</code>状态？</p>\n<p>假设先写<code>redo log</code>直接提交，然后写<code>binlog</code>，写完<code>redo log</code>后，机器挂了，<code>binlog</code>日志没有被写入，那么机器重启后，这台机器会通过<code>redo log</code>恢复数据，但是这个时候<code>binlog</code>并没有记录该数据，后续进行机器备份的时候，就会丢失这一条数据，同时主从同步也会丢失这一条数据。</p>\n<h2 id=\"23exist和in的区别\">23、exist和in的区别</h2>\n<blockquote>\n<pre><code class=\"language-sql\">SELECT * FROM A WHERE cc IN (SELECT cc FROM B)；\n</code></pre>\n<pre><code class=\"language-sql\">SELECT * FROM A WHERE EXISTS (SELECT cc FROM B WHERE B.cc = A.cc)；\n</code></pre>\n<p><code>IN</code>是在A中选择一条记录，然后在B中查找该记录是否存在。<code>EXISTS</code>是在B中选择一条符合条件的记录，然后在A中查找该记录是否存在。</p>\n</blockquote>\n<ul>\n<li><strong>In</strong>：确定给定的值是否与子查询或列表中的值相匹配。in在查询的时候，首先查询子查询的表，然后将内表和外表做一个笛卡尔积，然后按照条件进行筛选。所以相对内表比较小的时候，in的速度较快。</li>\n<li><strong>exists</strong>：指定一个子查询，检测行的存在。循环遍历外表，然后看外表中的记录有没有和内表的数据一样的。匹配上就将结果放入结果集中。</li>\n</ul>\n<p><strong>子查询的表比较大的时候</strong>，使用<code>exists</code>可以有效减少总的循环次数来提升速度；<strong>当外查询的表比较大的时候</strong>，使用<code>in</code>可以有效减少对外查询表循环遍历来提升速度。</p>\n<h2 id=\"24mysql中int10和char10的区别\">24、MySQL中int(10)和char(10)的区别</h2>\n<p>int(10)中的10表示的是<strong>显示数据</strong>的长度，而char(10)表示的是<strong>存储数据</strong>的长度。</p>\n<h2 id=\"25truncatedelete与drop区别\">25、truncate、delete与drop区别</h2>\n<p><strong>相同点：</strong></p>\n<ol>\n<li><code>truncate</code>和不带<code>where</code>子句的<code>delete</code>、以及<code>drop</code>都会删除表内的数据。</li>\n<li><strong><code>drop</code>、<code>truncate</code>都是<code>DDL</code>语句（数据定义语言）</strong>，执行后会自动提交。</li>\n</ol>\n<p><strong>不同点：</strong></p>\n<ol>\n<li>truncate 和 delete 只删除数据不删除表的结构；<strong>drop 语句将删除表的结构、被依赖的约束、触发器、索引</strong>；</li>\n<li>一般来说，<strong>执行速度: drop &gt; truncate &gt; delete</strong>。</li>\n</ol>\n<h2 id=\"26having和where区别\">26、having和where区别？</h2>\n<ul>\n<li>二者作用的对象不同，<code>where</code>子句作用于表和视图，<code>having</code>作用于组。</li>\n<li><code>where</code>在数据分组前进行过滤，<code>having</code>在数据分组后进行过滤。</li>\n</ul>\n<h2 id=\"27什么是mysql主从同步\">27、什么是MySQL主从同步？</h2>\n<p>主从同步使得数据可以从一个数据库服务器复制到其他服务器上，在复制数据时，一个服务器充当主服务器（<code>master</code>），其余的服务器充当从服务器（<code>slave</code>）。</p>\n<p>因为复制是异步进行的，所以从服务器不需要一直连接着主服务器，从服务器甚至可以通过拨号断断续续地连接主服务器。通过配置文件，可以指定复制所有的数据库，某个数据库，甚至是某个数据库上的某个表。</p>\n<hr />\n<p><em><strong>MySQL 的主从复制</strong></em>依赖于 binlog ，也就是记录 MySQL 上的所有变化并以二进制形式保存在磁盘上。复制的过程就是将 binlog 中的数据从主库传输到从库上。</p>\n<p>这个过程一般是异步的，也就是主库上执行事务操作的线程不会等待复制 binlog 的线程同步完成。\n<img src=\"http://110.41.141.141:9000/weblog/weblog/69437c3c7def4cc1bec0f1de78dc13d3.png\">\n</p>\n<p>MySQL 集群的主从复制过程梳理成 3 个阶段：</p>\n<ul>\n<li><strong>写入 Binlog</strong>：主库写 binlog 日志，提交事务，并更新本地存储数据。</li>\n<li><strong>同步 Binlog</strong>：把 binlog 复制到所有从库上，每个从库把 binlog 写到暂存日志中。</li>\n<li><strong>回放 Binlog</strong>：回放 binlog，并更新存储引擎中的数据。</li>\n</ul>\n<p>具体详细过程如下：</p>\n<ul>\n<li>MySQL 主库在收到客户端提交事务的请求之后，会先写入 binlog，再提交事务，更新存储引擎中的数据，事务提交完成后，返回给客户端“操作成功”的响应。</li>\n<li>从库会创建一个专门的 I/O 线程，连接主库的 log dump 线程，来接收主库的 binlog 日志，再把 binlog 信息写入 relay log 的中继日志里，再返回给主库“复制成功”的响应。</li>\n<li>从库会创建一个用于回放 binlog 的线程，去读 relay log 中继日志，然后回放 binlog 更新存储引擎中的数据，最终实现主从的数据一致性。</li>\n</ul>\n<h2 id=\"28为什么要做主从同步\">28、为什么要做主从同步？</h2>\n<ol>\n<li><strong>读写分离</strong>，使数据库能支撑更大的并发。</li>\n<li><strong>在主服务器上生成实时数据，而在从服务器上分析这些数据</strong>，从而提高主服务器的性能。</li>\n<li><strong>数据备份</strong>，保证数据的安全。</li>\n</ol>\n<h2 id=\"29乐观锁和悲观锁是什么\">29、乐观锁和悲观锁是什么</h2>\n<p>数据库中的并发控制是确保在多个事务同时存取数据库中同一数据时不破坏事务的隔离性和一致性以及数据库的一致性。乐观锁和悲观锁是并发控制主要采用的技术手段。</p>\n<ul>\n<li><strong>悲观锁</strong>：假定会发生并发冲突，会对操作的数据进行加锁，直到提交事务，才会释放锁，其他事务才能进行修改。实现方式：使用数据库中的锁机制。</li>\n<li><strong>乐观锁</strong>：假设不会发生并发冲突，只在提交操作时检查数据是否被修改过。给表增加<code>version</code>字段，在修改提交之前检查<code>version</code>与原来取到的<code>version</code>值是否相等，若相等，表示数据没有被修改，可以更新，否则，数据为脏数据，不能更新。实现方式：乐观锁一般使用版本号机制或<code>CAS</code>算法实现。</li>\n</ul>\n<h2 id=\"30用过processlist吗\">30、用过processlist吗</h2>\n<p><code>show processlist</code> 或 <code>show full processlist</code> 可以查看当前 MySQL 是否有压力，正在运行的<code>SQL</code>，有没有慢<code>SQL</code>正在执行。返回参数如下：</p>\n<ol>\n<li>\n<p><strong>id</strong>：线程ID，可以用<code>kill id</code>杀死某个线程</p>\n</li>\n<li>\n<p><strong>db</strong>：数据库名称</p>\n</li>\n<li>\n<p><strong>user</strong>：数据库用户</p>\n</li>\n<li>\n<p><strong>host</strong>：数据库实例的IP</p>\n</li>\n<li>\n<p><strong>command</strong>：当前执行的命令，比如<code>Sleep</code>，<code>Query</code>，<code>Connect </code>等</p>\n</li>\n<li>\n<p><strong>time</strong>：消耗时间，单位秒</p>\n</li>\n<li>\n<p>state</p>\n<p>：执行状态，主要有以下状态：</p>\n<ul>\n<li>Sleep，线程正在等待客户端发送新的请求</li>\n<li>Locked，线程正在等待锁</li>\n<li>Sending data，正在处理<code>SELECT</code>查询的记录，同时把结果发送给客户端</li>\n<li>Kill，正在执行<code>kill</code>语句，杀死指定线程</li>\n<li>Connect，一个从节点连上了主节点</li>\n<li>Quit，线程正在退出</li>\n<li>Sorting for group，正在为<code>GROUP BY</code>做排序</li>\n<li>Sorting for order，正在为<code>ORDER BY</code>做排序</li>\n</ul>\n</li>\n<li>\n<p><strong>info</strong>：正在执行的<code>SQL</code>语句</p>\n</li>\n</ol>\n<h2 id=\"31mysql查询-limit-100010-和limit-10-速度一样快吗\">31、MySQL查询 limit 1000,10 和limit 10 速度一样快吗</h2>\n<p>两种查询方式。对应 <code>limit offset, size</code> 和 <code>limit size</code> 两种方式。</p>\n<p>而其实 <code>limit size</code> ，相当于 <code>limit 0, size</code>。也就是从0开始取size条数据。</p>\n<p>也就是说，两种方式的<strong>区别在于offset是否为0。</strong></p>\n<p>先来看下limit sql的内部执行逻辑。</p>\n<p>MySQL内部分为<strong>server层</strong>和<strong>存储引擎层</strong>。一般情况下存储引擎都用innodb。</p>\n<p>server层有很多模块，其中需要关注的是<strong>执行器</strong>是用于跟存储引擎打交道的组件。</p>\n<p>执行器可以通过调用存储引擎提供的接口，将一行行数据取出，当这些数据完全符合要求（比如满足其他where条件），则会放到<strong>结果集</strong>中，最后返回给调用mysql的<strong>客户端</strong>。</p>\n<p>以主键索引的limit执行过程为例：</p>\n<p>执行<code>select * from xxx order by id limit 0, 10;</code>，select后面带的是<strong>星号</strong>，也就是要求获得行数据的<strong>所有字段信息。</strong></p>\n<p>server层会调用innodb的接口，在innodb里的主键索引中获取到第0到10条<strong>完整行数据</strong>，依次返回给server层，并放到server层的结果集中，返回给客户端。</p>\n<p>把offset搞大点，比如执行的是：<code>select * from xxx order by id limit 500000, 10;</code></p>\n<p>server层会调用innodb的接口，由于这次的offset=500000，会在innodb里的主键索引中获取到第0到（500000 + 10）条<strong>完整行数据</strong>，<strong>返回给server层之后根据offset的值挨个抛弃，最后只留下最后面的size条</strong>，也就是10条数据，放到server层的结果集中，返回给客户端。</p>\n<p>可以看出，当offset非0时，server层会从引擎层获取到<strong>很多无用的数据</strong>，而获取的这些无用数据都是要耗时的。</p>\n<p>因此，mysql查询中 limit 1000,10 会比 limit 10 更慢。原因是 limit 1000,10 会取出1000+10条数据，并抛弃前1000条，这部分耗时更大。</p>\n<h2 id=\"32深分页怎么优化\">32、深分页怎么优化</h2>\n<p>还是以上面的SQL为空：<code>select * from xxx order by id limit 500000, 10;</code></p>\n<p><strong>方法一</strong>：</p>\n<p>从上面的分析可以看出，当offset非常大时，server层会从引擎层获取到很多无用的数据，而当select后面是*号时，就需要拷贝完整的行信息，<strong>拷贝完整数据</strong>相比<strong>只拷贝行数据里的其中一两个列字段</strong>更耗费时间。</p>\n<p>因为前面的offset条数据最后都是不要的，没有必要拷贝完整字段，所以可以将sql语句修改成：</p>\n<pre><code class=\"language-sql\">select * from xxx  where id &gt;=(select id from xxx order by id limit 500000, 1) order by id limit 10;\n</code></pre>\n<p>先执行子查询 <code>select id from xxx by id limit 500000, 1</code>, 这个操作，其实也是将在innodb中的主键索引中获取到<code>500000+1</code>条数据，然后server层会抛弃前500000条，只保留最后一条数据的id。</p>\n<p>但不同的地方在于，在返回server层的过程中，只会拷贝数据行内的id这一列，而不会拷贝数据行的所有列，当数据量较大时，这部分的耗时还是比较明显的。</p>\n<p>在拿到了上面的id之后，假设这个id正好等于500000，那sql就变成了</p>\n<pre><code class=\"language-sql\">select * from xxx  where id &gt;=500000 order by id limit 10;\n</code></pre>\n<p>这样innodb再走一次<strong>主键索引</strong>，通过B+树快速定位到id=500000的行数据，时间复杂度是lg(n)，然后向后取10条数据。</p>\n<p><strong>方法二：</strong></p>\n<p>将所有的数据<strong>根据id主键进行排序</strong>，然后分批次取，将当前批次的最大id作为下次筛选的条件进行查询。</p>\n<pre><code class=\"language-sql\">select * from xxx where id &gt; start_id order by id limit 10;\n</code></pre>\n<p>通过主键索引，每次定位到start_id的位置，然后往后遍历10个数据，这样不管数据多大，查询性能都较为稳定。</p>\n<ul>\n<li>\n<p>子查询+索引</p>\n<blockquote>\n<p>思路：通过将 select * 转变为 select id，把符合条件的 id 筛选出来后，最后通过嵌套查询的方式按顺序取出 id 对应的行。</p>\n<pre><code class=\"language-sql\">-- 优化前\nselect *\nfrom people\norder by create_time desc\nlimit 5000000, 10;\n\n-- 优化后\nselect a.*\nfrom people a\ninner join(\n select id\n from people\n order by create_time desc\n limit 5000000, 10\n) b ON a.id = b.id;\n</code></pre>\n</blockquote>\n</li>\n<li>\n<p>联合索引</p>\n<blockquote>\n<p>刚才我们优化后的 SQL 语句如下，是没有 where 条件的。</p>\n<pre><code class=\"language-sql\">select a.*\nfrom people a\ninner join(\n select id\n from people\n order by create_time desc\n limit 5000010, 10\n) b ON a.id = b.id;\n</code></pre>\n</blockquote>\n<blockquote>\n<p>为了更接近现实场景，假设我们要把 status = 1 的人筛出来，SQL 就要这么写：</p>\n<pre><code class=\"language-sql\">select a.*\nfrom people a\ninner join(\n select id\n from people\n where status=1\n order by create_time desc\n limit 5000010, 10\n) b ON a.id = b.id;\n</code></pre>\n</blockquote>\n<blockquote>\n<p>为了加速这个查询，我们可以创建一个 create_time 和 status 的联合索引，对应的 SQL 语句是：</p>\n<pre><code class=\"language-sql\">alter table people add index create_time_status(create_time, status);\n</code></pre>\n</blockquote>\n</li>\n<li>\n<p>where id &gt; start_id</p>\n<blockquote>\n<p>将上一页的查询结果中的最大 id 作为下一页查询的 where 条件，这样可以大幅减少扫描行数，提高查询性能。</p>\n<pre><code class=\"language-sql\">select a.*\nfrom people a\ninner join(\n select id\n from people\n where status=1\n       and id &gt; ${id}\n order by create_time desc\n limit 10\n) b ON a.id = b.id;\n</code></pre>\n</blockquote>\n</li>\n</ul>\n<h2 id=\"33高度为3的b树可以存放多少数据\">33、高度为3的B+树，可以存放多少数据</h2>\n<p>InnoDB存储引擎有自己的最小储存单元——页（Page）。</p>\n<p>查询InnoDB页大小的命令如下：</p>\n<pre><code>mysql&gt; show global status like 'innodb_page_size';\n+------------------+-------+\n| Variable_name    | Value |\n+------------------+-------+\n| Innodb_page_size | 16384 |\n+------------------+-------+\n</code></pre>\n<p>可以看出 <strong>innodb 默认的一页大小为 16384B = 16384/1024 = 16kb</strong>。</p>\n<p>在MySQL中，B+树一个节点的大小设为一页或页的倍数最为合适。因为如果一个节点的大小 &lt; 1页，那么读取这个节点的时候其实读取的还是一页，这样就造成了资源的浪费。</p>\n<p>B+树中<strong>非叶子节点存的是key + 指针</strong>；<strong>叶子节点存的是数据行</strong>。</p>\n<p>对于叶子节点，如果一行数据大小为1k，那么一页就能存16条数据。</p>\n<p>对于非叶子节点，如果key使用的是bigint，则为8字节，指针在MySQL中为6字节，一共是14字节，则16k能存放 16 * 1024 / 14 = 1170 个索引指针。</p>\n<p>于是可以算出，对于一颗高度为2的B+树，根节点存储索引指针节点，那么它有1170个叶子节点存储数据，每个叶子节点可以存储16条数据，一共 1170 x 16 = 18720 条数据。而对于高度为3的B+树，就可以存放 1170 x 1170 x 16 = 21902400 条数据（<strong>两千多万条数据</strong>），也就是对于两千多万条的数据，我们只需要<strong>高度为3</strong>的B+树就可以完成，通过主键查询只需要3次IO操作就能查到对应数据。</p>\n<p>所以在 InnoDB 中B+树高度一般为3层时，就能满足千万级的数据存储。</p>\n<h2 id=\"34mysql单表多大进行分库分表\">34、MySQL单表多大进行分库分表</h2>\n<p>目前主流的有两种说法：</p>\n<ol>\n<li>MySQL 单表数据量大于 2000 万行，性能会明显下降，考虑进行分库分表。</li>\n<li>阿里巴巴《Java 开发手册》提出单表行数超过 500 万行或者单表容量超过 2GB，才推荐进行分库分表。</li>\n</ol>\n<p>事实上，这个数值和实际记录的条数无关，而与 MySQL 的配置以及机器的硬件有关。因为MySQL为了提高性能，会将表的索引装载到内存中。在InnoDB buffer size 足够的情况下，其能完成全加载进内存，查询不会有问题。但是，当单表数据库到达某个量级的上限时，导致内存无法存储其索引，使得之后的 SQL 查询会产生磁盘 IO，从而导致性能下降。当然，这个还有具体的表结构的设计有关，最终导致的问题都是内存限制。</p>\n<p>因此，对于分库分表，需要结合实际需求，不宜过度设计，在项目一开始不采用分库与分表设计，而是随着业务的增长，在无法继续优化的情况下，再考虑分库与分表提高系统的性能。对此，阿里巴巴《Java 开发手册》补充到：如果预计三年后的数据量根本达不到这个级别，请不要在创建表时就分库分表。</p>\n<p>至于MySQL单表多大进行分库分表，应当根据机器资源进行评估。</p>\n<h2 id=\"35大表查询慢怎么优化\">35、大表查询慢怎么优化</h2>\n<p>某个表有近千万数据，查询比较慢，如何优化？</p>\n<p>当MySQL单表记录数过大时，数据库的性能会明显下降，一些常见的优化措施如下：</p>\n<ul>\n<li><strong>合理建立索引</strong>。在合适的字段上建立索引，例如在WHERE和ORDER BY命令上涉及的列建立索引，可根据EXPLAIN来查看是否用了索引还是全表扫描</li>\n<li><strong>索引优化，SQL优化</strong>。索引要符合最左匹配原则等，参考：<a href=\"https://topjavaer.cn/database/mysql.html#什么是覆盖索引\" ref=\"nofollow\" target=\"_blank\">https://topjavaer.cn/database/mysql.html#什么是覆盖索引open in new window</a><span><svg xmlns=\"http://www.w3.org/2000/svg\" class=\"inline ml-1\" style=\"color: #aaa;\" aria-hidden=\"true\" focusable=\"false\" x=\"0px\" y=\"0px\" viewBox=\"0 0 100 100\" width=\"15\" height=\"15\" class=\"icon outbound\"><path fill=\"currentColor\" d=\"M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z\"></path> <polygon fill=\"currentColor\" points=\"45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9\"></polygon></svg> <span class=\"sr-only\"></span></span></li>\n<li><strong>建立分区</strong>。对关键字段建立水平分区，比如时间字段，若查询条件往往通过时间范围来进行查询，能提升不少性能</li>\n<li><strong>利用缓存</strong>。利用Redis等缓存热点数据，提高查询效率</li>\n<li><strong>限定数据的范围</strong>。比如：用户在查询历史信息的时候，可以控制在一个月的时间范围内</li>\n<li><strong>读写分离</strong>。经典的数据库拆分方案，主库负责写，从库负责读</li>\n<li>通过<strong>分库分表</strong>的方式进行优化，主要有垂直拆分和水平拆分</li>\n<li><strong>数据异构到es</strong></li>\n<li><strong>冷热数据分离</strong>。几个月之前不常用的数据放到冷库中，最新的数据或比较新的数据放到热库中</li>\n</ul>\n<blockquote>\n<ul>\n<li>\n<p>通过 explain 执行结果，查看 sql 是否走索引，如果不走索引，考虑增加索引。</p>\n</li>\n<li>\n<p>可以通过建立联合索引，实现覆盖索引优化，减少回表，使用联合索引符合最左匹配原则，不然会索引失效</p>\n</li>\n<li>\n<p>避免索引失效，比如不要用左模糊匹配、函数计算、表达式计算等等。</p>\n</li>\n<li>\n<p>联表查询最好要以小表驱动大表，并且被驱动表的字段要有索引，当然最好通过冗余字段的设计，避免联表查询。</p>\n</li>\n<li>\n<p>针对 limit n,y 深分页的查询优化，可以把Limit查询转换成某个位置的查询：select * from tb_sku where id&gt;20000 limit 10，该方案适用于主键自增的表，</p>\n</li>\n<li>\n<p>将字段多的表分解成多个表，有些字段使用频率高，有些低，数据量大时，会由于使用频率低的存在而变慢，可以考虑分开</p>\n</li>\n</ul>\n</blockquote>\n<h2 id=\"36说说count1count和count字段名的区别\">36、说说count(1)、count(*)和count(字段名)的区别</h2>\n<blockquote>\n<ul>\n<li>count(1)、 count(*)、 count(主键字段)在执行的时候，如果表里存在二级索引，优化器就会选择二级索引进行扫描。</li>\n<li>所以，如果要执行 count(1)、 count(*)、 count(主键字段) 时，尽量在数据表上建立二级索引，这样优化器会自动采用 key_len 最小的二级索引进行扫描，相比于扫描主键索引效率会高一些。</li>\n<li>再来，就是不要使用 count(字段) 来统计记录个数，因为它的效率是最差的，会采用全表扫描的方式来统计。如果你非要统计表中该字段不为 NULL 的记录个数，建议给这个字段建立一个二级索引。</li>\n</ul>\n</blockquote>\n<p>嗯，先说说count(1) and count(字段名)的区别。</p>\n<p>两者的主要区别是</p>\n<ol>\n<li>count(1) 会统计表中的所有的记录数，包含字段为null 的记录。</li>\n<li>count(字段名) 会统计该字段在表中出现的次数，忽略字段为null 的情况。即不统计字段为null 的记录。</li>\n</ol>\n<p>接下来看看三者之间的区别。</p>\n<p>执行效果上：</p>\n<ul>\n<li>count(*)包括了所有的列，相当于行数，在统计结果的时候，<strong>不会忽略列值为NULL</strong></li>\n<li>count(1)包括了忽略所有列，用1代表代码行，在统计结果的时候，<strong>不会忽略列值为NULL</strong></li>\n<li>count(字段名)只包括列名那一列，在统计结果的时候，会忽略列值为空（这里的空不是只空字符串或者0，而是表示null）的计数，<strong>即某个字段值为NULL时，不统计</strong>。</li>\n</ul>\n<p>执行效率上：</p>\n<ul>\n<li>列名为主键，count(字段名)会比count(1)快</li>\n<li>列名不为主键，count(1)会比count(列名)快</li>\n<li>如果表多个列并且没有主键，则 count(1) 的执行效率优于 count(*)</li>\n<li>如果有主键，则 select count(主键)的执行效率是最优的</li>\n<li>如果表只有一个字段，则 select count(*)最优。</li>\n</ul>\n<p><strong>COUNT(<code>*</code>)是SQL92定义的标准统计行数的语法，效率高，MySQL对它进行了很多优化，MyISAM中会直接把表的总行数单独记录下来供COUNT(*)查询，而InnoDB则会在扫表的时候选择最小的索引来降低成本</strong>。</p>\n<h2 id=\"37mysql中datetime-和-timestamp有什么区别\">37、MySQL中DATETIME 和 TIMESTAMP有什么区别</h2>\n<p>嗯，<code>TIMESTAMP</code>和<code>DATETIME</code>都可以用来存储时间，它们主要有以下区别：</p>\n<p>1.表示范围</p>\n<ul>\n<li>DATETIME：1000-01-01 00:00:00.000000 到 9999-12-31 23:59:59.999999</li>\n<li>TIMESTAMP：'1970-01-01 00:00:01.000000' UTC 到 '2038-01-09 03:14:07.999999' UTC</li>\n</ul>\n<p><code>TIMESTAMP</code>支持的时间范围比<code>DATATIME</code>要小，容易出现超出的情况。</p>\n<p>2.空间占用</p>\n<ul>\n<li>TIMESTAMP ：占 4 个字节</li>\n<li>DATETIME：在 MySQL 5.6.4 之前，占 8 个字节 ，之后版本，占 5 个字节</li>\n</ul>\n<p>3.存入时间是否会自动转换</p>\n<p><code>TIMESTAMP</code>类型在默认情况下，insert、update 数据时，<code>TIMESTAMP</code>列会自动以当前时间（<code>CURRENT_TIMESTAMP</code>）填充/更新。<code>DATETIME</code>则不会做任何转换，也不会检测时区，你给什么数据，它存什么数据。</p>\n<p>4.<code>TIMESTAMP</code>比较受时区timezone的影响以及MYSQL版本和服务器的SQL MODE的影响。因为<code>TIMESTAMP</code>存的是时间戳，在不同的时区得出的时间不一致。</p>\n<p>5.如果存进NULL，两者实际存储的值不同。</p>\n<ul>\n<li>TIMESTAMP：会自动存储当前时间 now() 。</li>\n<li>DATETIME：不会自动存储当前时间，会直接存入 NULL 值。</li>\n</ul>\n<h2 id=\"38说说为什么不建议用外键\">38、说说为什么不建议用外键</h2>\n<p>外键是一种约束，这个约束的存在，会保证表间数据的关系始终完整。外键的存在，并非全然没有优点。</p>\n<p>外键可以保证数据的完整性和一致性，级联操作方便。而且使用外键可以将数据完整性判断托付给了数据库完成，减少了程序的代码量。</p>\n<p>虽然外键能够保证数据的完整性，但是会给系统带来很多缺陷。</p>\n<p>1、并发问题。在使用外键的情况下，每次修改数据都需要去另外一个表检查数据，需要获取额外的锁。若是在高并发大流量事务场景，使用外键更容易造成死锁。</p>\n<p>2、扩展性问题。比如从<code>MySQL</code>迁移到<code>Oracle</code>，外键依赖于数据库本身的特性，做迁移可能不方便。</p>\n<p>3、不利于分库分表。在水平拆分和分库的情况下，外键是无法生效的。将数据间关系的维护，放入应用程序中，为将来的分库分表省去很多的麻烦。</p>\n<h2 id=\"39使用自增主键有什么好处\">39、使用自增主键有什么好处</h2>\n<p>自增主键可以让主键索引尽量地保持递增顺序插入，避免了页分裂，因此索引更紧凑，在查询的时候，效率也就更高。</p>\n<h2 id=\"40自增主键保存在什么地方\">40、自增主键保存在什么地方</h2>\n<p>不同的引擎对于自增值的保存策略不同：</p>\n<ul>\n<li><strong>MyISAM引擎的自增值保存在数据文件中</strong>。</li>\n<li>在MySQL8.0以前，InnoDB引擎的自增值是存在内存中。MySQL重启之后内存中的这个值就丢失了，每次重启后第一次打开表的时候，会找自增值的最大值max(id)，然后将最大值加1作为这个表的自增值；MySQL8.0版本会将自增值的变更记录在redo log中，重启时依靠redo log恢复。</li>\n</ul>\n<h2 id=\"41自增主键一定是连续的吗\">41、自增主键一定是连续的吗</h2>\n<p>不一定，有几种情况会导致自增主键不连续。</p>\n<p>1、<strong>唯一键冲突导致自增主键不连续</strong>。当我们向一个自增主键的InnoDB表中插入数据的时候，如果违反表中定义的唯一索引的唯一约束，会导致插入数据失败。此时表的自增主键的键值是会向后加1滚动的。下次再次插入数据的时候，就不能再使用上次因插入数据失败而滚动生成的键值了，必须使用新滚动生成的键值。</p>\n<p>2、<strong>事务回滚导致自增主键不连续</strong>。当我们向一个自增主键的InnoDB表中插入数据的时候，如果显式开启了事务，然后因为某种原因最后回滚了事务，此时表的自增值也会发生滚动，而接下里新插入的数据，也将不能使用滚动过的自增值，而是需要重新申请一个新的自增值。</p>\n<p>3、<strong>批量插入导致自增值不连续</strong>。MySQL有一个批量申请自增id的策略：</p>\n<ul>\n<li>语句执行过程中，第一次申请自增id，分配1个自增id</li>\n<li>1个用完以后，第二次申请，会分配2个自增id</li>\n<li>2个用完以后，第三次申请，会分配4个自增id</li>\n<li>依次类推，每次申请都是上一次的两倍（最后一次申请不一定全部使用）</li>\n</ul>\n<p>如果下一个事务再次插入数据的时候，则会基于上一个事务申请后的自增值基础上再申请。此时就出现自增值不连续的情况出现。</p>\n<p>4、<strong>自增步长不是1</strong>，也会导致自增主键不连续。</p>\n<h2 id=\"42innodb的自增值为什么不能回收利用\">42、InnoDB的自增值为什么不能回收利用</h2>\n<p><strong>主要为了提升插入数据的效率和并行度</strong>。</p>\n<p>假设有两个并行执行的事务，在申请自增值的时候，为了避免两个事务申请到相同的自增 id，肯定要加锁，然后顺序申请。</p>\n<p>假设事务 A 申请到了 id=2， 事务 B 申请到 id=3，那么这时候表 t 的自增值是 4，之后继续执行。</p>\n<p>事务 B 正确提交了，但事务 A 出现了唯一键冲突。</p>\n<p>如果允许事务 A 把自增 id 回退，也就是把表 t 的当前自增值改回 2，那么就会出现这样的情况：表里面已经有 id=3 的行，而当前的自增 id 值是 2。</p>\n<p>接下来，继续执行的其他事务就会申请到 id=2，然后再申请到 id=3。这时，就会出现插入语句报错“主键冲突”。</p>\n<p>而为了解决这个主键冲突，有两种方法：</p>\n<ul>\n<li>每次申请 id 之前，先判断表里面是否已经存在这个 id。如果存在，就跳过这个 id。但是，这个方法的成本很高。因为，本来申请 id 是一个很快的操作，现在还要再去主键索引树上判断 id 是否存在。</li>\n<li>把自增 id 的锁范围扩大，必须等到一个事务执行完成并提交，下一个事务才能再申请自增 id。这个方法的问题，就是锁的粒度太大，系统并发能力大大下降。</li>\n</ul>\n<p>可见，这两个方法都会导致性能问题。</p>\n<p>因此，InnoDB 放弃了“允许自增 id 回退”这个设计，语句执行失败也不回退自增 id。</p>\n<h2 id=\"43mysql数据如何同步到redis缓存\">43、MySQL数据如何同步到Redis缓存</h2>\n<p>有两种方案：</p>\n<p>1、通过MySQL自动同步刷新Redis，<strong>MySQL触发器+UDF函数</strong>实现。</p>\n<blockquote>\n<p>UDF函数：常见的函数类型，可以操作单个数据行，且产生一个数据行作为输出，大多数函数为这一类。</p>\n</blockquote>\n<p>过程大致如下：</p>\n<ol>\n<li>在MySQL中对要操作的数据设置触发器Trigger，监听操作</li>\n<li>客户端向MySQL中写入数据时，触发器会被触发，触发之后调用MySQL的UDF函数</li>\n<li>UDF函数可以把数据写入到Redis中，从而达到同步的效果</li>\n</ol>\n<p>2、<strong>解析MySQL的binlog</strong>，实现将数据库中的数据同步到Redis。可以通过canal实现。canal是阿里巴巴旗下的一款开源项目，基于数据库增量日志解析，提供增量数据订阅&amp;消费。</p>\n<p>canal的原理如下：</p>\n<ol>\n<li>canal模拟mysql slave的交互协议，伪装自己为mysql slave，向mysql master发送dump协议</li>\n<li>mysql master收到dump请求，开始推送binary log给canal</li>\n<li>canal解析binary log对象（原始为byte流），将数据同步写入Redis。</li>\n</ol>\n<h2 id=\"44为什么阿里java手册禁止使用存储过程\">44、为什么阿里Java手册禁止使用存储过程</h2>\n<p>先看看什么是存储过程。</p>\n<p>存储过程是在大型数据库系统中，一组为了完成特定功能的SQL 语句集，它存储在数据库中，一次编译后永久有效，用户通过指定存储过程的名字并给出参数（如果该存储过程带有参数）来执行它。</p>\n<p>存储过程主要有以下几个缺点。</p>\n<ol>\n<li><strong>存储过程难以调试</strong>。存储过程的开发一直缺少有效的 IDE 环境。SQL 本身经常很长，调试时要把句子拆开分别独立执行，非常麻烦。</li>\n<li><strong>移植性差</strong>。存储过程的移植困难，一般业务系统总会不可避免地用到数据库独有的特性和语法，更换数据库时这部分代码就需要重写，成本较高。</li>\n<li><strong>管理困难</strong>。存储过程的目录是扁平的，而不是文件系统那样的树形结构，脚本少的时候还好办，一旦多起来，目录就会陷入混乱。</li>\n<li>存储过程是<strong>只优化一次</strong>，有的时候随着数据量的增加或者数据结构的变化，原来存储过程选择的执行计划也许并不是最优的了，所以这个时候需要手动干预或者重新编译了。</li>\n</ol>\n<h2 id=\"45mysql-update-是锁行还是锁表\">45、MySQL update 是锁行还是锁表？</h2>\n<p>首先，InnoDB行锁是通过给索引上的索引项加锁来实现的，只有通过索引条件检索数据，InnoDB才使用行级锁，否则，InnoDB将使用表锁。</p>\n<ol>\n<li>当执行update语句时，where中的过滤条件列，如果用到索引，就是锁行；如果无法用索引，就是锁表。</li>\n<li>如果两个update语句同时执行，第一个先执行触发行锁，但是第二个没有索引触发表锁，因为有个行锁住了，所以还是会等待行锁释放，才能锁表。</li>\n<li>当执行insert或者delete语句时，锁行。</li>\n</ol>\n<h2 id=\"46selectfor-update会锁表还是锁行\">46、select...for update会锁表还是锁行？</h2>\n<p>如果查询条件用了索引/主键，那么<code>select ... for update</code>就会加行锁。</p>\n<p>如果是普通字段(没有索引/主键)，那么<code>select ..... for update</code>就会加表锁。</p>\n<h2 id=\"47mysql的binlog有几种格式分别有什么区别\">47、MySQL的binlog有几种格式？分别有什么区别？</h2>\n<p>有三种格式，<strong>statement，row和mixed</strong>。</p>\n<ul>\n<li>statement：<strong>每一条会修改数据的sql都会记录在binlog中</strong>。不需要记录每一行的变化，减少了binlog日志量，节约了IO，提高性能。由于sql的执行是有上下文的，因此<strong>在保存的时候需要保存相关的信息</strong>，同时还有一些使用了函数之类的语句无法被记录复制。</li>\n<li>row：<strong>不记录sql语句上下文相关信息，仅保存哪条记录被修改</strong>。记录单元为每一行的改动，由于很多操作，会导致大量行的改动(比如alter table)，因此这种模式的文件保存的信息太多，日志量太大。</li>\n<li>mixed：一种折中的方案，普通操作使用statement记录，当无法使用statement的时候使用row。</li>\n</ul>\n<h2 id=\"48阿里手册为什么禁止使用-count列名或-count常量来替代-count\">48、阿里手册为什么禁止使用 count(列名)或 count(常量)来替代 count(*)</h2>\n<p>先看下这几种方式的区别。</p>\n<p>count(主键id)：InnoDB引擎会遍历整张表，把每一行id值都取出来，返给server层。server层拿到id后，判断是不可能为空的，就按行累加，不再对每个值进行NULL判断。</p>\n<p>count(常量)：InnoDB引擎会遍历整张表，但不取值。server层对于返回的每一行，放一个常量进去，判断是不可能为空的，按行累加，不再对每个值进行NULL判断。count(常量)比count(主键id)执行的要快，因为从引擎放回id会涉及解析数据行，以及拷贝字段值的操作。</p>\n<p>count(字段)：全表扫描，分情况讨论。</p>\n<p>1、如果参数字段定义NOT NULL，判断是不可能为空的，按行累加，不再对每个值进行NULL判断。 2、如果参数字段定义允许为NULL，那么执行的时候，判断可能是NULL，还要把值取出来再判断一下，不是NULL才累加。</p>\n<p>count(*)：统计所有的列，相当于行数，统计结果中会包含字段值为null的列；</p>\n<p><strong>COUNT(<code>*</code>)是SQL92定义的标准统计行数的语法，效率高，MySQL对它进行了很多优化，MyISAM中会直接把表的总行数单独记录下来供COUNT(*)查询，而InnoDB则会在扫表的时候选择最小的索引来降低成本</strong>。</p>\n<p>所以，建议使用COUNT(*)查询表的行数！</p>\n<h2 id=\"49存储md5值应该用varchar还是用char\">49、存储MD5值应该用VARCHAR还是用CHAR？</h2>\n<p>首先说说CHAR和VARCHAR的区别：</p>\n<p>1、存储长度：</p>\n<p><strong>CHAR类型的长度是固定的</strong></p>\n<p>当我们当定义CHAR(10)，输入的值是&quot;abc&quot;，但是它占用的空间一样是10个字节，会包含7个空字节。当输入的字符长度超过指定的数时，CHAR会截取超出的字符。而且，当存储为CHAR的时候，MySQL会自动删除输入字符串末尾的空格。</p>\n<p><strong>VARCHAR的长度是可变的</strong></p>\n<p>比如VARCHAR(10)，然后输入abc三个字符，那么实际存储大小为3个字节。</p>\n<p>除此之外，<strong>VARCHAR还会保留1个或2个额外的字节来记录字符串的实际长度</strong>。如果定义的最大长度小于等于255个字节，那么，就会预留1个字节；如果定义的最大长度大于255个字节，那么就会预留2个字节。</p>\n<p>2、存储效率</p>\n<p><strong>CHAR类型每次修改后的数据长度不变，效率更高</strong>。</p>\n<p>VARCHAR每次修改的数据要更新数据长度，效率更低。</p>\n<p>3、存储空间</p>\n<p>CHAR存储空间是<strong>初始的预计长度字符串再加上一个记录字符串长度的字节</strong>，可能会存在多余的空间。</p>\n<p>VARCHAR存储空间的时候是<strong>实际字符串再加上一个记录字符串长度的字节</strong>，占用空间较小。</p>\n<p>根据以上的分析，由于<strong>MD5是一个定长的值，所以MD5值适合使用CHAR存储</strong>。对于固定长度的非常短的列，CHAR比VARCHAR效率也更高。</p>\n","createTime":"2024-05-07 22:20:34","categoryId":36,"categoryName":"MySQL","readNum":84,"tags":[{"id":63,"name":"mysql","articlesTotal":null}],"preArticle":{"articleId":48,"articleTitle":"JVM"},"nextArticle":{"articleId":45,"articleTitle":"Redis"},"totalWords":22922,"readTime":"约 76 分钟","updateTime":"2024-06-03 20:17:54"}} =================================== 
2024-08-04 21:28:17.567 [http-nio-8088-exec-10] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 21:28:22.800 [http-nio-8088-exec-7] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 21:28:22.800 [http-nio-8088-exec-7] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 21:28:22.821 [http-nio-8088-exec-9] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 21:28:22.821 [http-nio-8088-exec-9] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 21:28:22.830 [http-nio-8088-exec-7] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [前台获取博客详情], 入参: , 请求类: BlogSettingsController, 请求方法: findDetail =================================== 
2024-08-04 21:28:22.841 [http-nio-8088-exec-9] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [获取文章详情], 入参: {"articleId":42}, 请求类: ArticleController, 请求方法: findArticleDetail =================================== 
2024-08-04 21:28:22.857 [http-nio-8088-exec-3] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 21:28:22.857 [http-nio-8088-exec-3] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 21:28:22.858 [http-nio-8088-exec-3] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [获取知识库文章上下页], 入参: {"id":1,"articleId":42}, 请求类: WikiController, 请求方法: findArticlePreNext =================================== 
2024-08-04 21:28:22.864 [http-nio-8088-exec-7] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [前台获取博客详情], 耗时: 34ms, 出参: {"success":true,"message":null,"errorCode":null,"data":{"logo":"http://110.41.141.141:9000/weblog/weblog/9853f8be13cb4f7fae00e3f5233dd688.png","name":"WebLog","author":"Jacob","introduction":"求知若饥，虚心若愚","avatar":"http://110.41.141.141:9000/weblog/weblog/cf0958d87787449fb05aae4cc84015c6.jpg","githubHomepage":"https://github.com/jdw-art","csdnHomepage":"https://www.csdn.net/?spm=1010.2135.3001.4476","giteeHomepage":"","zhihuHomepage":"https://www.zhihu.com/people/54-10-50-93"}} =================================== 
2024-08-04 21:28:22.866 [http-nio-8088-exec-7] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 21:28:23.026 [http-nio-8088-exec-3] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [获取知识库文章上下页], 耗时: 169ms, 出参: {"success":true,"message":null,"errorCode":null,"data":{"preArticle":{"articleId":47,"articleTitle":"MySQL"},"nextArticle":{"articleId":45,"articleTitle":"Redis"}}} =================================== 
2024-08-04 21:28:23.027 [http-nio-8088-exec-3] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 21:28:23.329 [http-nio-8088-exec-9] INFO  c.j.weblog.event.subscriber.ReadArticleSubscriber - ==> threadName: http-nio-8088-exec-9
2024-08-04 21:28:23.329 [http-nio-8088-exec-9] INFO  c.j.weblog.event.subscriber.ReadArticleSubscriber - ==> 文章阅读事件消费成功，articleId: 42
2024-08-04 21:28:23.368 [http-nio-8088-exec-9] INFO  c.j.weblog.event.subscriber.ReadArticleSubscriber - ==> 文章阅读量 +1 操作成功，articleId: 42
2024-08-04 21:28:23.436 [http-nio-8088-exec-9] INFO  c.j.weblog.event.subscriber.ReadArticleSubscriber - ==> 当日文章 PV 访问量 +1 操作成功，date: 2024-08-04
2024-08-04 21:28:23.436 [http-nio-8088-exec-9] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [获取文章详情], 耗时: 595ms, 出参: {"success":true,"message":null,"errorCode":null,"data":{"title":"MongoDB","content":"<h2 id=\"1mongodb是什么\">1、mongodb是什么？</h2>\n<p>MongoDB 是由 C++语言编写的，是一个基于分布式文件存储的开源数据库系统。 再高负载的情况下，添加更多的节点，可以保证服务器性能。 MongoDB 旨在给 WEB 应用提供可扩展的高性能数据存储解决方案。</p>\n<p>MongoDB 将数据存储为一个文档，数据结构由键值(key=&gt;value)对组成。 MongoDB 文档类似于 JSON 对象。字段值可以包含其他文档，数组及文档数组。</p>\n<h2 id=\"2mongodb有哪些特点\">2、mongodb有哪些特点？</h2>\n<p>（1）MongoDB 是一个面向文档存储的数据库，操作起来比较简单和容易。</p>\n<p>（2）你可以在 MongoDB 记录中设置任何属性的索引 (如： FirstName=&quot;Sameer&quot;,Address=&quot;8 Gandhi Road&quot;)来实现更快的排序。</p>\n<p>（3）你可以通过本地或者网络创建数据镜像，这使得 MongoDB 有更强的扩展性。</p>\n<p>（4）如果负载的增加（需要更多的存储空间和更强的处理能力） ，它可以分布在计算机网络中的其他节点上这就是所谓的分片。</p>\n<p>（5）Mongo 支持丰富的查询表达式。查询指令使用 JSON 形式的标记，可轻易查询文档中内嵌的对象及数组。</p>\n<p>（6）MongoDb 使用 update()命令可以实现替换完成的文档（数据）或者一些指定的数据字段 。</p>\n<p>（7）Mongodb 中的 Map/reduce 主要是用来对数据进行批量处理和聚合操作。</p>\n<p>（8）Map 和 Reduce。 Map 函数调用 emit(key,value)遍历集合中所有的记录，将 key 与 value 传给 Reduce 函数进行处理。</p>\n<p>（9）Map 函数和 Reduce 函数是使用 Javascript 编写的，并可以通过 db.runCommand 或 mapreduce 命令来执行 MapReduce 操作。</p>\n<p>（10）GridFS 是 MongoDB 中的一个内置功能，可以用于存放大量小文件。</p>\n<p>（11） MongoDB 允许在服务端执行脚本， 可以用 Javascript 编写某个函数，直接在服务端执行，也可以把函数的定义存储在服务端，下次直接调用即可。</p>\n<h2 id=\"3什么是非关系型数据库\">3、什么是非关系型数据库</h2>\n<p>非关系型数据库是对不同于传统关系型数据库的统称。非关系型数据库的显著特点是不使用SQL作为查询语言，数据存储不需要特定的表格模式。由于简单的设计和非常好的性能所以被用于大数据和Web Apps等</p>\n<h2 id=\"4为什么用mongodb\">4、为什么用MongoDB？</h2>\n<ul>\n<li>架构简单</li>\n<li>没有复杂的连接</li>\n<li>深度查询能力,MongoDB支持动态查询。</li>\n<li>容易调试</li>\n<li>容易扩展</li>\n<li>不需要转化/映射应用对象到数据库对象</li>\n<li>使用内部内存作为存储工作区,以便更快的存取数据。</li>\n</ul>\n<h2 id=\"5在哪些场景使用mongodb\">5、在哪些场景使用MongoDB</h2>\n<ul>\n<li>大数据</li>\n<li>内容管理系统</li>\n<li>移动端Apps</li>\n<li>数据管理</li>\n</ul>\n<h2 id=\"6mysql与mongodb之间最基本的差别是什么\">6、MySQL与MongoDB之间最基本的差别是什么?</h2>\n<p>MySQL和MongoDB两者都是免费开源的数据库。MySQL和MongoDB有许多基本差别包括数据的表示(data representation)，查询，关系，事务，schema的设计和定义，标准化(normalization)，速度和性能。</p>\n<p>通过比较MySQL和MongoDB，实际上我们是在比较关系型和非关系型数据库，即数据存储结构不同。</p>\n<h2 id=\"7mongodb成为最好nosql数据库的原因是什么\">7、MongoDB成为最好NoSQL数据库的原因是什么?</h2>\n<p>以下特点使得MongoDB成为最好的NoSQL数据库：</p>\n<ul>\n<li>面向文件的</li>\n<li>高性能</li>\n<li>高可用性</li>\n<li>易扩展性</li>\n<li>丰富的查询语言</li>\n</ul>\n<h2 id=\"8journal回放在条目entry不完整时比如恰巧有一个中途故障了会遇到问题吗\">8、journal回放在条目(entry)不完整时(比如恰巧有一个中途故障了)会遇到问题吗?</h2>\n<p>每个journal (group)的写操作都是一致的，除非它是完整的否则在恢复过程中它不会回放。</p>\n<h2 id=\"9分析器在mongodb中的作用是什么\">9、分析器在MongoDB中的作用是什么?</h2>\n<p>MongoDB中包括了一个可以显示数据库中每个操作性能特点的数据库分析器。通过这个分析器你可以找到比预期慢的查询(或写操作);利用这一信息，比如，可以确定是否需要添加索引。</p>\n<h2 id=\"10名字空间namespace是什么\">10、名字空间(namespace)是什么?</h2>\n<p>MongoDB存储BSON对象在丛集(collection)中。数据库名字和丛集名字以句点连结起来叫做名字空间(namespace)。</p>\n<h2 id=\"11允许空值null吗\">11、允许空值null吗?</h2>\n<p>对于对象成员而言，是的。然而用户不能够添加空值(null)到数据库丛集(collection)因为空值不是对象。然而用户能够添加空对象{}。</p>\n<h2 id=\"12更新操作立刻fsync到磁盘\">12、更新操作立刻fsync到磁盘?</h2>\n<p>不会，磁盘写操作默认是延迟执行的。写操作可能在两三秒(默认在60秒内)后到达磁盘。例如，如果一秒内数据库收到一千个对一个对象递增的操作，仅刷新磁盘一次。(注意，尽管fsync选项在命令行和经过getLastError_old是有效的)</p>\n<h2 id=\"13如何执行事务加锁\">13、如何执行事务/加锁?</h2>\n<p>MongoDB没有使用传统的锁或者复杂的带回滚的事务，因为它设计的宗旨是轻量，快速以及可预计的高性能。可以把它类比成MySQLMylSAM的自动提交模式。通过精简对事务的支持，性能得到了提升，特别是在一个可能会穿过多个服务器的系统里。</p>\n<h2 id=\"14启用备份故障恢复需要多久\">14、启用备份故障恢复需要多久?</h2>\n<p>从备份数据库声明主数据库宕机到选出一个备份数据库作为新的主数据库将花费10到30秒时间。这期间在主数据库上的操作将会失败--包括</p>\n<p>写入和强一致性读取(strong consistent read)操作。然而，你还能在第二数据库上执行最终一致性查询(eventually consistent query)(在slaveOk模式下)，即使在这段时间里。</p>\n<h2 id=\"15什么是master或primary\">15、什么是master或primary?</h2>\n<p>它是当前备份集群(replica set)中负责处理所有写入操作的主要节点/成员。在一个备份集群中，当失效备援(failover)事件发生时，一个另外的成员会变成primary。</p>\n<h2 id=\"16什么是secondary或slave\">16、什么是secondary或slave?</h2>\n<p>Seconday从当前的primary上复制相应的操作。它是通过跟踪复制oplog(<a href=\"http://local.oplog.rs\" ref=\"nofollow\" target=\"_blank\">local.oplog.rsopen in new window</a><span><svg xmlns=\"http://www.w3.org/2000/svg\" class=\"inline ml-1\" style=\"color: #aaa;\" aria-hidden=\"true\" focusable=\"false\" x=\"0px\" y=\"0px\" viewBox=\"0 0 100 100\" width=\"15\" height=\"15\" class=\"icon outbound\"><path fill=\"currentColor\" d=\"M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z\"></path> <polygon fill=\"currentColor\" points=\"45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9\"></polygon></svg> <span class=\"sr-only\"></span></span>)做到的。</p>\n<h2 id=\"17应该启动一个集群分片sharded还是一个非集群分片的-mongodb-环境\">17、应该启动一个集群分片(sharded)还是一个非集群分片的 MongoDB 环境?</h2>\n<p>为开发便捷起见，我们建议以非集群分片(unsharded)方式开始一个 MongoDB 环境，除非一台服务器不足以存放你的初始数据集。从非集群分片升级到集群分片(sharding)是无缝的，所以在你的数据集还不是很大的时候没必要考虑集群分片(sharding)。</p>\n<h2 id=\"18分片sharding和复制replication是怎样工作的\">18、分片(sharding)和复制(replication)是怎样工作的?</h2>\n<p>每一个分片(shard)是一个分区数据的逻辑集合。分片可能由单一服务器或者集群组成，我们推荐为每一个分片(shard)使用集群。</p>\n<h2 id=\"19数据在什么时候才会扩展到多个分片shard里\">19、数据在什么时候才会扩展到多个分片(shard)里?</h2>\n<p>MongoDB 分片是基于区域(range)的。所以一个集合(collection)中的所有的对象都被存放到一个块(chunk)中。只有当存在多余一个块的时后，才会有多个分片获取数据的选项。现在，每个默认块的大小是 64Mb，所以你需要至少 64 Mb 空间才可以实施一个迁移。</p>\n<h2 id=\"20如果在一个分片shard停止或者很慢的时候发起一个查询会怎样\">20、如果在一个分片(shard)停止或者很慢的时候，发起一个查询会怎样?</h2>\n<p>如果一个分片(shard)停止了，除非查询设置了“Partial”选项，否则查询会返回一个错误。如果一个分片(shard)响应很慢，MongoDB则会等待它的响应。</p>\n<h2 id=\"21当更新一个正在被迁移的块chunk上的文档时会发生什么\">21、当更新一个正在被迁移的块（Chunk）上的文档时会发生什么？</h2>\n<p>更新操作会立即发生在旧的块（Chunk）上，然后更改才会在所有权转移前复制到新的分片上。</p>\n<h2 id=\"22mongodb在abc上建立索引查询abc和acb都会使用索引吗\">22、MongoDB在A:{B,C}上建立索引，查询A:{B,C}和A:{C,B}都会使用索引吗？</h2>\n<p>不会，只会在A:{B,C}上使用索引。</p>\n<h2 id=\"23如果一个分片shard停止或很慢的时候发起一个查询会怎样\">23、如果一个分片（Shard）停止或很慢的时候，发起一个查询会怎样？</h2>\n<p>如果一个分片停止了，除非查询设置了“Partial”选项，否则查询会返回一个错误。如果一个分片响应很慢，MongoDB会等待它的响应。</p>\n<h2 id=\"24mongodb支持存储过程吗如果支持的话怎么用\">24、MongoDB支持存储过程吗？如果支持的话，怎么用？</h2>\n<p>MongoDB支持存储过程，它是javascript写的，保存在db.system.js表中。</p>\n<h2 id=\"25如何理解mongodb中的gridfs机制mongodb为何使用gridfs来存储文件\">25、如何理解MongoDB中的GridFS机制，MongoDB为何使用GridFS来存储文件？</h2>\n<p>GridFS是一种将大型文件存储在MongoDB中的文件规范。使用GridFS可以将大文件分隔成多个小文档存放，这样我们能够有效的保存大文档，而且解决了BSON对象有限制的问题。</p>\n<h2 id=\"26mongodb的数据结构\">26、mongodb的数据结构</h2>\n<p>数据库中存储的对象设计bson，一种类似json的二进制文件，由键值对组成。</p>\n<h2 id=\"27mongodb的优势有哪些\">27、MongoDB的优势有哪些</h2>\n<ul>\n<li>面向文档的存储：以 JSON 格式的文档保存数据。</li>\n<li>任何属性都可以建立索引。</li>\n<li>复制以及高可扩展性。</li>\n<li>自动分片。</li>\n<li>丰富的查询功能。</li>\n<li>快速的即时更新。</li>\n<li>来自 MongoDB 的专业支持。</li>\n</ul>\n<h2 id=\"28什么是集合\">28、什么是集合</h2>\n<p>集合就是一组 MongoDB 文档。它相当于关系型数据库（RDBMS）中的表这种概念。集合位于单独的一个数据库中。一个集合内的多个文档可以有多个不同的字段。一般来说，集合中的文档都有着相同或相关的目的。</p>\n<h2 id=\"29什么是文档\">29、什么是文档</h2>\n<p>文档由一组key value组成。文档是动态模式,这意味着同一集合里的文档不需要有相同的字段和结构。在关系型数据库中table中的每一条记录相当于MongoDB中的一个文档。</p>\n<h2 id=\"30什么是mongod\">30、什么是”mongod“</h2>\n<p>mongod是处理MongoDB系统的主要进程。它处理数据请求，管理数据存储，和执行后台管理操作。当我们运行mongod命令意味着正在启动MongoDB进程,并且在后台运行。</p>\n<h2 id=\"31mongod参数有什么\">31、&quot;mongod&quot;参数有什么</h2>\n<ul>\n<li>传递数据库存储路径，默认是&quot;/data/db&quot;</li>\n<li>端口号 默认是 &quot;27017&quot;</li>\n</ul>\n<h2 id=\"32什么是mongo\">32、什么是&quot;mongo&quot;</h2>\n<p>它是一个命令行工具用于连接一个特定的mongod实例。当我们没有带参数运行mongo命令它将使用默认的端口号和localhost连接</p>\n<h2 id=\"33mongodb哪个命令可以切换数据库\">33、MongoDB哪个命令可以切换数据库</h2>\n<p>MongoDB 用 use +数据库名称的方式来创建数据库。 use 会创建一个新的数据库，如果该数据库存在，则返回这个数据库。</p>\n<h2 id=\"34mongodb中的命名空间是什么意思\">34、MongoDB中的命名空间是什么意思?</h2>\n<p>MongoDB内部有预分配空间的机制，每个预分配的文件都用0进行填充。</p>\n<p>数据文件每新分配一次，它的大小都是上一个数据文件大小的2倍，每个数据文件最大2G。</p>\n<p>MongoDB每个集合和每个索引都对应一个命名空间，这些命名空间的元数据集中在16M的*.ns文件中，平均每个命名占用约 628 字节，也即整个数据库的命名空间的上限约为24000。</p>\n<p>如果每个集合有一个索引（比如默认的_id索引），那么最多可以创建12000个集合。如果索引数更多，则可创建的集合数就更少了。同时，如果集合数太多，一些操作也会变慢。</p>\n<p>要建立更多的集合的话，MongoDB 也是支持的，只需要在启动时加上“--nssize”参数，这样对应数据库的命名空间文件就可以变得更大以便保存更多的命名。这个命名空间文件（.ns文件）最大可以为 2G。</p>\n<p>每个命名空间对应的盘区不一定是连续的。与数据文件增长相同，每个命名空间对应的盘区大小都是随分配次数不断增长的。目的是为了平衡命名空间浪费的空间与保持一个命名空间数据的连续性。</p>\n<p>需要注意的一个命名空间freelist，这个命名空间用于记录不再使用的盘区（被删除的Collection或索引）。每当命名空间需要分配新盘区时，会先查看freelist，这个命名空间用于记录不再使用的盘区（被删除的Collection或索引）。每当命名空间需要分配新盘区时，会先查看freelist，这个命名空间用于记录不再使用的盘区（被删除的Collection或索引）。每当命名空间需要分配新盘区时，会先查看freelist是否有大小合适的盘区可以使用，如果有就回收空闲的磁盘空间。</p>\n<h2 id=\"35在mongodb中如何创建一个新的数据库\">35、在MongoDB中如何创建一个新的数据库</h2>\n<p>MongoDB 用 use + 数据库名称 的方式来创建数据库。 use 会创建一个新的数据库，如果该数据库存在，则返回这个数据库。</p>\n<h2 id=\"36mongodb中的分片是什么意思\">36、MongoDB中的分片是什么意思</h2>\n<p>分片是将数据水平切分到不同的物理节点。当应用数据越来越大的时候，数据量也会越来越大。当数据量增长时，单台机器有可能无法存储数据或可接受的读取写入吞吐量。利用分片技术可以添加更多的机器来应对数据量增加以及读写操作的要求。</p>\n<h2 id=\"37什么是复制\">37、什么是复制</h2>\n<p>复制是将数据同步到多个服务器的过程，通过多个数据副本存储到多个服务器上增加数据可用性。复制可以保障数据的安全性，灾难恢复，无需停机维护（如备份，重建索引，压缩），分布式读取数据。</p>\n<h2 id=\"38在mongodb中如何在集合中插入一个文档\">38、在MongoDB中如何在集合中插入一个文档</h2>\n<p>要想将数据插入 MongoDB 集合中，需要使用 insert() 或 save() 方法。</p>\n<pre><code class=\"language-stylus\">&gt;db.collectionName.insert({&quot;key&quot;:&quot;value&quot;})\n&gt;db.collectionName.save({&quot;key&quot;:&quot;value&quot;})\n</code></pre>\n<h2 id=\"39为什么要在mongodb中使用分析器\">39、为什么要在MongoDB中使用分析器</h2>\n<p>数据库分析工具(Database Profiler)会针对正在运行的mongod实例收集数据库命令执行的相关信息。包括增删改查的命令以及配置和管理命令。分析器(profiler)会写入所有收集的数据到 system.profile集合，一个capped集合在管理员数据库。分析器默认是关闭的你能通过per数据库或per实例开启。</p>\n<h2 id=\"40mongodb支持主键外键关系吗\">40、MongoDB支持主键外键关系吗</h2>\n<p>默认MongoDB不支持主键和外键关系。 用Mongodb本身的API需要硬编码才能实现外键关联，不够直观且难度较大。</p>\n<h2 id=\"41mongodb支持哪些数据类型\">41、MongoDB支持哪些数据类型</h2>\n<p>String、Integer、Double、Boolean、Object、Object ID、Arrays、Min/Max Keys、Datetime、Code、Regular Expression等</p>\n<h2 id=\"42objectid由哪些部分组成\">42、&quot;ObjectID&quot;由哪些部分组成</h2>\n<p>一共有四部分组成:时间戳、客户端ID、客户进程ID、三个字节的增量计数器</p>\n<p>_id是一个 12 字节长的十六进制数，它保证了每一个文档的唯一性。在插入文档时，需要提供 _id 。如果你不提供，那么 MongoDB 就会为每一文档提供一个唯一的 id。 _id 的头 4 个字节代表的是当前的时间戳，接着的后 3 个字节表示的是机器 id 号，接着的 2 个字节表示MongoDB 服务器进程 id，最后的 3 个字节代表递增值。</p>\n<h2 id=\"43mongodb索引\">43、MongoDb索引</h2>\n<p>索引用于高效的执行查询。没有索引MongoDB将扫描查询整个集合中的所有文档这种扫描效率很低，需要处理大量数据。索引是一种特殊的数据结构，将一小块数据集保存为容易遍历的形式。索引能够存储某种特殊字段或字段集的值，并按照索引指定的方式将字段值进行排序。</p>\n<h2 id=\"44如何添加索引\">44、如何添加索引</h2>\n<p>使用 db.collection.createIndex() 在集合中创建一个索引</p>\n<pre><code class=\"language-reasonml\">&gt;db.collectionName.createIndex({columnName:1})\n</code></pre>\n<h2 id=\"45在mongodb中如何更新数据\">45、在MongoDB中如何更新数据</h2>\n<p>update() 与 save() 方法都能用于更新集合中的文档。 update() 方法更新已有文档中的值，而 save() 方法则是用传入该方法的文档来替换已有文档。</p>\n<h2 id=\"46如何删除文档\">46、如何删除文档</h2>\n<p>MongoDB 利用 remove() 方法 清除集合中的文档。它有 2 个可选参数：</p>\n<ul>\n<li>deletion criteria：（可选）删除文档的标准。</li>\n<li>justOne：（可选）如果设为 true 或 1，则只删除一个文档。</li>\n</ul>\n<pre><code class=\"language-maxima\">&gt;db.collectionName.remove({key:value})\n</code></pre>\n<h2 id=\"47在mongodb中如何排序\">47、在MongoDB中如何排序</h2>\n<p>MongoDB 中的文档排序是通过 sort() 方法来实现的。 sort() 方法可以通过一些参数来指定要进行排序的字段，并使用 1 和 -1 来指定排</p>\n<p>序方式，其中 1 表示升序，而 -1 表示降序。</p>\n<pre><code class=\"language-stylus\">&gt;db.connectionName.find({key:value}).sort({columnName:1})\n</code></pre>\n<h2 id=\"48什么是聚合\">48、什么是聚合</h2>\n<p>聚合操作能够处理数据记录并返回计算结果。聚合操作能将多个文档中的值组合起来，对成组数据执行各种操作，返回单一的结果。它相当于 SQL 中的 count(*) 组合 group by。对于 MongoDB 中的聚合操作，应该使用 aggregate() 方法。</p>\n<pre><code class=\"language-stylus\">&gt;db.COLLECTION_NAME.aggregate(AGGREGATE_OPERATION)\n</code></pre>\n<h2 id=\"49在mongodb中什么是副本集\">49、在MongoDB中什么是副本集</h2>\n<p>在MongoDB中副本集由一组MongoDB实例组成，包括一个主节点多个次节点，MongoDB客户端的所有数据都写入主节点(Primary),副节点从主节点同步写入数据，以保持所有复制集内存储相同的数据，提高数据可用性。</p>\n","createTime":"2024-04-27 15:28:09","categoryId":37,"categoryName":"MongoDB","readNum":9,"tags":[{"id":47,"name":"java","articlesTotal":null},{"id":60,"name":"mongodb","articlesTotal":null},{"id":61,"name":"nosql","articlesTotal":null}],"preArticle":{"articleId":43,"articleTitle":"E-Commerce"},"nextArticle":{"articleId":41,"articleTitle":"SpringBoot"},"totalWords":4378,"readTime":"约 14 分钟","updateTime":"2024-04-27 15:28:09"}} =================================== 
2024-08-04 21:28:23.454 [http-nio-8088-exec-9] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 21:28:28.720 [http-nio-8088-exec-6] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 21:28:28.720 [http-nio-8088-exec-6] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 21:28:28.720 [http-nio-8088-exec-6] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [前台获取博客详情], 入参: , 请求类: BlogSettingsController, 请求方法: findDetail =================================== 
2024-08-04 21:28:28.787 [http-nio-8088-exec-5] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 21:28:28.787 [http-nio-8088-exec-5] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 21:28:28.792 [http-nio-8088-exec-6] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [前台获取博客详情], 耗时: 72ms, 出参: {"success":true,"message":null,"errorCode":null,"data":{"logo":"http://110.41.141.141:9000/weblog/weblog/9853f8be13cb4f7fae00e3f5233dd688.png","name":"WebLog","author":"Jacob","introduction":"求知若饥，虚心若愚","avatar":"http://110.41.141.141:9000/weblog/weblog/cf0958d87787449fb05aae4cc84015c6.jpg","githubHomepage":"https://github.com/jdw-art","csdnHomepage":"https://www.csdn.net/?spm=1010.2135.3001.4476","giteeHomepage":"","zhihuHomepage":"https://www.zhihu.com/people/54-10-50-93"}} =================================== 
2024-08-04 21:28:28.794 [http-nio-8088-exec-6] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 21:28:28.795 [http-nio-8088-exec-6] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 21:28:28.795 [http-nio-8088-exec-6] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 21:28:28.796 [http-nio-8088-exec-6] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [获取文章详情], 入参: {"articleId":42}, 请求类: ArticleController, 请求方法: findArticleDetail =================================== 
2024-08-04 21:28:28.826 [http-nio-8088-exec-4] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 21:28:28.826 [http-nio-8088-exec-4] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 21:28:28.827 [http-nio-8088-exec-4] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [获取知识库文章上下页], 入参: {"id":1,"articleId":42}, 请求类: WikiController, 请求方法: findArticlePreNext =================================== 
2024-08-04 21:28:28.890 [http-nio-8088-exec-4] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [获取知识库文章上下页], 耗时: 63ms, 出参: {"success":true,"message":null,"errorCode":null,"data":{"preArticle":{"articleId":47,"articleTitle":"MySQL"},"nextArticle":{"articleId":45,"articleTitle":"Redis"}}} =================================== 
2024-08-04 21:28:28.891 [http-nio-8088-exec-4] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 21:28:28.981 [http-nio-8088-exec-6] INFO  c.j.weblog.event.subscriber.ReadArticleSubscriber - ==> threadName: http-nio-8088-exec-6
2024-08-04 21:28:28.981 [http-nio-8088-exec-6] INFO  c.j.weblog.event.subscriber.ReadArticleSubscriber - ==> 文章阅读事件消费成功，articleId: 42
2024-08-04 21:28:29.099 [http-nio-8088-exec-6] INFO  c.j.weblog.event.subscriber.ReadArticleSubscriber - ==> 文章阅读量 +1 操作成功，articleId: 42
2024-08-04 21:28:29.144 [http-nio-8088-exec-6] INFO  c.j.weblog.event.subscriber.ReadArticleSubscriber - ==> 当日文章 PV 访问量 +1 操作成功，date: 2024-08-04
2024-08-04 21:28:29.145 [http-nio-8088-exec-6] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [获取文章详情], 耗时: 349ms, 出参: {"success":true,"message":null,"errorCode":null,"data":{"title":"MongoDB","content":"<h2 id=\"1mongodb是什么\">1、mongodb是什么？</h2>\n<p>MongoDB 是由 C++语言编写的，是一个基于分布式文件存储的开源数据库系统。 再高负载的情况下，添加更多的节点，可以保证服务器性能。 MongoDB 旨在给 WEB 应用提供可扩展的高性能数据存储解决方案。</p>\n<p>MongoDB 将数据存储为一个文档，数据结构由键值(key=&gt;value)对组成。 MongoDB 文档类似于 JSON 对象。字段值可以包含其他文档，数组及文档数组。</p>\n<h2 id=\"2mongodb有哪些特点\">2、mongodb有哪些特点？</h2>\n<p>（1）MongoDB 是一个面向文档存储的数据库，操作起来比较简单和容易。</p>\n<p>（2）你可以在 MongoDB 记录中设置任何属性的索引 (如： FirstName=&quot;Sameer&quot;,Address=&quot;8 Gandhi Road&quot;)来实现更快的排序。</p>\n<p>（3）你可以通过本地或者网络创建数据镜像，这使得 MongoDB 有更强的扩展性。</p>\n<p>（4）如果负载的增加（需要更多的存储空间和更强的处理能力） ，它可以分布在计算机网络中的其他节点上这就是所谓的分片。</p>\n<p>（5）Mongo 支持丰富的查询表达式。查询指令使用 JSON 形式的标记，可轻易查询文档中内嵌的对象及数组。</p>\n<p>（6）MongoDb 使用 update()命令可以实现替换完成的文档（数据）或者一些指定的数据字段 。</p>\n<p>（7）Mongodb 中的 Map/reduce 主要是用来对数据进行批量处理和聚合操作。</p>\n<p>（8）Map 和 Reduce。 Map 函数调用 emit(key,value)遍历集合中所有的记录，将 key 与 value 传给 Reduce 函数进行处理。</p>\n<p>（9）Map 函数和 Reduce 函数是使用 Javascript 编写的，并可以通过 db.runCommand 或 mapreduce 命令来执行 MapReduce 操作。</p>\n<p>（10）GridFS 是 MongoDB 中的一个内置功能，可以用于存放大量小文件。</p>\n<p>（11） MongoDB 允许在服务端执行脚本， 可以用 Javascript 编写某个函数，直接在服务端执行，也可以把函数的定义存储在服务端，下次直接调用即可。</p>\n<h2 id=\"3什么是非关系型数据库\">3、什么是非关系型数据库</h2>\n<p>非关系型数据库是对不同于传统关系型数据库的统称。非关系型数据库的显著特点是不使用SQL作为查询语言，数据存储不需要特定的表格模式。由于简单的设计和非常好的性能所以被用于大数据和Web Apps等</p>\n<h2 id=\"4为什么用mongodb\">4、为什么用MongoDB？</h2>\n<ul>\n<li>架构简单</li>\n<li>没有复杂的连接</li>\n<li>深度查询能力,MongoDB支持动态查询。</li>\n<li>容易调试</li>\n<li>容易扩展</li>\n<li>不需要转化/映射应用对象到数据库对象</li>\n<li>使用内部内存作为存储工作区,以便更快的存取数据。</li>\n</ul>\n<h2 id=\"5在哪些场景使用mongodb\">5、在哪些场景使用MongoDB</h2>\n<ul>\n<li>大数据</li>\n<li>内容管理系统</li>\n<li>移动端Apps</li>\n<li>数据管理</li>\n</ul>\n<h2 id=\"6mysql与mongodb之间最基本的差别是什么\">6、MySQL与MongoDB之间最基本的差别是什么?</h2>\n<p>MySQL和MongoDB两者都是免费开源的数据库。MySQL和MongoDB有许多基本差别包括数据的表示(data representation)，查询，关系，事务，schema的设计和定义，标准化(normalization)，速度和性能。</p>\n<p>通过比较MySQL和MongoDB，实际上我们是在比较关系型和非关系型数据库，即数据存储结构不同。</p>\n<h2 id=\"7mongodb成为最好nosql数据库的原因是什么\">7、MongoDB成为最好NoSQL数据库的原因是什么?</h2>\n<p>以下特点使得MongoDB成为最好的NoSQL数据库：</p>\n<ul>\n<li>面向文件的</li>\n<li>高性能</li>\n<li>高可用性</li>\n<li>易扩展性</li>\n<li>丰富的查询语言</li>\n</ul>\n<h2 id=\"8journal回放在条目entry不完整时比如恰巧有一个中途故障了会遇到问题吗\">8、journal回放在条目(entry)不完整时(比如恰巧有一个中途故障了)会遇到问题吗?</h2>\n<p>每个journal (group)的写操作都是一致的，除非它是完整的否则在恢复过程中它不会回放。</p>\n<h2 id=\"9分析器在mongodb中的作用是什么\">9、分析器在MongoDB中的作用是什么?</h2>\n<p>MongoDB中包括了一个可以显示数据库中每个操作性能特点的数据库分析器。通过这个分析器你可以找到比预期慢的查询(或写操作);利用这一信息，比如，可以确定是否需要添加索引。</p>\n<h2 id=\"10名字空间namespace是什么\">10、名字空间(namespace)是什么?</h2>\n<p>MongoDB存储BSON对象在丛集(collection)中。数据库名字和丛集名字以句点连结起来叫做名字空间(namespace)。</p>\n<h2 id=\"11允许空值null吗\">11、允许空值null吗?</h2>\n<p>对于对象成员而言，是的。然而用户不能够添加空值(null)到数据库丛集(collection)因为空值不是对象。然而用户能够添加空对象{}。</p>\n<h2 id=\"12更新操作立刻fsync到磁盘\">12、更新操作立刻fsync到磁盘?</h2>\n<p>不会，磁盘写操作默认是延迟执行的。写操作可能在两三秒(默认在60秒内)后到达磁盘。例如，如果一秒内数据库收到一千个对一个对象递增的操作，仅刷新磁盘一次。(注意，尽管fsync选项在命令行和经过getLastError_old是有效的)</p>\n<h2 id=\"13如何执行事务加锁\">13、如何执行事务/加锁?</h2>\n<p>MongoDB没有使用传统的锁或者复杂的带回滚的事务，因为它设计的宗旨是轻量，快速以及可预计的高性能。可以把它类比成MySQLMylSAM的自动提交模式。通过精简对事务的支持，性能得到了提升，特别是在一个可能会穿过多个服务器的系统里。</p>\n<h2 id=\"14启用备份故障恢复需要多久\">14、启用备份故障恢复需要多久?</h2>\n<p>从备份数据库声明主数据库宕机到选出一个备份数据库作为新的主数据库将花费10到30秒时间。这期间在主数据库上的操作将会失败--包括</p>\n<p>写入和强一致性读取(strong consistent read)操作。然而，你还能在第二数据库上执行最终一致性查询(eventually consistent query)(在slaveOk模式下)，即使在这段时间里。</p>\n<h2 id=\"15什么是master或primary\">15、什么是master或primary?</h2>\n<p>它是当前备份集群(replica set)中负责处理所有写入操作的主要节点/成员。在一个备份集群中，当失效备援(failover)事件发生时，一个另外的成员会变成primary。</p>\n<h2 id=\"16什么是secondary或slave\">16、什么是secondary或slave?</h2>\n<p>Seconday从当前的primary上复制相应的操作。它是通过跟踪复制oplog(<a href=\"http://local.oplog.rs\" ref=\"nofollow\" target=\"_blank\">local.oplog.rsopen in new window</a><span><svg xmlns=\"http://www.w3.org/2000/svg\" class=\"inline ml-1\" style=\"color: #aaa;\" aria-hidden=\"true\" focusable=\"false\" x=\"0px\" y=\"0px\" viewBox=\"0 0 100 100\" width=\"15\" height=\"15\" class=\"icon outbound\"><path fill=\"currentColor\" d=\"M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z\"></path> <polygon fill=\"currentColor\" points=\"45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9\"></polygon></svg> <span class=\"sr-only\"></span></span>)做到的。</p>\n<h2 id=\"17应该启动一个集群分片sharded还是一个非集群分片的-mongodb-环境\">17、应该启动一个集群分片(sharded)还是一个非集群分片的 MongoDB 环境?</h2>\n<p>为开发便捷起见，我们建议以非集群分片(unsharded)方式开始一个 MongoDB 环境，除非一台服务器不足以存放你的初始数据集。从非集群分片升级到集群分片(sharding)是无缝的，所以在你的数据集还不是很大的时候没必要考虑集群分片(sharding)。</p>\n<h2 id=\"18分片sharding和复制replication是怎样工作的\">18、分片(sharding)和复制(replication)是怎样工作的?</h2>\n<p>每一个分片(shard)是一个分区数据的逻辑集合。分片可能由单一服务器或者集群组成，我们推荐为每一个分片(shard)使用集群。</p>\n<h2 id=\"19数据在什么时候才会扩展到多个分片shard里\">19、数据在什么时候才会扩展到多个分片(shard)里?</h2>\n<p>MongoDB 分片是基于区域(range)的。所以一个集合(collection)中的所有的对象都被存放到一个块(chunk)中。只有当存在多余一个块的时后，才会有多个分片获取数据的选项。现在，每个默认块的大小是 64Mb，所以你需要至少 64 Mb 空间才可以实施一个迁移。</p>\n<h2 id=\"20如果在一个分片shard停止或者很慢的时候发起一个查询会怎样\">20、如果在一个分片(shard)停止或者很慢的时候，发起一个查询会怎样?</h2>\n<p>如果一个分片(shard)停止了，除非查询设置了“Partial”选项，否则查询会返回一个错误。如果一个分片(shard)响应很慢，MongoDB则会等待它的响应。</p>\n<h2 id=\"21当更新一个正在被迁移的块chunk上的文档时会发生什么\">21、当更新一个正在被迁移的块（Chunk）上的文档时会发生什么？</h2>\n<p>更新操作会立即发生在旧的块（Chunk）上，然后更改才会在所有权转移前复制到新的分片上。</p>\n<h2 id=\"22mongodb在abc上建立索引查询abc和acb都会使用索引吗\">22、MongoDB在A:{B,C}上建立索引，查询A:{B,C}和A:{C,B}都会使用索引吗？</h2>\n<p>不会，只会在A:{B,C}上使用索引。</p>\n<h2 id=\"23如果一个分片shard停止或很慢的时候发起一个查询会怎样\">23、如果一个分片（Shard）停止或很慢的时候，发起一个查询会怎样？</h2>\n<p>如果一个分片停止了，除非查询设置了“Partial”选项，否则查询会返回一个错误。如果一个分片响应很慢，MongoDB会等待它的响应。</p>\n<h2 id=\"24mongodb支持存储过程吗如果支持的话怎么用\">24、MongoDB支持存储过程吗？如果支持的话，怎么用？</h2>\n<p>MongoDB支持存储过程，它是javascript写的，保存在db.system.js表中。</p>\n<h2 id=\"25如何理解mongodb中的gridfs机制mongodb为何使用gridfs来存储文件\">25、如何理解MongoDB中的GridFS机制，MongoDB为何使用GridFS来存储文件？</h2>\n<p>GridFS是一种将大型文件存储在MongoDB中的文件规范。使用GridFS可以将大文件分隔成多个小文档存放，这样我们能够有效的保存大文档，而且解决了BSON对象有限制的问题。</p>\n<h2 id=\"26mongodb的数据结构\">26、mongodb的数据结构</h2>\n<p>数据库中存储的对象设计bson，一种类似json的二进制文件，由键值对组成。</p>\n<h2 id=\"27mongodb的优势有哪些\">27、MongoDB的优势有哪些</h2>\n<ul>\n<li>面向文档的存储：以 JSON 格式的文档保存数据。</li>\n<li>任何属性都可以建立索引。</li>\n<li>复制以及高可扩展性。</li>\n<li>自动分片。</li>\n<li>丰富的查询功能。</li>\n<li>快速的即时更新。</li>\n<li>来自 MongoDB 的专业支持。</li>\n</ul>\n<h2 id=\"28什么是集合\">28、什么是集合</h2>\n<p>集合就是一组 MongoDB 文档。它相当于关系型数据库（RDBMS）中的表这种概念。集合位于单独的一个数据库中。一个集合内的多个文档可以有多个不同的字段。一般来说，集合中的文档都有着相同或相关的目的。</p>\n<h2 id=\"29什么是文档\">29、什么是文档</h2>\n<p>文档由一组key value组成。文档是动态模式,这意味着同一集合里的文档不需要有相同的字段和结构。在关系型数据库中table中的每一条记录相当于MongoDB中的一个文档。</p>\n<h2 id=\"30什么是mongod\">30、什么是”mongod“</h2>\n<p>mongod是处理MongoDB系统的主要进程。它处理数据请求，管理数据存储，和执行后台管理操作。当我们运行mongod命令意味着正在启动MongoDB进程,并且在后台运行。</p>\n<h2 id=\"31mongod参数有什么\">31、&quot;mongod&quot;参数有什么</h2>\n<ul>\n<li>传递数据库存储路径，默认是&quot;/data/db&quot;</li>\n<li>端口号 默认是 &quot;27017&quot;</li>\n</ul>\n<h2 id=\"32什么是mongo\">32、什么是&quot;mongo&quot;</h2>\n<p>它是一个命令行工具用于连接一个特定的mongod实例。当我们没有带参数运行mongo命令它将使用默认的端口号和localhost连接</p>\n<h2 id=\"33mongodb哪个命令可以切换数据库\">33、MongoDB哪个命令可以切换数据库</h2>\n<p>MongoDB 用 use +数据库名称的方式来创建数据库。 use 会创建一个新的数据库，如果该数据库存在，则返回这个数据库。</p>\n<h2 id=\"34mongodb中的命名空间是什么意思\">34、MongoDB中的命名空间是什么意思?</h2>\n<p>MongoDB内部有预分配空间的机制，每个预分配的文件都用0进行填充。</p>\n<p>数据文件每新分配一次，它的大小都是上一个数据文件大小的2倍，每个数据文件最大2G。</p>\n<p>MongoDB每个集合和每个索引都对应一个命名空间，这些命名空间的元数据集中在16M的*.ns文件中，平均每个命名占用约 628 字节，也即整个数据库的命名空间的上限约为24000。</p>\n<p>如果每个集合有一个索引（比如默认的_id索引），那么最多可以创建12000个集合。如果索引数更多，则可创建的集合数就更少了。同时，如果集合数太多，一些操作也会变慢。</p>\n<p>要建立更多的集合的话，MongoDB 也是支持的，只需要在启动时加上“--nssize”参数，这样对应数据库的命名空间文件就可以变得更大以便保存更多的命名。这个命名空间文件（.ns文件）最大可以为 2G。</p>\n<p>每个命名空间对应的盘区不一定是连续的。与数据文件增长相同，每个命名空间对应的盘区大小都是随分配次数不断增长的。目的是为了平衡命名空间浪费的空间与保持一个命名空间数据的连续性。</p>\n<p>需要注意的一个命名空间freelist，这个命名空间用于记录不再使用的盘区（被删除的Collection或索引）。每当命名空间需要分配新盘区时，会先查看freelist，这个命名空间用于记录不再使用的盘区（被删除的Collection或索引）。每当命名空间需要分配新盘区时，会先查看freelist，这个命名空间用于记录不再使用的盘区（被删除的Collection或索引）。每当命名空间需要分配新盘区时，会先查看freelist是否有大小合适的盘区可以使用，如果有就回收空闲的磁盘空间。</p>\n<h2 id=\"35在mongodb中如何创建一个新的数据库\">35、在MongoDB中如何创建一个新的数据库</h2>\n<p>MongoDB 用 use + 数据库名称 的方式来创建数据库。 use 会创建一个新的数据库，如果该数据库存在，则返回这个数据库。</p>\n<h2 id=\"36mongodb中的分片是什么意思\">36、MongoDB中的分片是什么意思</h2>\n<p>分片是将数据水平切分到不同的物理节点。当应用数据越来越大的时候，数据量也会越来越大。当数据量增长时，单台机器有可能无法存储数据或可接受的读取写入吞吐量。利用分片技术可以添加更多的机器来应对数据量增加以及读写操作的要求。</p>\n<h2 id=\"37什么是复制\">37、什么是复制</h2>\n<p>复制是将数据同步到多个服务器的过程，通过多个数据副本存储到多个服务器上增加数据可用性。复制可以保障数据的安全性，灾难恢复，无需停机维护（如备份，重建索引，压缩），分布式读取数据。</p>\n<h2 id=\"38在mongodb中如何在集合中插入一个文档\">38、在MongoDB中如何在集合中插入一个文档</h2>\n<p>要想将数据插入 MongoDB 集合中，需要使用 insert() 或 save() 方法。</p>\n<pre><code class=\"language-stylus\">&gt;db.collectionName.insert({&quot;key&quot;:&quot;value&quot;})\n&gt;db.collectionName.save({&quot;key&quot;:&quot;value&quot;})\n</code></pre>\n<h2 id=\"39为什么要在mongodb中使用分析器\">39、为什么要在MongoDB中使用分析器</h2>\n<p>数据库分析工具(Database Profiler)会针对正在运行的mongod实例收集数据库命令执行的相关信息。包括增删改查的命令以及配置和管理命令。分析器(profiler)会写入所有收集的数据到 system.profile集合，一个capped集合在管理员数据库。分析器默认是关闭的你能通过per数据库或per实例开启。</p>\n<h2 id=\"40mongodb支持主键外键关系吗\">40、MongoDB支持主键外键关系吗</h2>\n<p>默认MongoDB不支持主键和外键关系。 用Mongodb本身的API需要硬编码才能实现外键关联，不够直观且难度较大。</p>\n<h2 id=\"41mongodb支持哪些数据类型\">41、MongoDB支持哪些数据类型</h2>\n<p>String、Integer、Double、Boolean、Object、Object ID、Arrays、Min/Max Keys、Datetime、Code、Regular Expression等</p>\n<h2 id=\"42objectid由哪些部分组成\">42、&quot;ObjectID&quot;由哪些部分组成</h2>\n<p>一共有四部分组成:时间戳、客户端ID、客户进程ID、三个字节的增量计数器</p>\n<p>_id是一个 12 字节长的十六进制数，它保证了每一个文档的唯一性。在插入文档时，需要提供 _id 。如果你不提供，那么 MongoDB 就会为每一文档提供一个唯一的 id。 _id 的头 4 个字节代表的是当前的时间戳，接着的后 3 个字节表示的是机器 id 号，接着的 2 个字节表示MongoDB 服务器进程 id，最后的 3 个字节代表递增值。</p>\n<h2 id=\"43mongodb索引\">43、MongoDb索引</h2>\n<p>索引用于高效的执行查询。没有索引MongoDB将扫描查询整个集合中的所有文档这种扫描效率很低，需要处理大量数据。索引是一种特殊的数据结构，将一小块数据集保存为容易遍历的形式。索引能够存储某种特殊字段或字段集的值，并按照索引指定的方式将字段值进行排序。</p>\n<h2 id=\"44如何添加索引\">44、如何添加索引</h2>\n<p>使用 db.collection.createIndex() 在集合中创建一个索引</p>\n<pre><code class=\"language-reasonml\">&gt;db.collectionName.createIndex({columnName:1})\n</code></pre>\n<h2 id=\"45在mongodb中如何更新数据\">45、在MongoDB中如何更新数据</h2>\n<p>update() 与 save() 方法都能用于更新集合中的文档。 update() 方法更新已有文档中的值，而 save() 方法则是用传入该方法的文档来替换已有文档。</p>\n<h2 id=\"46如何删除文档\">46、如何删除文档</h2>\n<p>MongoDB 利用 remove() 方法 清除集合中的文档。它有 2 个可选参数：</p>\n<ul>\n<li>deletion criteria：（可选）删除文档的标准。</li>\n<li>justOne：（可选）如果设为 true 或 1，则只删除一个文档。</li>\n</ul>\n<pre><code class=\"language-maxima\">&gt;db.collectionName.remove({key:value})\n</code></pre>\n<h2 id=\"47在mongodb中如何排序\">47、在MongoDB中如何排序</h2>\n<p>MongoDB 中的文档排序是通过 sort() 方法来实现的。 sort() 方法可以通过一些参数来指定要进行排序的字段，并使用 1 和 -1 来指定排</p>\n<p>序方式，其中 1 表示升序，而 -1 表示降序。</p>\n<pre><code class=\"language-stylus\">&gt;db.connectionName.find({key:value}).sort({columnName:1})\n</code></pre>\n<h2 id=\"48什么是聚合\">48、什么是聚合</h2>\n<p>聚合操作能够处理数据记录并返回计算结果。聚合操作能将多个文档中的值组合起来，对成组数据执行各种操作，返回单一的结果。它相当于 SQL 中的 count(*) 组合 group by。对于 MongoDB 中的聚合操作，应该使用 aggregate() 方法。</p>\n<pre><code class=\"language-stylus\">&gt;db.COLLECTION_NAME.aggregate(AGGREGATE_OPERATION)\n</code></pre>\n<h2 id=\"49在mongodb中什么是副本集\">49、在MongoDB中什么是副本集</h2>\n<p>在MongoDB中副本集由一组MongoDB实例组成，包括一个主节点多个次节点，MongoDB客户端的所有数据都写入主节点(Primary),副节点从主节点同步写入数据，以保持所有复制集内存储相同的数据，提高数据可用性。</p>\n","createTime":"2024-04-27 15:28:09","categoryId":37,"categoryName":"MongoDB","readNum":10,"tags":[{"id":47,"name":"java","articlesTotal":null},{"id":60,"name":"mongodb","articlesTotal":null},{"id":61,"name":"nosql","articlesTotal":null}],"preArticle":{"articleId":43,"articleTitle":"E-Commerce"},"nextArticle":{"articleId":41,"articleTitle":"SpringBoot"},"totalWords":4378,"readTime":"约 14 分钟","updateTime":"2024-04-27 15:28:09"}} =================================== 
2024-08-04 21:28:29.187 [http-nio-8088-exec-6] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 21:28:29.888 [http-nio-8088-exec-5] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [获取知识库目录数据], 入参: {"id":1}, 请求类: WikiController, 请求方法: findWikiCatalogList =================================== 
2024-08-04 21:28:30.433 [http-nio-8088-exec-5] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [获取知识库目录数据], 耗时: 48ms, 出参: {"success":true,"message":null,"errorCode":null,"data":[{"id":89,"articleId":null,"title":"关系型数据库","level":1,"children":[{"id":90,"articleId":47,"title":"MySQL","level":2,"children":null}]},{"id":91,"articleId":null,"title":"非关系型数据库","level":1,"children":[{"id":92,"articleId":42,"title":"MongoDB","level":2,"children":null}]},{"id":93,"articleId":null,"title":"缓存","level":1,"children":[{"id":94,"articleId":45,"title":"Redis","level":2,"children":null},{"id":95,"articleId":51,"title":"RedisIncrement","level":2,"children":null}]}]} =================================== 
2024-08-04 21:28:30.434 [http-nio-8088-exec-5] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 21:28:31.958 [http-nio-8088-exec-1] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 21:28:31.958 [http-nio-8088-exec-1] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 21:28:31.958 [http-nio-8088-exec-8] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 21:28:31.958 [http-nio-8088-exec-8] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 21:28:31.959 [http-nio-8088-exec-8] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [前台获取博客详情], 入参: , 请求类: BlogSettingsController, 请求方法: findDetail =================================== 
2024-08-04 21:28:31.960 [http-nio-8088-exec-1] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [获取文章详情], 入参: {"articleId":47}, 请求类: ArticleController, 请求方法: findArticleDetail =================================== 
2024-08-04 21:28:31.980 [http-nio-8088-exec-10] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 21:28:31.980 [http-nio-8088-exec-10] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 21:28:31.981 [http-nio-8088-exec-10] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [获取知识库文章上下页], 入参: {"id":1,"articleId":47}, 请求类: WikiController, 请求方法: findArticlePreNext =================================== 
2024-08-04 21:28:32.416 [http-nio-8088-exec-10] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [获取知识库文章上下页], 耗时: 435ms, 出参: {"success":true,"message":null,"errorCode":null,"data":{"preArticle":null,"nextArticle":{"articleId":42,"articleTitle":"MongoDB"}}} =================================== 
2024-08-04 21:28:32.440 [http-nio-8088-exec-10] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 21:28:32.538 [http-nio-8088-exec-1] INFO  c.j.weblog.event.subscriber.ReadArticleSubscriber - ==> threadName: http-nio-8088-exec-1
2024-08-04 21:28:32.539 [http-nio-8088-exec-1] INFO  c.j.weblog.event.subscriber.ReadArticleSubscriber - ==> 文章阅读事件消费成功，articleId: 47
2024-08-04 21:28:32.545 [http-nio-8088-exec-1] INFO  c.j.weblog.event.subscriber.ReadArticleSubscriber - ==> 文章阅读量 +1 操作成功，articleId: 47
2024-08-04 21:28:32.607 [http-nio-8088-exec-1] INFO  c.j.weblog.event.subscriber.ReadArticleSubscriber - ==> 当日文章 PV 访问量 +1 操作成功，date: 2024-08-04
2024-08-04 21:28:32.608 [http-nio-8088-exec-1] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [获取文章详情], 耗时: 647ms, 出参: {"success":true,"message":null,"errorCode":null,"data":{"title":"MySQL","content":"<h2 id=\"1事务的四大特性\">1、事务的四大特性</h2>\n<p>事务特性ACID：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability）。</p>\n<ul>\n<li><strong>原子性</strong>：事务包含的所有操作要么全部成功，要么全部失败回滚。</li>\n<li><strong>一致性</strong>：一个事务在执行前和执行后都必须处于一致性状态。如a和b账户共1000块，两人之间转账无论成功还是失败之后，两个账户的总和都必须是1000块。</li>\n<li><strong>隔离性</strong>：和隔离级别相关，如<code>read committed</code>，<strong>一个事务只能读取到已经提交的修改</strong>。</li>\n<li><strong>持久性</strong>：一个事务一旦被提交了，那么对于数据库中的数据的改变将是永久性的，即使在数据库系统遇到故障的情况下也不会丢失提交事务的操作。</li>\n</ul>\n<h2 id=\"2数据库的三大范式\">2、数据库的三大范式</h2>\n<ol>\n<li>\n<p><strong>第一范式1NF</strong></p>\n<p><strong>确保数据库字段的原子性。</strong> 数据库中的字段不可再分</p>\n<p>比如字段 userInfo : 广东省 10086' ，依照第一范式必须拆分成 userInfo : 广东省 userTel : 10086\n两个字段。</p>\n</li>\n<li>\n<p><strong>第二范式2NF</strong></p>\n<p><strong>首先要满足第一范式，另外还要包含两个内容，一是表必须有主键，二是非主键必须完全依赖于主键，而不能依赖于主键的一部分。</strong></p>\n<p>举个例子。假定选课关系表为student_course (student_no, student_name, age, course_name,\ngrade, credit)，主键为(student_no, course_name)。其中学分完全依赖于课程名称，姓名年龄完全依\n赖学号，不符合第二范式，会导致数据冗余（学生选n门课，姓名年龄有n条记录）、插入异常（插入一\n门新课，因为没有学号，无法保存新课记录）等问题。</p>\n<p>应该拆分成三个表：学生： student (stuent_no, student_name, 年龄)；课程：\ncourse (course_name, credit)；选课关系： student_course_relation (student_no, course_name,\ngrade)。</p>\n</li>\n<li>\n<p><strong>第三范式3NF</strong></p>\n<p><strong>首先要满足第二范式，另外非主键列必须直接依赖于主键，不能存在传递依赖。即不能存在：非主键列A依赖于非主键列B，非主键列B依赖于主键的情况。</strong></p>\n<p>假定学生关系表为Student(student_no, student_name, age, academy_id, academy_telephone)，主\n键为&quot;学号&quot;，其中学院id依赖于学号，而学院地点和学院电话依赖于学院id，存在传递依赖，不符合第三\n范式。</p>\n<p>可以把学生关系表分为如下两个表：学生：(student_no, student_name, age, academy_id)；学院：\n(academy_id, academy_telephone)。</p>\n</li>\n</ol>\n<p><strong>2NF和3NF的区别：</strong></p>\n<ul>\n<li>2NF依据是非主键列是否完全依赖主键，还是依赖于主键的一部分。</li>\n<li>3NF依据是非主键列是直接依赖于主键，还是直接依赖于非主键。</li>\n</ul>\n<p><strong>第一范式：<strong>1NF是对属性的</strong>原子性约束</strong>，要求属性具有原子性，不可再分解；\n<strong>第二范式：<strong>2NF是对记录的</strong>惟一性约束</strong>，要求记录有惟一标识，即实体的惟一性；\n<strong>第三范式：<strong>3NF是对</strong>字段冗余性的约束</strong>，即任何字段不能由其他字段派生出来，它要求字段没有冗余。</p>\n<blockquote>\n<p>没有冗余的数据库设计可以做到。但是，没有冗余的数据库未必是最好的数据库，有时为了提高运行效率，就必须降低范式标准，适当保留冗余数据。具体做法是：在概念数据模型设计时遵守第三范式，降低范式标准的工作放到物理数据模型设计时考虑。降低范式就是增加字段，允许冗余。</p>\n</blockquote>\n<h2 id=\"3事务隔离的级别有哪些\">3、事务隔离的级别有哪些</h2>\n<p>先解释一下脏读、不可重复读和幻读的概念：</p>\n<ul>\n<li><strong>脏读：</strong> 在一个事务处理过程中读取了另一个未提交事务中的数据。</li>\n<li><strong>不可重复读：</strong> 对于数据库中的某行记录，一个事务范围内多次查询却返回了不同的数据值，这是由于在查询间隔，另一个事务修改了数据并提交了。</li>\n<li><strong>幻读：</strong> 当某个事务在读取某个范围内的记录时，另一个事务又在该范围内插入了新的记录。对幻读的正确理解是，一个事务内的读取操作的结论不足以支撑之后业务的执行。假设事务要新增一条记录，主键为id，在新增之前执行了select，发现没有id为xxx的记录，但插入时却出现了主键冲突，这就属于幻读，读取不到的记录却发生了主键冲突，是因为记录实际上已经被其他的事务插入了，但当前事务不可见。</li>\n</ul>\n<p>不可重复读和脏读的区别：<strong>脏读时某一事务读取了另一个事务未提交的脏数据，而不可重复读则是读取了前一个事务提交的数据</strong>。</p>\n<p>事务隔离就是为了解决上面提到的脏读、不可重复读、幻读这几个问题。</p>\n<p>MySQL数据库提供了四种隔离级别：</p>\n<ul>\n<li><strong>Serializable（串行化）</strong>：通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。</li>\n<li><strong>Repeatable read（可重复读）</strong>：<strong>MySQL的默认事务隔离级别，它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行，解决了不可重复读的问题</strong>。</li>\n<li><strong>Read committed（读已提交）</strong>：一个事务只能看见已经提交事务所作的改变，可避免脏读的发生。</li>\n<li><strong>Read uncommitted（读未提交）</strong>：所有事务都可以看见其他未提交事务的执行结果。</li>\n</ul>\n<p><strong>查看隔离级别：</strong></p>\n<pre><code class=\"language-sq\">select @@transaction_isolation;\n</code></pre>\n<p><strong>设置隔离级别：</strong></p>\n<pre><code class=\"language-sql\">set session transaction isolation level read uncommitted;\n</code></pre>\n<h2 id=\"4生产环境数据库一般用的隔离级别\">4、生产环境数据库一般用的隔离级别</h2>\n<p>生产环境一般使用读已提交（RC）隔离级别。为什么不使用可重复读（RR）隔离级别？</p>\n<blockquote>\n<p>可重复读(Repeatable Read)，简称为RR 读已提交(Read Commited)，简称为RC</p>\n</blockquote>\n<ul>\n<li>在可重复读（RR）隔离级别下，存在<strong>间隙锁</strong>，导致出现死锁的几率比读已提交（RC）大得多。</li>\n<li>在可重复读（RR）隔离级别下，<strong>条件列未命中索引会锁表</strong>，而在读已提交（RC）隔离级别下，只锁行。也就是说，读以提交的并发性要比可重复读高。</li>\n<li>并且大部分场景下，不可重复读是可以接受的。毕竟数据都已经提交了，读出来本身没有太大问题。</li>\n</ul>\n<h2 id=\"5编码和字符集的关系\">5、编码和字符集的关系</h2>\n<p>我们平时可以在编辑器上输入各种中文英文字母，但这些都是给人读的，不是给计算机读的，其实<strong>计算机真正保存和传输数据都是以二进制0101的格式进行的</strong>。</p>\n<p>那么就需要有一个规则，把中文和英文字母转化为二进制。其中d对应十六进制下的64，它可以转换为01二进制的格式。于是字母和数字就这样一一对应起来了，这就是ASCII编码格式。</p>\n<p>它用<strong>一个字节</strong>，也就是<code>8位</code>来标识字符，基础符号有128个，扩展符号也是128个。也就只能表示下<strong>英文字母和数字</strong>。</p>\n<p>这明显不够用。于是，为了标识<strong>中文</strong>，出现了<strong>GB2312</strong>的编码格式。为了标识<strong>希腊语</strong>，出现了<strong>greek</strong>编码格式，为了标识<strong>俄语</strong>，整了<strong>cp866</strong>编码格式。</p>\n<p>为了统一它们，于是出现了<strong>Unicode编码格式</strong>，它用了2~4个字节来表示字符，这样理论上所有符号都能被收录进去，并且它还完全兼容ASCII的编码，也就是说，同样是字母d，在ASCII用64表示，在Unicode里还是用64来表示。</p>\n<p>但<strong>不同的地方是ASCII编码用1个字节来表示，而Unicode用则两个字节来表示。</strong></p>\n<p>同样都是字母d，unicode比ascii多使用了一个字节，如下：</p>\n<pre><code class=\"language-\\\">D   ASCII:           01100100\nD Unicode:  00000000 01100100\n</code></pre>\n<p>可以看到，上面的unicode编码，前面的都是0，其实用不上，但还占了个字节，有点浪费。如果我们能做到该隐藏时隐藏，这样就能省下不少空间，按这个思路，就是就有了<strong>UTF-8编码</strong>。</p>\n<p>总结一下，<strong>按照一定规则把符号和二进制码对应起来，这就是编码。而把n多这种已经编码的字符聚在一起，就是我们常说的字符集</strong>。</p>\n<p>比如utf-8字符集就是所有utf-8编码格式的字符的合集。</p>\n<p>想看下mysql支持哪些字符集。可以执行 <code>show charset;</code></p>\n<h2 id=\"6utf8和utf8mb4的区别\">6、utf8和utf8mb4的区别</h2>\n<p>上面提到utf-8是在unicode的基础上做的优化，既然unicode有办法表示所有字符，那utf-8也一样可以表示所有字符，为了避免混淆，我在后面叫它<strong>大utf8</strong>。</p>\n<p>mysql支持的字符集中有utf8和utf8mb4。</p>\n<p>先说<strong>utf8mb4</strong>编码，mb4就是<strong>most bytes 4</strong>的意思，从上图最右边的<code>Maxlen</code>可以看到，它最大支持用<strong>4个字节</strong>来表示字符，它几乎可以用来表示目前已知的所有的字符。</p>\n<p>再说mysql字符集里的<strong>utf8</strong>，它是数据库的<strong>默认字符集</strong>。但注意，<strong>此utf8非彼utf8</strong>，我们叫它<strong>小utf8</strong>字符集。为什么这么说，因为从Maxlen可以看出，它最多支持用3个字节去表示字符，按utf8mb4的命名方式，准确点应该叫它<strong>utf8mb3</strong>。</p>\n<p>utf8 就像是阉割版的utf8mb4，只支持部分字符。比如<code>emoji</code>表情，它就不支持。</p>\n<p>而mysql支持的字符集里，第三列，<strong>collation</strong>，它是指<strong>字符集的比较规则</strong>。</p>\n<p>比如，&quot;debug&quot;和&quot;Debug&quot;是同一个单词，但它们大小写不同，该不该判为同一个单词呢。</p>\n<p>这时候就需要用到collation了。</p>\n<p>通过<code>SHOW COLLATION WHERE Charset = 'utf8mb4';</code>可以查看到<code>utf8mb4</code>下支持什么比较规则。</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/e89f7eae13cd4f3a933a6dffd7b73bdb.png\">\n</p>\n<p>如果<code>collation = utf8mb4_general_ci</code>，是指使用utf8mb4字符集的前提下，<strong>挨个字符进行比较</strong>（<code>general</code>），并且不区分大小写（<code>_ci，case insensitice</code>）。</p>\n<p>这种情况下，&quot;debug&quot;和&quot;Debug&quot;是同一个单词。</p>\n<p>如果改成<code>collation=utf8mb4_bin</code>，就是指<strong>挨个比较二进制位大小</strong>。</p>\n<p>于是&quot;debug&quot;和&quot;Debug&quot;就不是同一个单词。</p>\n<p><strong>那utf8mb4对比utf8有什么劣势吗？</strong></p>\n<p>我们知道数据库表里，字段类型如果是<code>char(2)</code>的话，里面的<code>2</code>是指<strong>字符个数</strong>，也就是说<strong>不管这张表用的是什么编码的字符集</strong>，都能放上2个字符。</p>\n<p>而char又是<strong>固定长度</strong>，为了能放下2个utf8mb4的字符，char会默认保留<code>2*4（maxlen=4）= 8</code>个字节的空间。</p>\n<p>如果是utf8mb3，则会默认保留 <code>2 * 3 (maxlen=3) = 6</code>个字节的空间。也就是说，在这种情况下，<strong>utf8mb4会比utf8mb3多使用一些空间。</strong></p>\n<p><strong>对于固定长度的char类型的字符串，utf8mb4会比utf8多使用一些空间</strong>。</p>\n<h2 id=\"7索引\">7、索引</h2>\n<h3 id=\"71索引的定义\">7.1、索引的定义</h3>\n<p>索引是存储引擎用于提高数据库表的访问速度的一种数据结构。</p>\n<h3 id=\"72索引的优缺点\">7.2、索引的优缺点</h3>\n<p><strong>优点：</strong></p>\n<ul>\n<li><strong>加快数据查找速度。</strong></li>\n<li>为用来排序或者是分组的字段添加索引，可以<strong>加快分组和排序的速度。</strong></li>\n<li><strong>加快表与表之间的连接。</strong></li>\n</ul>\n<p><strong>缺点：</strong></p>\n<ul>\n<li>建立索引需要<strong>占用物理空间</strong>。</li>\n<li>会<strong>降低表的增删改的效率</strong>，因为每次对表记录进行增删改，需要进行动态维护索引，导致增删改的时间变长。</li>\n</ul>\n<h3 id=\"73索引的作用\">7.3、索引的作用</h3>\n<p>数据是存储在磁盘上的，查询数据时，如果没有索引，就需要加载所有的数据到内存上，一次进行检索，读取磁盘次数较多。有了索引，就不需要加载全部数据了，因为B+树的高度一般在2-4层，最多只需要读取2-4次磁盘，查询速度大大提升。</p>\n<h3 id=\"74什么情况下需要建立索引\">7.4、什么情况下需要建立索引</h3>\n<ol>\n<li>经常用于<strong>查询</strong>的字段</li>\n<li>经常用于<strong>连接</strong>的字段建立索引，可以加快连接的速度。</li>\n<li>经常需要<strong>排序</strong>的字段建立索引，因为索引已经排好序，可以加快排序查询速度。</li>\n</ol>\n<h3 id=\"75什么情况下不建立索引\">7.5、什么情况下不建立索引</h3>\n<ol>\n<li><strong>where条件中用不到的字段</strong>不适合建立索引。</li>\n<li><strong>表记录较少</strong>。比如只有几百条数据，没必要加索引。</li>\n<li>需要<strong>经常增删改</strong>，需要评估是否适合加索引。</li>\n<li><strong>参与列计算的列</strong>不适合建立索引。</li>\n<li><strong>区分度不高的字段</strong>不适合建立索引，如性别，只有男/女/未知三个值。加了索引，查询效率也不会提高。</li>\n</ol>\n<h3 id=\"76索引的数据结构\">7.6、索引的数据结构</h3>\n<p>索引的数据结构主要有B+树和哈希表，对应的索引分别是<strong>B+树索引和哈希索引</strong>。InnoDB引擎的索引类型有B+树索引和哈希索引，默认的索引类型为B+树索引。</p>\n<p><strong>B+树索引</strong></p>\n<p>B+ 树是基于B 树和叶子节点顺序访问指针进行实现，它具有B树的平衡性，并且通过顺序访问指针来提高区间查询的性能。</p>\n<p>在 B+ 树中，节点中的 <code>key</code> 从左到右递增排列，如果某个指针的左右相邻 <code>key</code> 分别是 keyi 和 keyi+1，则该指针指向节点的所有 <code>key</code> 大于等于 keyi 且小于等于 keyi+1。</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/2d2a398bc5e94876a66d4e8359570895.png\">\n</p>\n<p>进行查找操作时，首先在根节点进行二分查找，找到key 所在的指针，然后递归地在指针所指向的节点进行查找。直到查找到叶子节点，然后在叶子节点上进行二分查找，找出key 所对应的数据项。</p>\n<p>MySQL 数据库使用最多的索引类型是BTREE 索引，底层基于B+树数据结构来实现。</p>\n<pre><code class=\"language-cmd\">mysql&gt; show index from blog\\G;\n*************************** 1. row ***************************\n        Table: blog\n   Non_unique: 0\n     Key_name: PRIMARY\n Seq_in_index: 1\n  Column_name: blog_id\n    Collation: A\n  Cardinality: 4\n     Sub_part: NULL\n       Packed: NULL\n         Null:\n   Index_type: BTREE\n      Comment:\nIndex_comment:\n      Visible: YES\n   Expression: NULL\n\n</code></pre>\n<p><strong>哈希索引</strong></p>\n<p>哈希索引是基于哈希表实现的，对于每一行数据，存储引擎会对索引列进行哈希计算得到哈希码，并且哈希算法要尽量保证不同的列值计算出的哈希码值是不同的，将哈希码的值作为哈希表的key值，将指向数据行的指针作为哈希表的value值。这样查找一个数据的时间复杂度就是O(1)，一般多用于精确查找。</p>\n<h3 id=\"77hash索引和b树索引的区别\">7.7、Hash索引和B+树索引的区别</h3>\n<ul>\n<li>哈希索引<strong>不支持排序</strong>，因为哈希表是无序的。</li>\n<li>哈希索引<strong>不支持范围查找</strong>。</li>\n<li>哈希索引<strong>不支持模糊查询</strong>以及多列索引的最左前缀匹配。</li>\n<li>因为哈希表会存在哈希冲突，所以<strong>哈希索引的性能是不稳定的，而B+树索引的性能相对稳定</strong>，每次查询都是从根节点到叶子节点。</li>\n</ul>\n<h3 id=\"78为什么b树比b树更适合实现数据库索引\">7.8、为什么B+树比B树更适合实现数据库索引</h3>\n<ul>\n<li>由于<strong>B+树的数据都存储在叶子节点中</strong>，叶子节点均为索引，方便扫库，只需要扫一遍叶子节点即可，但是B树因为其分支同样存储着数据，我们要找到具体的数据，需要进行一次中序遍历按序来扫，所以B+树更适合在区间查询的情况，而在数据库中基于范围的查询是非常频繁的，所以通常使用B+树用于数据库索引。</li>\n<li><strong>B+树的节点只存储索引key值，具体信息的地址存在于叶子节点的地址中</strong>。这就使<strong>以页为单位的索引中可以存放更多的节点</strong>。减少更多的I/O支出。</li>\n<li><strong>B+树的查询效率更加稳定，任何关键字的查找必须走一条从根结点到叶子结点的路</strong>。所有关键字查询的路径长度相同，导致每一个数据的查询效率相当。</li>\n</ul>\n<hr />\n<ul>\n<li>B+ 树的非叶子节点不存放实际的记录数据，仅存放索引，因此数据量相同的情况下，相比存储即存索引又存记录的 B 树，B+树的非叶子节点可以存放更多的索引，因此 B+ 树可以比 B 树更「矮胖」，查询底层节点的磁盘 I/O次数会更少。</li>\n<li>B+ 树有大量的冗余节点（所有非叶子节点都是冗余索引），这些冗余索引让 B+ 树在插入、删除的效率都更高，比如删除根节点的时候，不会像 B 树那样会发生复杂的树的变化；</li>\n<li>B+ 树叶子节点之间用链表连接了起来，有利于范围查询，而 B 树要实现范围查询，因此只能通过树的遍历来完成范围查询，这会涉及多个节点的磁盘 I/O 操作，范围查询效率不如 B+ 树。</li>\n</ul>\n<h3 id=\"79索引的分类\">7.9、索引的分类</h3>\n<ol>\n<li>\n<p><strong>主键索引</strong>：名为primary的唯一非空索引，不允许有空值。</p>\n</li>\n<li>\n<p><strong>唯一索引</strong>：索引列的值必须是唯一的，允许有空值。主键索引和唯一索引的区别在于：唯一索引字段可以为null且可以存在多个null值，而主键索引字段不可以为null。唯一索引的用途：唯一标识数据库中的每条记录，主要用来防止重复数据插入。创建唯一索引的SQL语句为：</p>\n<pre><code class=\"language-sql\">ALTER TABLE table_name\nADD CONSTRAINT constraint_name UNIQUE KEY(column_1,column_2,...);\n</code></pre>\n</li>\n<li>\n<p><strong>组合索引</strong>：在表中的多个字段组合上创建的索引，只有在查询条件中使用了这些字段的左边字段时，索引才会被使用，使用组合索引时需遵循最左前缀原则。</p>\n</li>\n<li>\n<p><strong>全文索引</strong>：只能在CHAR 、VARCHAR 和TEXT 类型字段上使用全文索引。</p>\n</li>\n<li>\n<p><strong>普通索引</strong>：普通索引是最基本的索引，它没有任何限制，值可以为空。</p>\n</li>\n</ol>\n<h3 id=\"710什么是最左匹配原则\">7.10、什么是最左匹配原则</h3>\n<p><strong>如果 SQL 语句中用到了组合索引中的最左边的索引，那么这条 SQL 语句就可以利用这个组合索引去进行匹配。当遇到范围查询(<code>&gt;</code>、<code>&lt;</code>、<code>between</code>、<code>like</code>)就会停止匹配，后面的字段不会用到索引</strong>。</p>\n<p>对<code>(a,b,c)</code>建立索引，查询条件使用 a/ab/abc 会走索引，使用 bc 不会走索引。</p>\n<p>对<code>(a,b,c,d)</code>建立索引，查询条件为<code>a = 1 and b = 2 and c &gt; 3 and d = 4</code>，那么a、b和c三个字段能用到索引，而d无法使用索引。因为遇到了范围查询。</p>\n<p>如下图，对(a, b) 建立索引，a 在索引树中是全局有序的，而 b 是全局无序，局部有序（当a相等时，会根据b进行排序）。直接执行<code>b = 2</code>这种查询条件无法使用索引。</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/fe96a1d64f5a4eaf9181803fe8fc1294.png\">\n</p>\n<p>当a的值确定的时候，b是有序的。例如<code>a = 1</code>时，b值为1，2是有序的状态。当<code>a = 2</code>时候，b的值为1，4也是有序状态。 当执行<code>a = 1 and b = 2</code>时a和b字段能用到索引。而执行<code>a &gt; 1 and b = 2</code>时，a字段能用到索引，b字段用不到索引。因为a的值此时是一个范围，不是固定的，在这个范围内b值不是有序的，因此b字段无法使用索引。</p>\n<h3 id=\"711什么是聚集索引\">7.11、什么是聚集索引</h3>\n<p>InnoDB使用<strong>表的主键构造主键索引树</strong>，同时叶子节点中存放的即为整张表的记录数据。聚集索引叶子节点的存储是逻辑上连续的，使用双向链表连接，叶子节点按照主键的顺序排序，因此对于主键的排序查找和范围查找速度比较快。如果表中没有显示指定主键，则会<strong>选择表中的第一个不允许为NULL 的唯一索引</strong>。如果没有主键也没有合适的唯一索引，那么InnoDB 内部会生成一个隐藏的主键作为聚集索引，这个隐藏的主键长度为6个字节，它的值会随着数据的插入自增。</p>\n<h3 id=\"712什么是覆盖索引\">7.12、什么是覆盖索引</h3>\n<p><em>InnoDB 的数据是按「数据页」为单位来读写的，默认数据页大小为 16 KB。每个数据页之间通过双向链表的形式组织起来，物理上不连续，但是逻辑上连续。</em></p>\n<p><em>数据页内包含用户记录，每个记录之间用单向链表的方式组织起来，为了加快在数据页内高效查询记录，设计了一个页目录，页目录存储各个槽（分组），且主键值是有序的，于是可以通过二分查找法的方式进行检索从而提高效率。</em></p>\n<p><em>为了高效查询记录所在的数据页，InnoDB 采用 b+ 树作为索引，每个节点都是一个数据页。</em></p>\n<p><em>如果叶子节点存储的是实际数据的就是聚簇索引，一个表只能有一个聚簇索引；如果叶子节点存储的不是实际数据，而是主键值则就是二级索引，一个表中可以有多个二级索引。</em></p>\n<p><em>在使用二级索引进行查找数据时，如果查询的数据能在二级索引找到，那么就是「索引覆盖」操作，如果查询的数据不在二级索引里，就需要先在二级索引找到主键值，需要去聚簇索引中获得数据行，这个过程就叫作「回表」。</em></p>\n<p><strong>覆盖索引（covering index ，或称为索引覆盖）即从非主键索引中就能查到的记录，而不需要查询主键索引中的记录，避免了回表的产生减少了树的搜索次数，显著提升性能</strong>。不是所有类型的索引都可以成为覆盖索引。覆盖索引要存储索引列的值，而哈希索引、全文索引不存储索引列的值，所以<strong>MySQL使用b+树索引做覆盖索引</strong>。</p>\n<p>对于使用了覆盖索引的查询，在查询前面使用<code>explain</code>，输出的extra列会显示为<code>using index</code>。</p>\n<p>比如<code>user_like</code> 用户点赞表，组合索引为<code>(user_id, blog_id)</code>，<code>user_id</code>和<code>blog_id</code>都不为<code>null</code>。</p>\n<pre><code class=\"language-sql\">explain select blog_id from user_like where user_id = 13;\n</code></pre>\n<p><code>explain</code>结果的<code>Extra</code>列为<code>Using index</code>，查询的列被索引覆盖，并且where筛选条件符合最左前缀原则，通过<strong>索引查找</strong>就能直接找到符合条件的数据，不需要回表查询数据。</p>\n<pre><code class=\"language-sql\">explain select user_id from user_like where blog_id = 1;\n</code></pre>\n<p><code>explain</code>结果的<code>Extra</code>列为<code>Using where; Using index</code>， 查询的列被索引覆盖，where筛选条件不符合最左前缀原则，无法通过索引查找找到符合条件的数据，但可以通过<strong>索引扫描</strong>找到符合条件的数据，也不需要回表查询数据。</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/01d86030cd5e4303bb22af9d6167c413.png\">\n</p>\n<blockquote>\n<p><em>如果某个查询语句使用了二级索引，但是查询的数据不是主键值，这时在二级索引找到主键值后，需要去聚簇索引中获得数据行，这个过程就叫作「回表」，也就是说要查两个 B+ 树才能查到数据。不过，当查询的数据是主键值时，因为只在二级索引就能查询到，不用再去聚簇索引查，这个过程就叫作「索引覆盖」，也就是只需要查一个 B+ 树就能找到数据。</em></p>\n</blockquote>\n<h3 id=\"713索引的设计原则\">7.13、索引的设计原则</h3>\n<ul>\n<li>\n<p>对于经常作为查询条件的字段，应该建立索引，以提高查询速度</p>\n</li>\n<li>\n<p>为经常需要排序、分组和联合操作的字段建立索引</p>\n</li>\n<li>\n<p>索引列的<strong>区分度越高</strong>，索引的效果越好。比如使用性别这种区分度很低的列作为索引，效果就会很差。</p>\n</li>\n<li>\n<p>避免给&quot;大字段&quot;建立索引。尽量使用数据量小的字段作为索引。因为<code>MySQL</code>在维护索引的时候是会将字段值一起维护的，那这样必然会导致索引占用更多的空间，另外在排序的时候需要花费更多的时间去对比。</p>\n</li>\n<li>\n<p>尽量使用<strong>短索引</strong>，对于较长的字符串进行索引时应该指定一个较短的前缀长度，因为较小的索引涉及到的磁盘I/O较少，查询速度更快。</p>\n</li>\n<li>\n<p>索引不是越多越好，每个索引都需要额外的物理空间，维护也需要花费时间。</p>\n</li>\n<li>\n<p>频繁增删改的字段不要建立索引。假设某个字段频繁修改，那就意味着需要频繁的重建索引，这必然影响MySQL的性能</p>\n</li>\n<li>\n<p>利用<strong>最左前缀原则</strong>。</p>\n</li>\n<li>\n<p>自增主键可以让主键索引尽量地保持递增顺序插入，避免了页分裂，因此索引更紧凑，在查询的时候，效率也就更高。</p>\n</li>\n</ul>\n<h3 id=\"714索引什么时候会失效\">7.14、索引什么时候会失效</h3>\n<p>导致索引失效的情况：</p>\n<ul>\n<li>对于组合索引，不是使用组合索引最左边的字段，则不会使用索引</li>\n<li>以%开头的like查询如<code>%abc</code>，无法使用索引；非%开头的like查询如<code>abc%</code>，相当于范围查询，会使用索引</li>\n<li>查询条件中列类型是字符串，没有使用引号，可能会因为类型不同发生隐式转换，使索引失效</li>\n<li>判断索引列是否不等于某个值时</li>\n<li>对索引列进行运算</li>\n<li>查询条件使用<code>or</code>连接，也会导致索引失效</li>\n</ul>\n<h3 id=\"715什么是前缀索引\">7.15、什么是前缀索引</h3>\n<p>有时需要在很长的字符列上创建索引，这会造成索引特别大且慢。使用前缀索引可以避免这个问题。</p>\n<p><strong>前缀索引是指对文本或者字符串的前几个字符建立索引</strong>，这样索引的长度更短，查询速度更快。</p>\n<p>创建前缀索引的关键在于选择足够长的前缀以<strong>保证较高的索引选择性</strong>。索引选择性越高查询效率就越高，因为选择性高的索引可以让MySQL在查找时过滤掉更多的数据行。</p>\n<p>建立前缀索引的方式：</p>\n<pre><code class=\"language-sql\">// email列创建前缀索引\nALTER TABLE table_name ADD KEY(column_name(prefix_length));\n</code></pre>\n<h2 id=\"8常见的存储引擎有哪些\">8、常见的存储引擎有哪些</h2>\n<p>MySQL中常用的四种存储引擎分别是： <strong>MyISAM</strong>、<strong>InnoDB</strong>、<strong>MEMORY</strong>、<strong>ARCHIVE</strong>。MySQL 5.5版本后默认的存储引擎为<code>InnoDB</code>。</p>\n<p><strong>InnoDB存储引擎</strong></p>\n<p>InnoDB是MySQL<strong>默认的事务型存储引擎</strong>，使用最广泛，基于聚簇索引建立的。InnoDB内部做了很多优化，如能够自动在内存中创建自适应hash索引，以加速读操作。</p>\n<p><strong>优点</strong>：支持事务和崩溃修复能力；引入了行级锁和外键约束。</p>\n<p><strong>缺点</strong>：占用的数据空间相对较大。</p>\n<p><strong>适用场景</strong>：需要事务支持，并且有较高的并发读写频率。</p>\n<p><strong>MyISAM存储引擎</strong></p>\n<p>数据以紧密格式存储。对于只读数据，或者表比较小、可以容忍修复操作，可以使用MyISAM引擎。MyISAM会将表存储在两个文件中，数据文件<code>.MYD</code>和索引文件<code>.MYI</code>。</p>\n<p><strong>优点</strong>：访问速度快。</p>\n<p><strong>缺点</strong>：MyISAM不支持事务和行级锁，不支持崩溃后的安全恢复，也不支持外键。</p>\n<p><strong>适用场景</strong>：对事务完整性没有要求；表的数据都是只读的。</p>\n<p><strong>MEMORY存储引擎</strong></p>\n<p>MEMORY引擎将数据全部放在内存中，访问速度较快，但是一旦系统奔溃的话，数据都会丢失。</p>\n<p>MEMORY引擎默认使用哈希索引，将键的哈希值和指向数据行的指针保存在哈希索引中。</p>\n<p><strong>优点</strong>：访问速度较快。</p>\n<p><strong>缺点</strong>：</p>\n<ol>\n<li>哈希索引数据不是按照索引值顺序存储，无法用于排序。</li>\n<li>不支持部分索引匹配查找，因为哈希索引是使用索引列的全部内容来计算哈希值的。</li>\n<li>只支持等值比较，不支持范围查询。</li>\n<li>当出现哈希冲突时，存储引擎需要遍历链表中所有的行指针，逐行进行比较，直到找到符合条件的行。</li>\n</ol>\n<p><strong>ARCHIVE存储引擎</strong></p>\n<p>ARCHIVE存储引擎非常适合存储大量独立的、作为历史记录的数据。ARCHIVE提供了压缩功能，拥有高效的插入速度，但是这种引擎不支持索引，所以查询性能较差。</p>\n<h2 id=\"9myisam和innodb的区别\">9、MyISAM和InnoDB的区别</h2>\n<ul>\n<li>\n<p><strong>存储结构的区别</strong>。每个MyISAM在磁盘上存储成三个文件。文件的名字以表的名字开始，扩展名指出文件类型。 .frm文件存储表定义。数据文件的扩展名为.MYD (MYData)。索引文件的扩展名是.MYI (MYIndex)。InnoDB所有的表都保存在同一个数据文件中（也可能是多个文件，或者是独立的表空间文件），InnoDB表的大小只受限于操作系统文件的大小，一般为2GB。</p>\n</li>\n<li>\n<p><strong>存储空间的区别</strong>。MyISAM支持支持三种不同的存储格式：静态表(默认，但是注意数据末尾不能有空格，会被去掉)、动态表、压缩表。当表在创建之后并导入数据之后，不会再进行修改操作，可以使用压缩表，极大的减少磁盘的空间占用。InnoDB需要更多的内存和存储，它会在主内存中建立其专用的缓冲池用于高速缓冲数据和索引。</p>\n</li>\n<li>\n<p><strong>可移植性、备份及恢复</strong>。MyISAM数据是以文件的形式存储，所以在跨平台的数据转移中会很方便。在备份和恢复时可单独针对某个表进行操作。对于InnoDB，可行的方案是拷贝数据文件、备份 binlog，或者用mysqldump，在数据量达到几十G的时候就相对麻烦了。</p>\n</li>\n<li>\n<p><strong>是否支持行级锁</strong>。MyISAM 只支持表级锁，用户在操作myisam表时，select，update，delete，insert语句都会给表自动加锁，如果加锁以后的表满足insert并发的情况下，可以在表的尾部插入新的数据。而InnoDB 支持行级锁和表级锁，默认为行级锁。行锁大幅度提高了多用户并发操作的性能。</p>\n</li>\n<li>\n<p><strong>是否支持事务和崩溃后的安全恢复</strong>。 MyISAM 不提供事务支持。而InnoDB 提供事务支持，具有事务、回滚和崩溃修复能力。</p>\n</li>\n<li>\n<p><strong>是否支持外键</strong>。MyISAM不支持，而InnoDB支持。</p>\n</li>\n<li>\n<p><strong>是否支持MVCC</strong>。MyISAM不支持，InnoDB支持。应对高并发事务，MVCC比单纯的加锁更高效。</p>\n</li>\n<li>\n<p><strong>是否支持聚集索引</strong>。MyISAM不支持聚集索引，InnoDB支持聚集索引。</p>\n</li>\n<li>\n<p><strong>全文索引</strong>。MyISAM支持 FULLTEXT类型的全文索引。InnoDB不支持FULLTEXT类型的全文索引，但是innodb可以使用sphinx插件支持全文索引，并且效果更好。</p>\n</li>\n<li>\n<p><strong>表主键</strong>。MyISAM允许没有任何索引和主键的表存在，索引都是保存行的地址。对于InnoDB，如果没有设定主键或者非空唯一索引，就会自动生成一个6字节的主键(用户不可见)。</p>\n</li>\n<li>\n<p><strong>表的行数</strong>。MyISAM保存有表的总行数，如果<code>select count(*) from table</code>;会直接取出该值。InnoDB没有保存表的总行数，如果使用select count(*) from table；就会遍历整个表，消耗相当大，但是在加了where条件后，MyISAM和InnoDB处理的方式都一样。</p>\n</li>\n</ul>\n<hr />\n<ul>\n<li>\n<p><strong>存储结构的区别</strong>。每个MyISAM在磁盘上存储成三个文件（索引文件、数据文件和表结构文件）。InnoDB所有的表都保存在同一个数据文件中（也可能是多个文件，或者是独立的表空间文件）。</p>\n</li>\n<li>\n<p><strong>存储空间的区别</strong>。MyISAM支持三种不同的存储格式：静态表、动态表、压缩表。当表在创建之后并导入数据之后，不会再进行修改操作，可以使用压缩表，极大的减少磁盘的空间占用。InnoDB需要更多的内存和存储，它会在主内存中建立其专用的缓冲池用于高速缓冲数据和索引。</p>\n</li>\n<li>\n<p><strong>可移植性、备份及恢复</strong>。MyISAM数据是以文件的形式存储，所以在跨平台的数据转移中会很方便。对于InnoDB，可行的方案是拷贝数据文件、备份 binlog，或者用mysqldump，在数据量达到几十G的时候就相对麻烦了。</p>\n</li>\n<li>\n<p><strong>是否支持行级锁</strong>。MyISAM 只支持表级锁，而InnoDB 支持行级锁和表级锁，默认为行级锁。行锁大幅度提高了多用户并发操作的性能。</p>\n</li>\n<li>\n<p><strong>是否支持事务和崩溃后的安全恢复</strong>。 MyISAM 不提供事务支持。而InnoDB 提供事务支持，具有事务、回滚和崩溃修复能力。</p>\n</li>\n<li>\n<p><strong>是否支持外键</strong>。MyISAM不支持，而InnoDB支持。</p>\n</li>\n<li>\n<p><strong>是否支持MVCC</strong>。MyISAM不支持，InnoDB支持。应对高并发事务，MVCC比单纯的加锁更高效。</p>\n</li>\n<li>\n<p><strong>是否支持聚集索引</strong>。MyISAM不支持聚集索引，InnoDB支持聚集索引。</p>\n</li>\n<li>\n<p><strong>全文索引</strong>。MyISAM支持 FULLTEXT类型的全文索引。InnoDB不支持FULLTEXT类型的全文索引，但是innodb可以使用sphinx插件支持全文索引，并且效果更好。</p>\n</li>\n<li>\n<p><strong>表主键</strong>。MyISAM允许没有任何索引和主键的表存在，索引都是保存行的地址。对于InnoDB，如果没有设定主键或者非空唯一索引，就会自动生成一个6字节的主键(用户不可见)。</p>\n</li>\n<li>\n<p><strong>表的行数</strong>。MyISAM保存有表的总行数，如果<code>select count(*) from table</code>;会直接取出该值。InnoDB没有保存表的总行数，如果使用select count(*) from table；就会遍历整个表，消耗相当大，但是在加了where条件后，MyISAM和InnoDB处理的方式都一样。</p>\n</li>\n</ul>\n<h2 id=\"10mysql有哪些锁\">10、MySQL有哪些锁</h2>\n<p><strong>按锁粒度分类</strong>，有行级锁、表级锁和页级锁。</p>\n<ol>\n<li>行级锁是mysql中锁的粒度最细的一种锁。表示只针对当前操作的行进行加锁。行级锁能大大减少数据库操作的冲突，其加锁粒度最小，但加锁的开销也最大。行级锁的类型主要有三类：\n<ul>\n<li>Record Lock，记录锁，也就是仅仅把一条记录锁上；</li>\n<li>Gap Lock，间隙锁，锁定一个范围，但是不包含记录本身；</li>\n<li>Next-Key Lock：Record Lock + Gap Lock 的组合，锁定一个范围，并且锁定记录本身。</li>\n</ul>\n</li>\n<li>表级锁是mysql中锁定粒度最大的一种锁，表示对当前操作的整张表加锁，它实现简单，资源消耗较少，被大部分mysql引擎支持。最常使用的MyISAM与InnoDB都支持表级锁定。</li>\n<li>页级锁是 MySQL 中锁定粒度介于行级锁和表级锁中间的一种锁。表级锁速度快，但冲突多，行级冲突少，但速度慢。因此，采取了折衷的页级锁，一次锁定相邻的一组记录。</li>\n</ol>\n<p><strong>按锁级别分类</strong>，有共享锁、排他锁和意向锁。</p>\n<ol>\n<li>共享锁又称读锁，是读取操作创建的锁。其他用户可以并发读取数据，但任何事务都不能对数据进行修改（获取数据上的排他锁），直到已释放所有共享锁。</li>\n<li>排他锁又称写锁、独占锁，如果事务T对数据A加上排他锁后，则其他事务不能再对A加任何类型的封锁。获准排他锁的事务既能读数据，又能修改数据。</li>\n<li>意向锁是表级锁，其设计目的主要是为了在一个事务中揭示下一行将要被请求锁的类型。InnoDB 中的两个表锁：\n<ul>\n<li>意向共享锁（IS）：表示事务准备给数据行加入共享锁，也就是说一个数据行加共享锁前必须先取得该表的IS锁；</li>\n<li>意向排他锁（IX）：类似上面，表示事务准备给数据行加入排他锁，说明事务在一个数据行加排他锁前必须先取得该表的IX锁。</li>\n</ul>\n</li>\n</ol>\n<p>意向锁是 InnoDB 自动加的，不需要用户干预。</p>\n<p><strong>对于INSERT、UPDATE和DELETE，InnoDB 会自动给涉及的数据加排他锁；对于一般的SELECT语句，InnoDB 不会加任何锁</strong>，事务可以通过以下语句显式加共享锁或排他锁。</p>\n<p>共享锁：<code>SELECT … LOCK IN SHARE MODE;</code></p>\n<p>排他锁：<code>SELECT … FOR UPDATE;</code></p>\n<h2 id=\"11mvcc-实现原理\">11、MVCC 实现原理</h2>\n<p>我们需要了解两个知识：</p>\n<p>Read View 中四个字段作用；\n聚簇索引记录中两个跟事务有关的隐藏列；\n那 Read View 到底是个什么东西？</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/26e9aa37ce3440db94c8e15459fe2e53.png\">\n</p>\n<p>Read View 有四个重要的字段：</p>\n<ul>\n<li><em><strong>m_ids</strong></em> ：指的是在创建 Read View 时，当前数据库中「活跃事务」的事务 id 列表，注意是一个列表，“活跃事务”指的就是，启动了但还没提交的事务。</li>\n<li><em><strong>min_trx_id</strong></em> ：指的是在创建 Read View 时，当前数据库中「活跃事务」中事务 id 最小的事务，也就是 m_ids 的最小值。</li>\n<li><em><strong>max_trx_id</strong></em> ：这个并不是 m_ids 的最大值，而是创建 Read View 时当前数据库中应该给下一个事务的 id 值，也就是全局事务中最大的事务 id 值 + 1；</li>\n<li><em><strong>creator_trx_id</strong></em> ：指的是创建该 Read View 的事务的事务 id。\n知道了 Read View 的字段，我们还需要了解聚簇索引记录中的两个隐藏列。</li>\n</ul>\n<p>假设在账户余额表插入一条小林余额为 100 万的记录，然后我把这两个隐藏列也画出来，该记录的整个示意图如下：</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/30528a510be44807b7535d94eacd1304.png\">\n</p>\n<p>对于使用 InnoDB 存储引擎的数据库表，它的聚簇索引记录中都包含下面两个隐藏列：</p>\n<ul>\n<li><em><strong>trx_id</strong></em>，当一个事务对某条聚簇索引记录进行改动时，就会把该事务的事务 id 记录在 trx_id 隐藏列里；</li>\n<li><em><strong>roll_pointer</strong></em>，每次对某条聚簇索引记录进行改动时，都会把旧版本的记录写入到 undo 日志中，然后这个隐藏列是个指针，指向每一个旧版本记录，于是就可以通过它找到修改前的记录。\n在创建 Read View 后，我们可以将记录中的 trx_id 划分这三种情况：</li>\n</ul>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/8dc914e744dc4d9aa514ed1bd0fef737.png\">\n</p>\n<p>一个事务去访问记录的时候，除了自己的更新记录总是可见之外，还有这几种情况：</p>\n<ul>\n<li>如果记录的 trx_id 值小于 Read View 中的 min_trx_id 值，表示这个版本的记录是在创建 Read View 前已经提交的事务生成的，所以该版本的记录对当前事务可见。</li>\n<li>如果记录的 trx_id 值大于等于 Read View 中的 max_trx_id 值，表示这个版本的记录是在创建 Read View 后才启动的事务生成的，所以该版本的记录对当前事务不可见。</li>\n<li>如果记录的 trx_id 值在 Read View 的 min_trx_id 和 max_trx_id 之间，需要判断 trx_id 是否在 m_ids 列表中：\n<ul>\n<li>如果记录的 trx_id 在 m_ids 列表中，表示生成该版本记录的活跃事务依然活跃着（还没提交事务），所以该版本的记录对当前事务不可见。</li>\n<li>如果记录的 trx_id 不在 m_ids列表中，表示生成该版本记录的活跃事务已经被提交，所以该版本的记录对当前事务可见。</li>\n</ul>\n</li>\n</ul>\n<p><em><strong>这种通过「版本链」来控制并发事务访问同一个记录时的行为就叫 MVCC（多版本并发控制）</strong></em>。</p>\n<h2 id=\"12快照读和当前读\">12、快照读和当前读</h2>\n<p>表记录有两种读取方式。</p>\n<ul>\n<li>快照读：<strong>读取的是快照版本</strong>。普通的<code>SELECT</code>就是快照读。通过mvcc来进行并发控制的，不用加锁。</li>\n<li>当前读：<strong>读取的是最新版本</strong>。<code>UPDATE、DELETE、INSERT、SELECT … LOCK IN SHARE MODE、SELECT … FOR UPDATE</code>是当前读。</li>\n</ul>\n<p>快照读情况下，InnoDB通过<code>mvcc</code>机制避免了幻读现象。而<code>mvcc</code>机制无法避免当前读情况下出现的幻读现象。因为当前读每次读取的都是最新数据，这时如果两次查询中间有其它事务插入数据，就会产生幻读。</p>\n<p>下面举个例子说明下：</p>\n<p>1、首先，user表只有两条记录，具体如下：</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/a0f3abf69e0e4a45868300c8f96ae0ad.png\">\n</p>\n<p>2、事务a和事务b同时开启事务<code>start transaction</code>；</p>\n<p>3、事务a插入数据然后提交；</p>\n<pre><code class=\"language-sql\">insert into user(user_name, user_password, user_mail, user_state) values('tyson', 'a', 'a', 0);\n</code></pre>\n<p>4、事务b执行全表的update；</p>\n<pre><code class=\"language-sql\">update user set user_name = 'a';\n</code></pre>\n<p>5、事务b然后执行查询，查到了事务a中插入的数据。（下图左边是事务b，右边是事务a。事务开始之前只有两条记录，事务a插入一条数据之后，事务b查询出来是三条数据）</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/f98b40f48d1641018a638e91fda8a69d.png\">\n</p>\n<p>以上就是当前读出现的幻读现象。</p>\n<p><strong>那么MySQL是如何避免幻读？</strong></p>\n<ul>\n<li>针对快照读（普通 select 语句），是通过 MVCC 方式解决了幻读，因为可重复读隔离级别下，事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，即使中途有其他事务插入了一条数据，是查询不出来这条数据的，所以就很好了避免幻读问题。</li>\n<li>针对当前读（select ... for update 等语句），是通过 next-key lock（记录锁+间隙锁）方式解决了幻读，因为当执行 select ... for update 语句的时候，会加上 next-key lock，如果有其他事务在 next-key lock 锁范围内插入了一条记录，那么这个插入语句就会被阻塞，无法成功插入，所以就很好了避免幻读问题。</li>\n</ul>\n<p>next-key包括两部分：行锁和间隙锁。行锁是加在索引上的锁，间隙锁是加在索引之间的。</p>\n<p><code>Serializable</code>隔离级别也可以避免幻读，会锁住整张表，并发性极低，一般不会使用。</p>\n<h2 id=\"13共享锁和排他锁\">13、共享锁和排他锁</h2>\n<p>SELECT 的读取锁定主要分为两种方式：共享锁和排他锁。</p>\n<pre><code class=\"language-sql\">select * from table where id&lt;6 lock in share mode;--共享锁\nselect * from table where id&lt;6 for update;--排他锁\n</code></pre>\n<p>这两种方式主要的不同在于<code>LOCK IN SHARE MODE </code>多个事务同时更新同一个表单时很容易造成死锁。</p>\n<p>申请排他锁的前提是，没有线程对该结果集的任何行数据使用排它锁或者共享锁，否则申请会受到阻塞。在进行事务操作时，MySQL会对查询结果集的每行数据添加排它锁，其他线程对这些数据的更改或删除操作会被阻塞（只能读操作），直到该语句的事务被<code>commit</code>语句或<code>rollback</code>语句结束为止。</p>\n<p><code>SELECT... FOR UPDATE</code> 使用注意事项：</p>\n<ol>\n<li><code>for update</code> 仅适用于innodb，且必须在事务范围内才能生效。</li>\n<li>根据主键进行查询，查询条件为<code>like</code>或者不等于，主键字段产生<strong>表锁</strong>。</li>\n<li>根据非索引字段进行查询，会产生<strong>表锁</strong>。</li>\n</ol>\n<h2 id=\"14bin-logredo-logundo-log\">14、bin log/redo log/undo log</h2>\n<p>MySQL日志主要包括查询日志、慢查询日志、事务日志、错误日志、二进制日志等。其中比较重要的是 <code>bin log</code>（二进制日志）和 <code>redo log</code>（重做日志）和 <code>undo log</code>（回滚日志）。</p>\n<p><strong>bin log</strong></p>\n<p><code>bin log</code>是MySQL数据库级别的文件，记录对MySQL数据库执行修改的所有操作，不会记录select和show语句，主要用于恢复数据库和同步数据库。</p>\n<p><strong>redo log</strong></p>\n<p><code>redo log</code>是innodb引擎级别，用来记录innodb存储引擎的事务日志，不管事务是否提交都会记录下来，用于数据恢复。当数据库发生故障，innoDB存储引擎会使用<code>redo log</code>恢复到发生故障前的时刻，以此来保证数据的完整性。将参数<code>innodb_flush_log_at_tx_commit</code>设置为1，那么在执行commit时会将<code>redo log</code>同步写到磁盘。</p>\n<p><strong>undo log</strong></p>\n<p>除了记录<code>redo log</code>外，当进行数据修改时还会记录<code>undo log</code>，<code>undo log</code>用于数据的撤回操作，它保留了记录修改前的内容。通过<code>undo log</code>可以实现事务回滚，并且可以根据<code>undo log</code>回溯到某个特定的版本的数据，<strong>实现MVCC</strong>。</p>\n<blockquote>\n<ul>\n<li><em><strong>实现事务回滚</strong></em>，保障事务的原子性。事务处理过程中，如果出现了错误或者用户执 行了 ROLLBACK 语句，MySQL 可以利用 undo log 中的历史数据将数据恢复到事务开始之前的状态。</li>\n<li><em><strong>实现 MVCC（多版本并发控制）关键因素之一</strong></em>。MVCC 是通过 ReadView + undo log 实现的。undo log 为每条记录保存多份历史数据，MySQL 在执行快照读（普通 select 语句）的时候，会根据事务的 Read View 里的信息，顺着 undo log 的版本链找到满足其可见性的记录。</li>\n</ul>\n<p><strong>Buffer Poll：</strong> MySQL 的数据都是存在磁盘中的，那么我们要更新一条记录的时候，得先要从磁盘读取该记录，然后在内存中修改这条记录。修改完这条记录后将记录放入缓存中，下次有查询语句命中了这条记录，直接读取缓存中的记录，就不需要从磁盘获取数据了。</p>\n<ul>\n<li>当读取数据时，如果数据存在于 Buffer Pool 中，客户端就会直接读取 Buffer Pool 中的数据，否则再去磁盘中读取。</li>\n<li>当修改数据时，如果数据存在于 Buffer Pool 中，那直接修改 Buffer Pool 中数据所在的页，然后将其页设置为脏页（该页的内存数据和磁盘上的数据已经不一致），为了减少磁盘I/O，不会立即将脏页写入磁盘，后续由后台线程选择一个合适的时机将脏页写入到磁盘。</li>\n</ul>\n<p><strong>redo log：</strong> Buffer Pool 是提高了读写效率没错，但是问题来了，Buffer Pool 是基于内存的，而内存总是不可靠，万一断电重启，还没来得及落盘的脏页数据就会丢失。\n为了防止断电导致数据丢失的问题，当有一条记录需要更新的时候，InnoDB 引擎就会先更新内存（同时标记为脏页），然后将本次对这个页的修改以 redo log 的形式记录下来，这个时候更新就算完成了。</p>\n<p><strong>bin log：</strong> MySQL 在完成一条更新操作后，Server 层还会生成一条 binlog，等之后事务提交的时候，会将该事物执行过程中产生的所有 binlog 统一写 入 binlog 文件。\nbinlog 文件是记录了所有数据库表结构变更和表数据修改的日志，不会记录查询类的操作，比如 SELECT 和 SHOW 操作。</p>\n</blockquote>\n<h2 id=\"15bin-log和redo-log有什么区别\">15、bin log和redo log有什么区别</h2>\n<ul>\n<li>\n<p>binlog 是 MySQL 的 Server 层实现的日志，所有存储引擎都可以使用；redo log 是 Innodb 存储引擎实现的日志；</p>\n</li>\n<li>\n<p>binlog 是追加写，写满一个文件，就创建一个新的文件继续写，不会覆盖以前的日志，保存的是全量的日志。redo log 是循环写，日志空间大小是固定，全部写满就从头开始，保存未被刷入磁盘的脏页日志。</p>\n</li>\n<li>\n<p><code>bin log</code>是逻辑日志，记录的是SQL语句的原始逻辑；<code>redo log</code>是物理日志，记录的是在某个数据页上做了什么修改。</p>\n</li>\n</ul>\n<h2 id=\"16讲一下mysql架构\">16、讲一下MySQL架构</h2>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/3feecf7a27754947b91011da413669b4.png\">\n</p>\n<p>MySQL主要分为 Server 层和存储引擎层：</p>\n<ul>\n<li><strong>Server 层</strong>：主要包括连接器、查询缓存、分析器、优化器、执行器等，所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图，函数等，还有一个通用的日志模块 binglog 日志模块。</li>\n<li><strong>存储引擎</strong>： 主要负责数据的存储和读取。server 层通过api与存储引擎进行通信。</li>\n</ul>\n<blockquote>\n<p>Server 层负责建立连接、分析和执行 SQL。MySQL 大多数的核心功能模块都在这实现，主要包括连接器，查询缓存、解析器、预处理器、优化器、执行器等。另外，所有的内置函数（如日期、时间、数学和加密函数等）和所有跨存储引擎的功能（如存储过程、触发器、视图等。）都在 Server 层实现。</p>\n<p>存储引擎层负责数据的存储和提取。支持 InnoDB、MyISAM、Memory 等多个存储引擎，不同的存储引擎共用一个 Server 层。现在最常用的存储引擎是 InnoDB，从 MySQL 5.5 版本开始， InnoDB 成为了 MySQL 的默认存储引擎。我们常说的索引数据结构，就是由存储引擎层实现的，不同的存储引擎支持的索引类型也不相同，比如 InnoDB 支持索引类型是 B+树 ，且是默认使用，也就是说在数据表中创建的主键索引和二级索引默认使用的是 B+ 树索引。</p>\n</blockquote>\n<p><strong>Server 层基本组件</strong></p>\n<ul>\n<li><strong>连接器：</strong> 当客户端连接 MySQL 时，server层会对其进行身份认证和权限校验。</li>\n<li><strong>查询缓存:</strong> 执行查询语句的时候，会先查询缓存，先校验这个 sql 是否执行过，如果有缓存这个 sql，就会直接返回给客户端，如果没有命中，就会执行后续的操作。</li>\n<li><strong>分析器:</strong> 没有命中缓存的话，SQL 语句就会经过分析器，主要分为两步，词法分析和语法分析，先看 SQL 语句要做什么，再检查 SQL 语句语法是否正确。</li>\n<li><strong>优化器：</strong> 优化器对查询进行优化，包括重写查询、决定表的读写顺序以及选择合适的索引等，生成执行计划。</li>\n<li><strong>执行器：</strong> 首先执行前会校验该用户有没有权限，如果没有权限，就会返回错误信息，如果有权限，就会根据执行计划去调用引擎的接口，返回结果。</li>\n</ul>\n<h2 id=\"17分库分表\">17、分库分表</h2>\n<p>当单表的数据量达到1000W或100G以后，优化索引、添加从库等可能对数据库性能提升效果不明显，此时就要考虑对其进行切分了。切分的目的就在于减少数据库的负担，缩短查询的时间。</p>\n<p>数据切分可以分为两种方式：垂直划分和水平划分。</p>\n<p><strong>垂直划分</strong></p>\n<p>垂直划分数据库是根据业务进行划分，例如购物场景，可以将库中涉及商品、订单、用户的表分别划分出成一个库，通过降低单库的大小来提高性能。同样的，分表的情况就是将一个大表根据业务功能拆分成一个个子表，例如商品基本信息和商品描述，商品基本信息一般会展示在商品列表，商品描述在商品详情页，可以将商品基本信息和商品描述拆分成两张表。</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/a5ceca78b476467ab019021543fa4244.png\">\n</p>\n<p><strong>优点</strong>：行记录变小，数据页可以存放更多记录，在查询时减少I/O次数。</p>\n<p><strong>缺点</strong>：</p>\n<ul>\n<li>主键出现冗余，需要管理冗余列；</li>\n<li>会引起表连接JOIN操作，可以通过在业务服务器上进行join来减少数据库压力；</li>\n<li>依然存在单表数据量过大的问题。</li>\n</ul>\n<p><strong>水平划分</strong></p>\n<p>水平划分是根据一定规则，例如时间或id序列值等进行数据的拆分。比如根据年份来拆分不同的数据库。每个数据库结构一致，但是数据得以拆分，从而提升性能。\n<img src=\"http://110.41.141.141:9000/weblog/weblog/72decfe442834c83aa1c8c3480fb85ef.png\">\n</p>\n<p><strong>优点</strong>：单库（表）的数据量得以减少，提高性能；切分出的表结构相同，程序改动较少。</p>\n<p><strong>缺点</strong>：</p>\n<ul>\n<li>分片事务一致性难以解决</li>\n<li>跨节点<code>join</code>性能差，逻辑复杂</li>\n<li>数据分片在扩容时需要迁移</li>\n</ul>\n<h2 id=\"18什么是分区表\">18、什么是分区表</h2>\n<p>分区是把一张表的数据分成N多个区块。分区表是一个独立的逻辑表，但是底层由多个物理子表组成。</p>\n<p>当查询条件的数据分布在某一个分区的时候，查询引擎只会去某一个分区查询，而不是遍历整个表。在管理层面，如果需要删除某一个分区的数据，只需要删除对应的分区即可。</p>\n<p>分区一般都是放在单机里的，用的比较多的是时间范围分区，方便归档。只不过分库分表需要代码实现，分区则是mysql内部实现。分库分表和分区并不冲突，可以结合使用。</p>\n<h2 id=\"19分区表类型\">19、分区表类型</h2>\n<p><strong>range分区</strong>，按照范围分区。比如按照时间范围分区</p>\n<pre><code class=\"language-sql\">CREATE TABLE test_range_partition(\n       id INT auto_increment,\n       createdate DATETIME,\n       primary key (id,createdate)\n   ) \n   PARTITION BY RANGE (TO_DAYS(createdate) ) (\n      PARTITION p201801 VALUES LESS THAN ( TO_DAYS('20180201') ),\n      PARTITION p201802 VALUES LESS THAN ( TO_DAYS('20180301') ),\n      PARTITION p201803 VALUES LESS THAN ( TO_DAYS('20180401') ),\n      PARTITION p201804 VALUES LESS THAN ( TO_DAYS('20180501') ),\n      PARTITION p201805 VALUES LESS THAN ( TO_DAYS('20180601') ),\n      PARTITION p201806 VALUES LESS THAN ( TO_DAYS('20180701') ),\n      PARTITION p201807 VALUES LESS THAN ( TO_DAYS('20180801') ),\n      PARTITION p201808 VALUES LESS THAN ( TO_DAYS('20180901') ),\n      PARTITION p201809 VALUES LESS THAN ( TO_DAYS('20181001') ),\n      PARTITION p201810 VALUES LESS THAN ( TO_DAYS('20181101') ),\n      PARTITION p201811 VALUES LESS THAN ( TO_DAYS('20181201') ),\n      PARTITION p201812 VALUES LESS THAN ( TO_DAYS('20190101') )\n   );\n</code></pre>\n<p>在<code>/var/lib/mysql/data/</code>可以找到对应的数据文件，每个分区表都有一个使用#分隔命名的表文件：</p>\n<pre><code class=\"language-sql\">   -rw-r----- 1 MySQL MySQL    65 Mar 14 21:47 db.opt\n   -rw-r----- 1 MySQL MySQL  8598 Mar 14 21:50 test_range_partition.frm\n   -rw-r----- 1 MySQL MySQL 98304 Mar 14 21:50 test_range_partition#P#p201801.ibd\n   -rw-r----- 1 MySQL MySQL 98304 Mar 14 21:50 test_range_partition#P#p201802.ibd\n   -rw-r----- 1 MySQL MySQL 98304 Mar 14 21:50 test_range_partition#P#p201803.ibd\n...\n</code></pre>\n<p><strong>list分区</strong></p>\n<p>list分区和range分区相似，主要区别在于list是枚举值列表的集合，range是连续的区间值的集合。对于list分区，分区字段必须是已知的，如果插入的字段不在分区时的枚举值中，将无法插入。</p>\n<pre><code class=\"language-sql\">create table test_list_partiotion\n   (\n       id int auto_increment,\n       data_type tinyint,\n       primary key(id,data_type)\n   )partition by list(data_type)\n   (\n       partition p0 values in (0,1,2,3,4,5,6),\n       partition p1 values in (7,8,9,10,11,12),\n       partition p2 values in (13,14,15,16,17)\n   );\n</code></pre>\n<p><strong>hash分区</strong></p>\n<p>可以将数据均匀地分布到预先定义的分区中。</p>\n<pre><code class=\"language-sql\">create table test_hash_partiotion\n   (\n       id int auto_increment,\n       create_date datetime,\n       primary key(id,create_date)\n   )partition by hash(year(create_date)) partitions 10;\n</code></pre>\n<h2 id=\"20分区的问题\">20、分区的问题</h2>\n<ul>\n<li>\n<p>打开和锁住所有底层表的成本可能很高。当查询访问分区表时，MySQL 需要打开并锁住所有的底层表，这个操作在分区过滤之前发生，所以无法通过分区过滤来降低此开销，会影响到查询速度。可以通过批量操作来降低此类开销，比如批量插入、<code>LOAD DATA INFILE</code>和一次删除多行数据。</p>\n</li>\n<li>\n<p>维护分区的成本可能很高。例如重组分区，会先创建一个临时分区，然后将数据复制到其中，最后再删除原分区。</p>\n</li>\n<li>\n<p>所有分区必须使用相同的存储引擎。</p>\n</li>\n</ul>\n<h2 id=\"21查询语句执行流程\">21、查询语句执行流程</h2>\n<p>查询语句的执行流程如下：权限校验、查询缓存、分析器、优化器、权限校验、执行器、引擎。</p>\n<p>举个例子，查询语句如下：</p>\n<pre><code class=\"language-sql\">select * from user where id &gt; 1 and name = '大彬';\n</code></pre>\n<ul>\n<li>\n<p>首先检查权限，没有权限则返回错误；</p>\n</li>\n<li>\n<p>MySQL8.0以前会查询缓存，缓存命中则直接返回，没有则执行下一步；</p>\n</li>\n<li>\n<p>词法分析和语法分析。提取表名、查询条件，检查语法是否有错误；</p>\n</li>\n<li>\n<p>两种执行方案，先查 <code>id &gt; 1</code> 还是 <code>name = '大彬'</code>，优化器根据自己的优化算法选择执行效率最好的方案；</p>\n</li>\n<li>\n<p>校验权限，有权限就调用数据库引擎接口，返回引擎的执行结果。</p>\n</li>\n</ul>\n<h2 id=\"22更新语句执行过程\">22、更新语句执行过程</h2>\n<p>更新语句执行流程如下：分析器、权限校验、执行器、引擎、<code>redo log</code>（<code>prepare</code>状态）、<code>binlog</code>、<code>redo log</code>（<code>commit</code>状态）</p>\n<p>举个例子，更新语句如下：</p>\n<pre><code class=\"language-sql\">update user set name = '大彬' where id = 1;\n</code></pre>\n<ol>\n<li>先查询到 id 为1的记录，有缓存会使用缓存。</li>\n<li>拿到查询结果，将 name 更新为大彬，然后调用引擎接口，写入更新数据，innodb 引擎将数据保存在内存中，同时记录<code>redo log</code>，此时<code>redo log</code>进入 <code>prepare</code>状态。</li>\n<li>执行器收到通知后记录<code>binlog</code>，然后调用引擎接口，提交<code>redo log</code>为<code>commit</code>状态。</li>\n<li>更新完成。</li>\n</ol>\n<p>为什么记录完<code>redo log</code>，不直接提交，而是先进入<code>prepare</code>状态？</p>\n<p>假设先写<code>redo log</code>直接提交，然后写<code>binlog</code>，写完<code>redo log</code>后，机器挂了，<code>binlog</code>日志没有被写入，那么机器重启后，这台机器会通过<code>redo log</code>恢复数据，但是这个时候<code>binlog</code>并没有记录该数据，后续进行机器备份的时候，就会丢失这一条数据，同时主从同步也会丢失这一条数据。</p>\n<h2 id=\"23exist和in的区别\">23、exist和in的区别</h2>\n<blockquote>\n<pre><code class=\"language-sql\">SELECT * FROM A WHERE cc IN (SELECT cc FROM B)；\n</code></pre>\n<pre><code class=\"language-sql\">SELECT * FROM A WHERE EXISTS (SELECT cc FROM B WHERE B.cc = A.cc)；\n</code></pre>\n<p><code>IN</code>是在A中选择一条记录，然后在B中查找该记录是否存在。<code>EXISTS</code>是在B中选择一条符合条件的记录，然后在A中查找该记录是否存在。</p>\n</blockquote>\n<ul>\n<li><strong>In</strong>：确定给定的值是否与子查询或列表中的值相匹配。in在查询的时候，首先查询子查询的表，然后将内表和外表做一个笛卡尔积，然后按照条件进行筛选。所以相对内表比较小的时候，in的速度较快。</li>\n<li><strong>exists</strong>：指定一个子查询，检测行的存在。循环遍历外表，然后看外表中的记录有没有和内表的数据一样的。匹配上就将结果放入结果集中。</li>\n</ul>\n<p><strong>子查询的表比较大的时候</strong>，使用<code>exists</code>可以有效减少总的循环次数来提升速度；<strong>当外查询的表比较大的时候</strong>，使用<code>in</code>可以有效减少对外查询表循环遍历来提升速度。</p>\n<h2 id=\"24mysql中int10和char10的区别\">24、MySQL中int(10)和char(10)的区别</h2>\n<p>int(10)中的10表示的是<strong>显示数据</strong>的长度，而char(10)表示的是<strong>存储数据</strong>的长度。</p>\n<h2 id=\"25truncatedelete与drop区别\">25、truncate、delete与drop区别</h2>\n<p><strong>相同点：</strong></p>\n<ol>\n<li><code>truncate</code>和不带<code>where</code>子句的<code>delete</code>、以及<code>drop</code>都会删除表内的数据。</li>\n<li><strong><code>drop</code>、<code>truncate</code>都是<code>DDL</code>语句（数据定义语言）</strong>，执行后会自动提交。</li>\n</ol>\n<p><strong>不同点：</strong></p>\n<ol>\n<li>truncate 和 delete 只删除数据不删除表的结构；<strong>drop 语句将删除表的结构、被依赖的约束、触发器、索引</strong>；</li>\n<li>一般来说，<strong>执行速度: drop &gt; truncate &gt; delete</strong>。</li>\n</ol>\n<h2 id=\"26having和where区别\">26、having和where区别？</h2>\n<ul>\n<li>二者作用的对象不同，<code>where</code>子句作用于表和视图，<code>having</code>作用于组。</li>\n<li><code>where</code>在数据分组前进行过滤，<code>having</code>在数据分组后进行过滤。</li>\n</ul>\n<h2 id=\"27什么是mysql主从同步\">27、什么是MySQL主从同步？</h2>\n<p>主从同步使得数据可以从一个数据库服务器复制到其他服务器上，在复制数据时，一个服务器充当主服务器（<code>master</code>），其余的服务器充当从服务器（<code>slave</code>）。</p>\n<p>因为复制是异步进行的，所以从服务器不需要一直连接着主服务器，从服务器甚至可以通过拨号断断续续地连接主服务器。通过配置文件，可以指定复制所有的数据库，某个数据库，甚至是某个数据库上的某个表。</p>\n<hr />\n<p><em><strong>MySQL 的主从复制</strong></em>依赖于 binlog ，也就是记录 MySQL 上的所有变化并以二进制形式保存在磁盘上。复制的过程就是将 binlog 中的数据从主库传输到从库上。</p>\n<p>这个过程一般是异步的，也就是主库上执行事务操作的线程不会等待复制 binlog 的线程同步完成。\n<img src=\"http://110.41.141.141:9000/weblog/weblog/69437c3c7def4cc1bec0f1de78dc13d3.png\">\n</p>\n<p>MySQL 集群的主从复制过程梳理成 3 个阶段：</p>\n<ul>\n<li><strong>写入 Binlog</strong>：主库写 binlog 日志，提交事务，并更新本地存储数据。</li>\n<li><strong>同步 Binlog</strong>：把 binlog 复制到所有从库上，每个从库把 binlog 写到暂存日志中。</li>\n<li><strong>回放 Binlog</strong>：回放 binlog，并更新存储引擎中的数据。</li>\n</ul>\n<p>具体详细过程如下：</p>\n<ul>\n<li>MySQL 主库在收到客户端提交事务的请求之后，会先写入 binlog，再提交事务，更新存储引擎中的数据，事务提交完成后，返回给客户端“操作成功”的响应。</li>\n<li>从库会创建一个专门的 I/O 线程，连接主库的 log dump 线程，来接收主库的 binlog 日志，再把 binlog 信息写入 relay log 的中继日志里，再返回给主库“复制成功”的响应。</li>\n<li>从库会创建一个用于回放 binlog 的线程，去读 relay log 中继日志，然后回放 binlog 更新存储引擎中的数据，最终实现主从的数据一致性。</li>\n</ul>\n<h2 id=\"28为什么要做主从同步\">28、为什么要做主从同步？</h2>\n<ol>\n<li><strong>读写分离</strong>，使数据库能支撑更大的并发。</li>\n<li><strong>在主服务器上生成实时数据，而在从服务器上分析这些数据</strong>，从而提高主服务器的性能。</li>\n<li><strong>数据备份</strong>，保证数据的安全。</li>\n</ol>\n<h2 id=\"29乐观锁和悲观锁是什么\">29、乐观锁和悲观锁是什么</h2>\n<p>数据库中的并发控制是确保在多个事务同时存取数据库中同一数据时不破坏事务的隔离性和一致性以及数据库的一致性。乐观锁和悲观锁是并发控制主要采用的技术手段。</p>\n<ul>\n<li><strong>悲观锁</strong>：假定会发生并发冲突，会对操作的数据进行加锁，直到提交事务，才会释放锁，其他事务才能进行修改。实现方式：使用数据库中的锁机制。</li>\n<li><strong>乐观锁</strong>：假设不会发生并发冲突，只在提交操作时检查数据是否被修改过。给表增加<code>version</code>字段，在修改提交之前检查<code>version</code>与原来取到的<code>version</code>值是否相等，若相等，表示数据没有被修改，可以更新，否则，数据为脏数据，不能更新。实现方式：乐观锁一般使用版本号机制或<code>CAS</code>算法实现。</li>\n</ul>\n<h2 id=\"30用过processlist吗\">30、用过processlist吗</h2>\n<p><code>show processlist</code> 或 <code>show full processlist</code> 可以查看当前 MySQL 是否有压力，正在运行的<code>SQL</code>，有没有慢<code>SQL</code>正在执行。返回参数如下：</p>\n<ol>\n<li>\n<p><strong>id</strong>：线程ID，可以用<code>kill id</code>杀死某个线程</p>\n</li>\n<li>\n<p><strong>db</strong>：数据库名称</p>\n</li>\n<li>\n<p><strong>user</strong>：数据库用户</p>\n</li>\n<li>\n<p><strong>host</strong>：数据库实例的IP</p>\n</li>\n<li>\n<p><strong>command</strong>：当前执行的命令，比如<code>Sleep</code>，<code>Query</code>，<code>Connect </code>等</p>\n</li>\n<li>\n<p><strong>time</strong>：消耗时间，单位秒</p>\n</li>\n<li>\n<p>state</p>\n<p>：执行状态，主要有以下状态：</p>\n<ul>\n<li>Sleep，线程正在等待客户端发送新的请求</li>\n<li>Locked，线程正在等待锁</li>\n<li>Sending data，正在处理<code>SELECT</code>查询的记录，同时把结果发送给客户端</li>\n<li>Kill，正在执行<code>kill</code>语句，杀死指定线程</li>\n<li>Connect，一个从节点连上了主节点</li>\n<li>Quit，线程正在退出</li>\n<li>Sorting for group，正在为<code>GROUP BY</code>做排序</li>\n<li>Sorting for order，正在为<code>ORDER BY</code>做排序</li>\n</ul>\n</li>\n<li>\n<p><strong>info</strong>：正在执行的<code>SQL</code>语句</p>\n</li>\n</ol>\n<h2 id=\"31mysql查询-limit-100010-和limit-10-速度一样快吗\">31、MySQL查询 limit 1000,10 和limit 10 速度一样快吗</h2>\n<p>两种查询方式。对应 <code>limit offset, size</code> 和 <code>limit size</code> 两种方式。</p>\n<p>而其实 <code>limit size</code> ，相当于 <code>limit 0, size</code>。也就是从0开始取size条数据。</p>\n<p>也就是说，两种方式的<strong>区别在于offset是否为0。</strong></p>\n<p>先来看下limit sql的内部执行逻辑。</p>\n<p>MySQL内部分为<strong>server层</strong>和<strong>存储引擎层</strong>。一般情况下存储引擎都用innodb。</p>\n<p>server层有很多模块，其中需要关注的是<strong>执行器</strong>是用于跟存储引擎打交道的组件。</p>\n<p>执行器可以通过调用存储引擎提供的接口，将一行行数据取出，当这些数据完全符合要求（比如满足其他where条件），则会放到<strong>结果集</strong>中，最后返回给调用mysql的<strong>客户端</strong>。</p>\n<p>以主键索引的limit执行过程为例：</p>\n<p>执行<code>select * from xxx order by id limit 0, 10;</code>，select后面带的是<strong>星号</strong>，也就是要求获得行数据的<strong>所有字段信息。</strong></p>\n<p>server层会调用innodb的接口，在innodb里的主键索引中获取到第0到10条<strong>完整行数据</strong>，依次返回给server层，并放到server层的结果集中，返回给客户端。</p>\n<p>把offset搞大点，比如执行的是：<code>select * from xxx order by id limit 500000, 10;</code></p>\n<p>server层会调用innodb的接口，由于这次的offset=500000，会在innodb里的主键索引中获取到第0到（500000 + 10）条<strong>完整行数据</strong>，<strong>返回给server层之后根据offset的值挨个抛弃，最后只留下最后面的size条</strong>，也就是10条数据，放到server层的结果集中，返回给客户端。</p>\n<p>可以看出，当offset非0时，server层会从引擎层获取到<strong>很多无用的数据</strong>，而获取的这些无用数据都是要耗时的。</p>\n<p>因此，mysql查询中 limit 1000,10 会比 limit 10 更慢。原因是 limit 1000,10 会取出1000+10条数据，并抛弃前1000条，这部分耗时更大。</p>\n<h2 id=\"32深分页怎么优化\">32、深分页怎么优化</h2>\n<p>还是以上面的SQL为空：<code>select * from xxx order by id limit 500000, 10;</code></p>\n<p><strong>方法一</strong>：</p>\n<p>从上面的分析可以看出，当offset非常大时，server层会从引擎层获取到很多无用的数据，而当select后面是*号时，就需要拷贝完整的行信息，<strong>拷贝完整数据</strong>相比<strong>只拷贝行数据里的其中一两个列字段</strong>更耗费时间。</p>\n<p>因为前面的offset条数据最后都是不要的，没有必要拷贝完整字段，所以可以将sql语句修改成：</p>\n<pre><code class=\"language-sql\">select * from xxx  where id &gt;=(select id from xxx order by id limit 500000, 1) order by id limit 10;\n</code></pre>\n<p>先执行子查询 <code>select id from xxx by id limit 500000, 1</code>, 这个操作，其实也是将在innodb中的主键索引中获取到<code>500000+1</code>条数据，然后server层会抛弃前500000条，只保留最后一条数据的id。</p>\n<p>但不同的地方在于，在返回server层的过程中，只会拷贝数据行内的id这一列，而不会拷贝数据行的所有列，当数据量较大时，这部分的耗时还是比较明显的。</p>\n<p>在拿到了上面的id之后，假设这个id正好等于500000，那sql就变成了</p>\n<pre><code class=\"language-sql\">select * from xxx  where id &gt;=500000 order by id limit 10;\n</code></pre>\n<p>这样innodb再走一次<strong>主键索引</strong>，通过B+树快速定位到id=500000的行数据，时间复杂度是lg(n)，然后向后取10条数据。</p>\n<p><strong>方法二：</strong></p>\n<p>将所有的数据<strong>根据id主键进行排序</strong>，然后分批次取，将当前批次的最大id作为下次筛选的条件进行查询。</p>\n<pre><code class=\"language-sql\">select * from xxx where id &gt; start_id order by id limit 10;\n</code></pre>\n<p>通过主键索引，每次定位到start_id的位置，然后往后遍历10个数据，这样不管数据多大，查询性能都较为稳定。</p>\n<ul>\n<li>\n<p>子查询+索引</p>\n<blockquote>\n<p>思路：通过将 select * 转变为 select id，把符合条件的 id 筛选出来后，最后通过嵌套查询的方式按顺序取出 id 对应的行。</p>\n<pre><code class=\"language-sql\">-- 优化前\nselect *\nfrom people\norder by create_time desc\nlimit 5000000, 10;\n\n-- 优化后\nselect a.*\nfrom people a\ninner join(\n select id\n from people\n order by create_time desc\n limit 5000000, 10\n) b ON a.id = b.id;\n</code></pre>\n</blockquote>\n</li>\n<li>\n<p>联合索引</p>\n<blockquote>\n<p>刚才我们优化后的 SQL 语句如下，是没有 where 条件的。</p>\n<pre><code class=\"language-sql\">select a.*\nfrom people a\ninner join(\n select id\n from people\n order by create_time desc\n limit 5000010, 10\n) b ON a.id = b.id;\n</code></pre>\n</blockquote>\n<blockquote>\n<p>为了更接近现实场景，假设我们要把 status = 1 的人筛出来，SQL 就要这么写：</p>\n<pre><code class=\"language-sql\">select a.*\nfrom people a\ninner join(\n select id\n from people\n where status=1\n order by create_time desc\n limit 5000010, 10\n) b ON a.id = b.id;\n</code></pre>\n</blockquote>\n<blockquote>\n<p>为了加速这个查询，我们可以创建一个 create_time 和 status 的联合索引，对应的 SQL 语句是：</p>\n<pre><code class=\"language-sql\">alter table people add index create_time_status(create_time, status);\n</code></pre>\n</blockquote>\n</li>\n<li>\n<p>where id &gt; start_id</p>\n<blockquote>\n<p>将上一页的查询结果中的最大 id 作为下一页查询的 where 条件，这样可以大幅减少扫描行数，提高查询性能。</p>\n<pre><code class=\"language-sql\">select a.*\nfrom people a\ninner join(\n select id\n from people\n where status=1\n       and id &gt; ${id}\n order by create_time desc\n limit 10\n) b ON a.id = b.id;\n</code></pre>\n</blockquote>\n</li>\n</ul>\n<h2 id=\"33高度为3的b树可以存放多少数据\">33、高度为3的B+树，可以存放多少数据</h2>\n<p>InnoDB存储引擎有自己的最小储存单元——页（Page）。</p>\n<p>查询InnoDB页大小的命令如下：</p>\n<pre><code>mysql&gt; show global status like 'innodb_page_size';\n+------------------+-------+\n| Variable_name    | Value |\n+------------------+-------+\n| Innodb_page_size | 16384 |\n+------------------+-------+\n</code></pre>\n<p>可以看出 <strong>innodb 默认的一页大小为 16384B = 16384/1024 = 16kb</strong>。</p>\n<p>在MySQL中，B+树一个节点的大小设为一页或页的倍数最为合适。因为如果一个节点的大小 &lt; 1页，那么读取这个节点的时候其实读取的还是一页，这样就造成了资源的浪费。</p>\n<p>B+树中<strong>非叶子节点存的是key + 指针</strong>；<strong>叶子节点存的是数据行</strong>。</p>\n<p>对于叶子节点，如果一行数据大小为1k，那么一页就能存16条数据。</p>\n<p>对于非叶子节点，如果key使用的是bigint，则为8字节，指针在MySQL中为6字节，一共是14字节，则16k能存放 16 * 1024 / 14 = 1170 个索引指针。</p>\n<p>于是可以算出，对于一颗高度为2的B+树，根节点存储索引指针节点，那么它有1170个叶子节点存储数据，每个叶子节点可以存储16条数据，一共 1170 x 16 = 18720 条数据。而对于高度为3的B+树，就可以存放 1170 x 1170 x 16 = 21902400 条数据（<strong>两千多万条数据</strong>），也就是对于两千多万条的数据，我们只需要<strong>高度为3</strong>的B+树就可以完成，通过主键查询只需要3次IO操作就能查到对应数据。</p>\n<p>所以在 InnoDB 中B+树高度一般为3层时，就能满足千万级的数据存储。</p>\n<h2 id=\"34mysql单表多大进行分库分表\">34、MySQL单表多大进行分库分表</h2>\n<p>目前主流的有两种说法：</p>\n<ol>\n<li>MySQL 单表数据量大于 2000 万行，性能会明显下降，考虑进行分库分表。</li>\n<li>阿里巴巴《Java 开发手册》提出单表行数超过 500 万行或者单表容量超过 2GB，才推荐进行分库分表。</li>\n</ol>\n<p>事实上，这个数值和实际记录的条数无关，而与 MySQL 的配置以及机器的硬件有关。因为MySQL为了提高性能，会将表的索引装载到内存中。在InnoDB buffer size 足够的情况下，其能完成全加载进内存，查询不会有问题。但是，当单表数据库到达某个量级的上限时，导致内存无法存储其索引，使得之后的 SQL 查询会产生磁盘 IO，从而导致性能下降。当然，这个还有具体的表结构的设计有关，最终导致的问题都是内存限制。</p>\n<p>因此，对于分库分表，需要结合实际需求，不宜过度设计，在项目一开始不采用分库与分表设计，而是随着业务的增长，在无法继续优化的情况下，再考虑分库与分表提高系统的性能。对此，阿里巴巴《Java 开发手册》补充到：如果预计三年后的数据量根本达不到这个级别，请不要在创建表时就分库分表。</p>\n<p>至于MySQL单表多大进行分库分表，应当根据机器资源进行评估。</p>\n<h2 id=\"35大表查询慢怎么优化\">35、大表查询慢怎么优化</h2>\n<p>某个表有近千万数据，查询比较慢，如何优化？</p>\n<p>当MySQL单表记录数过大时，数据库的性能会明显下降，一些常见的优化措施如下：</p>\n<ul>\n<li><strong>合理建立索引</strong>。在合适的字段上建立索引，例如在WHERE和ORDER BY命令上涉及的列建立索引，可根据EXPLAIN来查看是否用了索引还是全表扫描</li>\n<li><strong>索引优化，SQL优化</strong>。索引要符合最左匹配原则等，参考：<a href=\"https://topjavaer.cn/database/mysql.html#什么是覆盖索引\" ref=\"nofollow\" target=\"_blank\">https://topjavaer.cn/database/mysql.html#什么是覆盖索引open in new window</a><span><svg xmlns=\"http://www.w3.org/2000/svg\" class=\"inline ml-1\" style=\"color: #aaa;\" aria-hidden=\"true\" focusable=\"false\" x=\"0px\" y=\"0px\" viewBox=\"0 0 100 100\" width=\"15\" height=\"15\" class=\"icon outbound\"><path fill=\"currentColor\" d=\"M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z\"></path> <polygon fill=\"currentColor\" points=\"45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9\"></polygon></svg> <span class=\"sr-only\"></span></span></li>\n<li><strong>建立分区</strong>。对关键字段建立水平分区，比如时间字段，若查询条件往往通过时间范围来进行查询，能提升不少性能</li>\n<li><strong>利用缓存</strong>。利用Redis等缓存热点数据，提高查询效率</li>\n<li><strong>限定数据的范围</strong>。比如：用户在查询历史信息的时候，可以控制在一个月的时间范围内</li>\n<li><strong>读写分离</strong>。经典的数据库拆分方案，主库负责写，从库负责读</li>\n<li>通过<strong>分库分表</strong>的方式进行优化，主要有垂直拆分和水平拆分</li>\n<li><strong>数据异构到es</strong></li>\n<li><strong>冷热数据分离</strong>。几个月之前不常用的数据放到冷库中，最新的数据或比较新的数据放到热库中</li>\n</ul>\n<blockquote>\n<ul>\n<li>\n<p>通过 explain 执行结果，查看 sql 是否走索引，如果不走索引，考虑增加索引。</p>\n</li>\n<li>\n<p>可以通过建立联合索引，实现覆盖索引优化，减少回表，使用联合索引符合最左匹配原则，不然会索引失效</p>\n</li>\n<li>\n<p>避免索引失效，比如不要用左模糊匹配、函数计算、表达式计算等等。</p>\n</li>\n<li>\n<p>联表查询最好要以小表驱动大表，并且被驱动表的字段要有索引，当然最好通过冗余字段的设计，避免联表查询。</p>\n</li>\n<li>\n<p>针对 limit n,y 深分页的查询优化，可以把Limit查询转换成某个位置的查询：select * from tb_sku where id&gt;20000 limit 10，该方案适用于主键自增的表，</p>\n</li>\n<li>\n<p>将字段多的表分解成多个表，有些字段使用频率高，有些低，数据量大时，会由于使用频率低的存在而变慢，可以考虑分开</p>\n</li>\n</ul>\n</blockquote>\n<h2 id=\"36说说count1count和count字段名的区别\">36、说说count(1)、count(*)和count(字段名)的区别</h2>\n<blockquote>\n<ul>\n<li>count(1)、 count(*)、 count(主键字段)在执行的时候，如果表里存在二级索引，优化器就会选择二级索引进行扫描。</li>\n<li>所以，如果要执行 count(1)、 count(*)、 count(主键字段) 时，尽量在数据表上建立二级索引，这样优化器会自动采用 key_len 最小的二级索引进行扫描，相比于扫描主键索引效率会高一些。</li>\n<li>再来，就是不要使用 count(字段) 来统计记录个数，因为它的效率是最差的，会采用全表扫描的方式来统计。如果你非要统计表中该字段不为 NULL 的记录个数，建议给这个字段建立一个二级索引。</li>\n</ul>\n</blockquote>\n<p>嗯，先说说count(1) and count(字段名)的区别。</p>\n<p>两者的主要区别是</p>\n<ol>\n<li>count(1) 会统计表中的所有的记录数，包含字段为null 的记录。</li>\n<li>count(字段名) 会统计该字段在表中出现的次数，忽略字段为null 的情况。即不统计字段为null 的记录。</li>\n</ol>\n<p>接下来看看三者之间的区别。</p>\n<p>执行效果上：</p>\n<ul>\n<li>count(*)包括了所有的列，相当于行数，在统计结果的时候，<strong>不会忽略列值为NULL</strong></li>\n<li>count(1)包括了忽略所有列，用1代表代码行，在统计结果的时候，<strong>不会忽略列值为NULL</strong></li>\n<li>count(字段名)只包括列名那一列，在统计结果的时候，会忽略列值为空（这里的空不是只空字符串或者0，而是表示null）的计数，<strong>即某个字段值为NULL时，不统计</strong>。</li>\n</ul>\n<p>执行效率上：</p>\n<ul>\n<li>列名为主键，count(字段名)会比count(1)快</li>\n<li>列名不为主键，count(1)会比count(列名)快</li>\n<li>如果表多个列并且没有主键，则 count(1) 的执行效率优于 count(*)</li>\n<li>如果有主键，则 select count(主键)的执行效率是最优的</li>\n<li>如果表只有一个字段，则 select count(*)最优。</li>\n</ul>\n<p><strong>COUNT(<code>*</code>)是SQL92定义的标准统计行数的语法，效率高，MySQL对它进行了很多优化，MyISAM中会直接把表的总行数单独记录下来供COUNT(*)查询，而InnoDB则会在扫表的时候选择最小的索引来降低成本</strong>。</p>\n<h2 id=\"37mysql中datetime-和-timestamp有什么区别\">37、MySQL中DATETIME 和 TIMESTAMP有什么区别</h2>\n<p>嗯，<code>TIMESTAMP</code>和<code>DATETIME</code>都可以用来存储时间，它们主要有以下区别：</p>\n<p>1.表示范围</p>\n<ul>\n<li>DATETIME：1000-01-01 00:00:00.000000 到 9999-12-31 23:59:59.999999</li>\n<li>TIMESTAMP：'1970-01-01 00:00:01.000000' UTC 到 '2038-01-09 03:14:07.999999' UTC</li>\n</ul>\n<p><code>TIMESTAMP</code>支持的时间范围比<code>DATATIME</code>要小，容易出现超出的情况。</p>\n<p>2.空间占用</p>\n<ul>\n<li>TIMESTAMP ：占 4 个字节</li>\n<li>DATETIME：在 MySQL 5.6.4 之前，占 8 个字节 ，之后版本，占 5 个字节</li>\n</ul>\n<p>3.存入时间是否会自动转换</p>\n<p><code>TIMESTAMP</code>类型在默认情况下，insert、update 数据时，<code>TIMESTAMP</code>列会自动以当前时间（<code>CURRENT_TIMESTAMP</code>）填充/更新。<code>DATETIME</code>则不会做任何转换，也不会检测时区，你给什么数据，它存什么数据。</p>\n<p>4.<code>TIMESTAMP</code>比较受时区timezone的影响以及MYSQL版本和服务器的SQL MODE的影响。因为<code>TIMESTAMP</code>存的是时间戳，在不同的时区得出的时间不一致。</p>\n<p>5.如果存进NULL，两者实际存储的值不同。</p>\n<ul>\n<li>TIMESTAMP：会自动存储当前时间 now() 。</li>\n<li>DATETIME：不会自动存储当前时间，会直接存入 NULL 值。</li>\n</ul>\n<h2 id=\"38说说为什么不建议用外键\">38、说说为什么不建议用外键</h2>\n<p>外键是一种约束，这个约束的存在，会保证表间数据的关系始终完整。外键的存在，并非全然没有优点。</p>\n<p>外键可以保证数据的完整性和一致性，级联操作方便。而且使用外键可以将数据完整性判断托付给了数据库完成，减少了程序的代码量。</p>\n<p>虽然外键能够保证数据的完整性，但是会给系统带来很多缺陷。</p>\n<p>1、并发问题。在使用外键的情况下，每次修改数据都需要去另外一个表检查数据，需要获取额外的锁。若是在高并发大流量事务场景，使用外键更容易造成死锁。</p>\n<p>2、扩展性问题。比如从<code>MySQL</code>迁移到<code>Oracle</code>，外键依赖于数据库本身的特性，做迁移可能不方便。</p>\n<p>3、不利于分库分表。在水平拆分和分库的情况下，外键是无法生效的。将数据间关系的维护，放入应用程序中，为将来的分库分表省去很多的麻烦。</p>\n<h2 id=\"39使用自增主键有什么好处\">39、使用自增主键有什么好处</h2>\n<p>自增主键可以让主键索引尽量地保持递增顺序插入，避免了页分裂，因此索引更紧凑，在查询的时候，效率也就更高。</p>\n<h2 id=\"40自增主键保存在什么地方\">40、自增主键保存在什么地方</h2>\n<p>不同的引擎对于自增值的保存策略不同：</p>\n<ul>\n<li><strong>MyISAM引擎的自增值保存在数据文件中</strong>。</li>\n<li>在MySQL8.0以前，InnoDB引擎的自增值是存在内存中。MySQL重启之后内存中的这个值就丢失了，每次重启后第一次打开表的时候，会找自增值的最大值max(id)，然后将最大值加1作为这个表的自增值；MySQL8.0版本会将自增值的变更记录在redo log中，重启时依靠redo log恢复。</li>\n</ul>\n<h2 id=\"41自增主键一定是连续的吗\">41、自增主键一定是连续的吗</h2>\n<p>不一定，有几种情况会导致自增主键不连续。</p>\n<p>1、<strong>唯一键冲突导致自增主键不连续</strong>。当我们向一个自增主键的InnoDB表中插入数据的时候，如果违反表中定义的唯一索引的唯一约束，会导致插入数据失败。此时表的自增主键的键值是会向后加1滚动的。下次再次插入数据的时候，就不能再使用上次因插入数据失败而滚动生成的键值了，必须使用新滚动生成的键值。</p>\n<p>2、<strong>事务回滚导致自增主键不连续</strong>。当我们向一个自增主键的InnoDB表中插入数据的时候，如果显式开启了事务，然后因为某种原因最后回滚了事务，此时表的自增值也会发生滚动，而接下里新插入的数据，也将不能使用滚动过的自增值，而是需要重新申请一个新的自增值。</p>\n<p>3、<strong>批量插入导致自增值不连续</strong>。MySQL有一个批量申请自增id的策略：</p>\n<ul>\n<li>语句执行过程中，第一次申请自增id，分配1个自增id</li>\n<li>1个用完以后，第二次申请，会分配2个自增id</li>\n<li>2个用完以后，第三次申请，会分配4个自增id</li>\n<li>依次类推，每次申请都是上一次的两倍（最后一次申请不一定全部使用）</li>\n</ul>\n<p>如果下一个事务再次插入数据的时候，则会基于上一个事务申请后的自增值基础上再申请。此时就出现自增值不连续的情况出现。</p>\n<p>4、<strong>自增步长不是1</strong>，也会导致自增主键不连续。</p>\n<h2 id=\"42innodb的自增值为什么不能回收利用\">42、InnoDB的自增值为什么不能回收利用</h2>\n<p><strong>主要为了提升插入数据的效率和并行度</strong>。</p>\n<p>假设有两个并行执行的事务，在申请自增值的时候，为了避免两个事务申请到相同的自增 id，肯定要加锁，然后顺序申请。</p>\n<p>假设事务 A 申请到了 id=2， 事务 B 申请到 id=3，那么这时候表 t 的自增值是 4，之后继续执行。</p>\n<p>事务 B 正确提交了，但事务 A 出现了唯一键冲突。</p>\n<p>如果允许事务 A 把自增 id 回退，也就是把表 t 的当前自增值改回 2，那么就会出现这样的情况：表里面已经有 id=3 的行，而当前的自增 id 值是 2。</p>\n<p>接下来，继续执行的其他事务就会申请到 id=2，然后再申请到 id=3。这时，就会出现插入语句报错“主键冲突”。</p>\n<p>而为了解决这个主键冲突，有两种方法：</p>\n<ul>\n<li>每次申请 id 之前，先判断表里面是否已经存在这个 id。如果存在，就跳过这个 id。但是，这个方法的成本很高。因为，本来申请 id 是一个很快的操作，现在还要再去主键索引树上判断 id 是否存在。</li>\n<li>把自增 id 的锁范围扩大，必须等到一个事务执行完成并提交，下一个事务才能再申请自增 id。这个方法的问题，就是锁的粒度太大，系统并发能力大大下降。</li>\n</ul>\n<p>可见，这两个方法都会导致性能问题。</p>\n<p>因此，InnoDB 放弃了“允许自增 id 回退”这个设计，语句执行失败也不回退自增 id。</p>\n<h2 id=\"43mysql数据如何同步到redis缓存\">43、MySQL数据如何同步到Redis缓存</h2>\n<p>有两种方案：</p>\n<p>1、通过MySQL自动同步刷新Redis，<strong>MySQL触发器+UDF函数</strong>实现。</p>\n<blockquote>\n<p>UDF函数：常见的函数类型，可以操作单个数据行，且产生一个数据行作为输出，大多数函数为这一类。</p>\n</blockquote>\n<p>过程大致如下：</p>\n<ol>\n<li>在MySQL中对要操作的数据设置触发器Trigger，监听操作</li>\n<li>客户端向MySQL中写入数据时，触发器会被触发，触发之后调用MySQL的UDF函数</li>\n<li>UDF函数可以把数据写入到Redis中，从而达到同步的效果</li>\n</ol>\n<p>2、<strong>解析MySQL的binlog</strong>，实现将数据库中的数据同步到Redis。可以通过canal实现。canal是阿里巴巴旗下的一款开源项目，基于数据库增量日志解析，提供增量数据订阅&amp;消费。</p>\n<p>canal的原理如下：</p>\n<ol>\n<li>canal模拟mysql slave的交互协议，伪装自己为mysql slave，向mysql master发送dump协议</li>\n<li>mysql master收到dump请求，开始推送binary log给canal</li>\n<li>canal解析binary log对象（原始为byte流），将数据同步写入Redis。</li>\n</ol>\n<h2 id=\"44为什么阿里java手册禁止使用存储过程\">44、为什么阿里Java手册禁止使用存储过程</h2>\n<p>先看看什么是存储过程。</p>\n<p>存储过程是在大型数据库系统中，一组为了完成特定功能的SQL 语句集，它存储在数据库中，一次编译后永久有效，用户通过指定存储过程的名字并给出参数（如果该存储过程带有参数）来执行它。</p>\n<p>存储过程主要有以下几个缺点。</p>\n<ol>\n<li><strong>存储过程难以调试</strong>。存储过程的开发一直缺少有效的 IDE 环境。SQL 本身经常很长，调试时要把句子拆开分别独立执行，非常麻烦。</li>\n<li><strong>移植性差</strong>。存储过程的移植困难，一般业务系统总会不可避免地用到数据库独有的特性和语法，更换数据库时这部分代码就需要重写，成本较高。</li>\n<li><strong>管理困难</strong>。存储过程的目录是扁平的，而不是文件系统那样的树形结构，脚本少的时候还好办，一旦多起来，目录就会陷入混乱。</li>\n<li>存储过程是<strong>只优化一次</strong>，有的时候随着数据量的增加或者数据结构的变化，原来存储过程选择的执行计划也许并不是最优的了，所以这个时候需要手动干预或者重新编译了。</li>\n</ol>\n<h2 id=\"45mysql-update-是锁行还是锁表\">45、MySQL update 是锁行还是锁表？</h2>\n<p>首先，InnoDB行锁是通过给索引上的索引项加锁来实现的，只有通过索引条件检索数据，InnoDB才使用行级锁，否则，InnoDB将使用表锁。</p>\n<ol>\n<li>当执行update语句时，where中的过滤条件列，如果用到索引，就是锁行；如果无法用索引，就是锁表。</li>\n<li>如果两个update语句同时执行，第一个先执行触发行锁，但是第二个没有索引触发表锁，因为有个行锁住了，所以还是会等待行锁释放，才能锁表。</li>\n<li>当执行insert或者delete语句时，锁行。</li>\n</ol>\n<h2 id=\"46selectfor-update会锁表还是锁行\">46、select...for update会锁表还是锁行？</h2>\n<p>如果查询条件用了索引/主键，那么<code>select ... for update</code>就会加行锁。</p>\n<p>如果是普通字段(没有索引/主键)，那么<code>select ..... for update</code>就会加表锁。</p>\n<h2 id=\"47mysql的binlog有几种格式分别有什么区别\">47、MySQL的binlog有几种格式？分别有什么区别？</h2>\n<p>有三种格式，<strong>statement，row和mixed</strong>。</p>\n<ul>\n<li>statement：<strong>每一条会修改数据的sql都会记录在binlog中</strong>。不需要记录每一行的变化，减少了binlog日志量，节约了IO，提高性能。由于sql的执行是有上下文的，因此<strong>在保存的时候需要保存相关的信息</strong>，同时还有一些使用了函数之类的语句无法被记录复制。</li>\n<li>row：<strong>不记录sql语句上下文相关信息，仅保存哪条记录被修改</strong>。记录单元为每一行的改动，由于很多操作，会导致大量行的改动(比如alter table)，因此这种模式的文件保存的信息太多，日志量太大。</li>\n<li>mixed：一种折中的方案，普通操作使用statement记录，当无法使用statement的时候使用row。</li>\n</ul>\n<h2 id=\"48阿里手册为什么禁止使用-count列名或-count常量来替代-count\">48、阿里手册为什么禁止使用 count(列名)或 count(常量)来替代 count(*)</h2>\n<p>先看下这几种方式的区别。</p>\n<p>count(主键id)：InnoDB引擎会遍历整张表，把每一行id值都取出来，返给server层。server层拿到id后，判断是不可能为空的，就按行累加，不再对每个值进行NULL判断。</p>\n<p>count(常量)：InnoDB引擎会遍历整张表，但不取值。server层对于返回的每一行，放一个常量进去，判断是不可能为空的，按行累加，不再对每个值进行NULL判断。count(常量)比count(主键id)执行的要快，因为从引擎放回id会涉及解析数据行，以及拷贝字段值的操作。</p>\n<p>count(字段)：全表扫描，分情况讨论。</p>\n<p>1、如果参数字段定义NOT NULL，判断是不可能为空的，按行累加，不再对每个值进行NULL判断。 2、如果参数字段定义允许为NULL，那么执行的时候，判断可能是NULL，还要把值取出来再判断一下，不是NULL才累加。</p>\n<p>count(*)：统计所有的列，相当于行数，统计结果中会包含字段值为null的列；</p>\n<p><strong>COUNT(<code>*</code>)是SQL92定义的标准统计行数的语法，效率高，MySQL对它进行了很多优化，MyISAM中会直接把表的总行数单独记录下来供COUNT(*)查询，而InnoDB则会在扫表的时候选择最小的索引来降低成本</strong>。</p>\n<p>所以，建议使用COUNT(*)查询表的行数！</p>\n<h2 id=\"49存储md5值应该用varchar还是用char\">49、存储MD5值应该用VARCHAR还是用CHAR？</h2>\n<p>首先说说CHAR和VARCHAR的区别：</p>\n<p>1、存储长度：</p>\n<p><strong>CHAR类型的长度是固定的</strong></p>\n<p>当我们当定义CHAR(10)，输入的值是&quot;abc&quot;，但是它占用的空间一样是10个字节，会包含7个空字节。当输入的字符长度超过指定的数时，CHAR会截取超出的字符。而且，当存储为CHAR的时候，MySQL会自动删除输入字符串末尾的空格。</p>\n<p><strong>VARCHAR的长度是可变的</strong></p>\n<p>比如VARCHAR(10)，然后输入abc三个字符，那么实际存储大小为3个字节。</p>\n<p>除此之外，<strong>VARCHAR还会保留1个或2个额外的字节来记录字符串的实际长度</strong>。如果定义的最大长度小于等于255个字节，那么，就会预留1个字节；如果定义的最大长度大于255个字节，那么就会预留2个字节。</p>\n<p>2、存储效率</p>\n<p><strong>CHAR类型每次修改后的数据长度不变，效率更高</strong>。</p>\n<p>VARCHAR每次修改的数据要更新数据长度，效率更低。</p>\n<p>3、存储空间</p>\n<p>CHAR存储空间是<strong>初始的预计长度字符串再加上一个记录字符串长度的字节</strong>，可能会存在多余的空间。</p>\n<p>VARCHAR存储空间的时候是<strong>实际字符串再加上一个记录字符串长度的字节</strong>，占用空间较小。</p>\n<p>根据以上的分析，由于<strong>MD5是一个定长的值，所以MD5值适合使用CHAR存储</strong>。对于固定长度的非常短的列，CHAR比VARCHAR效率也更高。</p>\n","createTime":"2024-05-07 22:20:34","categoryId":36,"categoryName":"MySQL","readNum":85,"tags":[{"id":63,"name":"mysql","articlesTotal":null}],"preArticle":{"articleId":48,"articleTitle":"JVM"},"nextArticle":{"articleId":45,"articleTitle":"Redis"},"totalWords":22922,"readTime":"约 76 分钟","updateTime":"2024-06-03 20:17:54"}} =================================== 
2024-08-04 21:28:32.697 [http-nio-8088-exec-1] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 21:28:32.804 [http-nio-8088-exec-8] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [前台获取博客详情], 耗时: 845ms, 出参: {"success":true,"message":null,"errorCode":null,"data":{"logo":"http://110.41.141.141:9000/weblog/weblog/9853f8be13cb4f7fae00e3f5233dd688.png","name":"WebLog","author":"Jacob","introduction":"求知若饥，虚心若愚","avatar":"http://110.41.141.141:9000/weblog/weblog/cf0958d87787449fb05aae4cc84015c6.jpg","githubHomepage":"https://github.com/jdw-art","csdnHomepage":"https://www.csdn.net/?spm=1010.2135.3001.4476","giteeHomepage":"","zhihuHomepage":"https://www.zhihu.com/people/54-10-50-93"}} =================================== 
2024-08-04 21:28:32.805 [http-nio-8088-exec-8] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 21:28:33.535 [http-nio-8088-exec-3] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 21:28:33.535 [http-nio-8088-exec-9] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 21:28:33.535 [http-nio-8088-exec-7] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 21:28:33.536 [http-nio-8088-exec-3] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 21:28:33.536 [http-nio-8088-exec-9] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 21:28:33.536 [http-nio-8088-exec-2] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 21:28:33.536 [http-nio-8088-exec-7] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 21:28:33.536 [http-nio-8088-exec-2] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 21:28:33.536 [http-nio-8088-exec-2] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [前台获取标签列表], 入参: , 请求类: TagController, 请求方法: findTagList =================================== 
2024-08-04 21:28:33.567 [http-nio-8088-exec-4] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 21:28:33.567 [http-nio-8088-exec-4] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 21:28:33.568 [http-nio-8088-exec-4] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [前台获取博客详情], 入参: , 请求类: BlogSettingsController, 请求方法: findDetail =================================== 
2024-08-04 21:28:33.602 [http-nio-8088-exec-9] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [前台获取统计信息], 入参: , 请求类: StatisticsController, 请求方法: findInfo =================================== 
2024-08-04 21:28:33.610 [http-nio-8088-exec-3] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [获取首页文章分页数据], 入参: {"current":1,"size":10}, 请求类: ArticleController, 请求方法: findArticlePageList =================================== 
2024-08-04 21:28:33.680 [http-nio-8088-exec-2] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [前台获取标签列表], 耗时: 144ms, 出参: {"success":true,"message":null,"errorCode":null,"data":[{"id":47,"name":"java","articlesTotal":14},{"id":48,"name":"脉冲神经网络","articlesTotal":6},{"id":49,"name":"snn","articlesTotal":6},{"id":54,"name":"联想记忆模型","articlesTotal":1},{"id":55,"name":"集合","articlesTotal":1},{"id":56,"name":"并发","articlesTotal":2},{"id":57,"name":"RabbitMQ","articlesTotal":2},{"id":58,"name":"spring","articlesTotal":4},{"id":59,"name":"springboot","articlesTotal":4},{"id":60,"name":"mongodb","articlesTotal":2},{"id":61,"name":"nosql","articlesTotal":1},{"id":62,"name":"redis","articlesTotal":2},{"id":63,"name":"mysql","articlesTotal":1},{"id":64,"name":"jvm","articlesTotal":1},{"id":65,"name":"elasticSearch","articlesTotal":1},{"id":66,"name":"SpringMVC","articlesTotal":1},{"id":67,"name":"计算机网络","articlesTotal":1},{"id":68,"name":"操作系统","articlesTotal":1},{"id":69,"name":"设计模式","articlesTotal":1},{"id":70,"name":"SpringCloud","articlesTotal":2},{"id":71,"name":"微服务","articlesTotal":4},{"id":72,"name":"jmm","articlesTotal":1},{"id":73,"name":"高可用","articlesTotal":3},{"id":74,"name":"系统设计","articlesTotal":3},{"id":76,"name":"海量数据","articlesTotal":1}]} =================================== 
2024-08-04 21:28:33.681 [http-nio-8088-exec-2] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 21:28:33.760 [http-nio-8088-exec-7] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [前台获取分类列表], 入参: {"size":10}, 请求类: CategoryController, 请求方法: findCategoryList =================================== 
2024-08-04 21:28:33.910 [http-nio-8088-exec-4] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [前台获取博客详情], 耗时: 342ms, 出参: {"success":true,"message":null,"errorCode":null,"data":{"logo":"http://110.41.141.141:9000/weblog/weblog/9853f8be13cb4f7fae00e3f5233dd688.png","name":"WebLog","author":"Jacob","introduction":"求知若饥，虚心若愚","avatar":"http://110.41.141.141:9000/weblog/weblog/cf0958d87787449fb05aae4cc84015c6.jpg","githubHomepage":"https://github.com/jdw-art","csdnHomepage":"https://www.csdn.net/?spm=1010.2135.3001.4476","giteeHomepage":"","zhihuHomepage":"https://www.zhihu.com/people/54-10-50-93"}} =================================== 
2024-08-04 21:28:33.912 [http-nio-8088-exec-4] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 21:28:34.053 [http-nio-8088-exec-9] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [前台获取统计信息], 耗时: 451ms, 出参: {"success":true,"message":null,"errorCode":null,"data":{"articleTotalCount":31,"categoryTotalCount":15,"tagTotalCount":25,"pvTotalCount":886}} =================================== 
2024-08-04 21:28:34.054 [http-nio-8088-exec-9] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 21:28:34.162 [http-nio-8088-exec-7] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [前台获取分类列表], 耗时: 402ms, 出参: {"success":true,"message":null,"errorCode":null,"data":[{"id":29,"name":"Java","articlesTotal":7},{"id":31,"name":"SNN","articlesTotal":6},{"id":30,"name":"工具","articlesTotal":3},{"id":33,"name":"SpringBoot","articlesTotal":3},{"id":34,"name":"Redis","articlesTotal":2},{"id":41,"name":"高可用","articlesTotal":2},{"id":32,"name":"Spring","articlesTotal":1},{"id":35,"name":"RabbitMQ","articlesTotal":1},{"id":36,"name":"MySQL","articlesTotal":1},{"id":37,"name":"MongoDB","articlesTotal":1}]} =================================== 
2024-08-04 21:28:34.163 [http-nio-8088-exec-7] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 21:28:35.339 [http-nio-8088-exec-3] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [获取首页文章分页数据], 耗时: 1569ms, 出参: {"success":true,"message":null,"errorCode":null,"data":[{"id":44,"cover":"http://110.41.141.141:9000/weblog/weblog/21bfecab22f14cef9536fc55c5ac3d11.png","title":"WebLog","createDate":"2024-04-27","summary":"WebLog项目摘要","category":{"id":33,"name":"SpringBoot","articlesTotal":null},"tags":[{"id":47,"name":"java","articlesTotal":null},{"id":58,"name":"spring","articlesTotal":null},{"id":59,"name":"springboot","articlesTotal":null}],"isTop":true},{"id":43,"cover":"http://110.41.141.141:9000/weblog/weblog/05192a6167704ae8851dbcab6d4ebb7a.jpg","title":"E-Commerce","createDate":"2024-04-27","summary":"E-Commerce项目摘要","category":{"id":33,"name":"SpringBoot","articlesTotal":null},"tags":[{"id":47,"name":"java","articlesTotal":null},{"id":58,"name":"spring","articlesTotal":null},{"id":59,"name":"springboot","articlesTotal":null},{"id":57,"name":"RabbitMQ","articlesTotal":null},{"id":60,"name":"mongodb","articlesTotal":null}],"isTop":true},{"id":62,"cover":"http://110.41.141.141:9000/weblog/8287b11a871e47afb5687f844bf437f5.jfif","title":"Concurrent Programming","createDate":"2024-07-24","summary":"Java并发编程","category":{"id":29,"name":"Java","articlesTotal":null},"tags":[{"id":56,"name":"并发","articlesTotal":null},{"id":47,"name":"java","articlesTotal":null}],"isTop":false},{"id":61,"cover":"http://110.41.141.141:9000/weblog/7559e152bde74a5492be7051e53e02ff.webp","title":"分布式锁","createDate":"2024-07-23","summary":"分布式锁及其实现方式","category":{"id":41,"name":"高可用","articlesTotal":null},"tags":[{"id":74,"name":"系统设计","articlesTotal":null},{"id":73,"name":"高可用","articlesTotal":null},{"id":71,"name":"微服务","articlesTotal":null}],"isTop":false},{"id":60,"cover":"http://110.41.141.141:9000/weblog/e04ed0de8db24bbeb8025ad7fb8fc8f3.png","title":"海量数据","createDate":"2024-07-23","summary":"海量数据场景","category":{"id":30,"name":"工具","articlesTotal":null},"tags":[{"id":76,"name":"海量数据","articlesTotal":null}],"isTop":false},{"id":59,"cover":"http://110.41.141.141:9000/weblog/245940328a974c3aa35a4fab259c2f93.jpg","title":"负载均衡","createDate":"2024-07-19","summary":"负载均衡","category":{"id":41,"name":"高可用","articlesTotal":null},"tags":[{"id":73,"name":"高可用","articlesTotal":null},{"id":74,"name":"系统设计","articlesTotal":null},{"id":71,"name":"微服务","articlesTotal":null}],"isTop":false},{"id":58,"cover":"http://110.41.141.141:9000/weblog/90cd980f59014947af531833d3d32666.jpg","title":"JMM","createDate":"2024-07-19","summary":"Java内存模型","category":{"id":29,"name":"Java","articlesTotal":null},"tags":[{"id":72,"name":"jmm","articlesTotal":null}],"isTop":false},{"id":57,"cover":"http://110.41.141.141:9000/weblog/14b767a52a0548a29129a64ec9c31b92.png","title":"微服务","createDate":"2024-07-19","summary":"微服务","category":{"id":43,"name":"系统设计","articlesTotal":null},"tags":[{"id":71,"name":"微服务","articlesTotal":null},{"id":70,"name":"SpringCloud","articlesTotal":null},{"id":74,"name":"系统设计","articlesTotal":null},{"id":73,"name":"高可用","articlesTotal":null}],"isTop":false},{"id":56,"cover":"http://110.41.141.141:9000/weblog/065adf0970a84db291e85a56d9927905.jpg","title":"SpringCloud","createDate":"2024-07-19","summary":"Spring Cloud 八股","category":{"id":40,"name":"SpringCloud","articlesTotal":null},"tags":[{"id":70,"name":"SpringCloud","articlesTotal":null},{"id":71,"name":"微服务","articlesTotal":null}],"isTop":false},{"id":55,"cover":"http://110.41.141.141:9000/weblog/031b75962ad54f2f8e6cde5799dbd0f5.jpg","title":"DesignPattern","createDate":"2024-07-19","summary":"设计模式","category":{"id":29,"name":"Java","articlesTotal":null},"tags":[{"id":69,"name":"设计模式","articlesTotal":null}],"isTop":false}],"total":31,"size":10,"current":1,"pages":4} =================================== 
2024-08-04 21:28:35.374 [http-nio-8088-exec-3] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 21:28:36.826 [http-nio-8088-exec-6] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 21:28:36.826 [http-nio-8088-exec-6] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 21:28:36.828 [http-nio-8088-exec-6] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [前台获取博客详情], 入参: , 请求类: BlogSettingsController, 请求方法: findDetail =================================== 
2024-08-04 21:28:36.830 [http-nio-8088-exec-10] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 21:28:36.831 [http-nio-8088-exec-10] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 21:28:36.831 [http-nio-8088-exec-10] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [获取知识库数据], 入参: , 请求类: WikiController, 请求方法: findWikiList =================================== 
2024-08-04 21:28:36.832 [http-nio-8088-exec-5] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 21:28:36.833 [http-nio-8088-exec-1] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 21:28:36.833 [http-nio-8088-exec-5] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 21:28:36.833 [http-nio-8088-exec-1] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 21:28:36.833 [http-nio-8088-exec-8] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 21:28:36.833 [http-nio-8088-exec-8] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 21:28:36.833 [http-nio-8088-exec-1] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [前台获取统计信息], 入参: , 请求类: StatisticsController, 请求方法: findInfo =================================== 
2024-08-04 21:28:36.833 [http-nio-8088-exec-8] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [前台获取标签列表], 入参: , 请求类: TagController, 请求方法: findTagList =================================== 
2024-08-04 21:28:36.847 [http-nio-8088-exec-8] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [前台获取标签列表], 耗时: 14ms, 出参: {"success":true,"message":null,"errorCode":null,"data":[{"id":47,"name":"java","articlesTotal":14},{"id":48,"name":"脉冲神经网络","articlesTotal":6},{"id":49,"name":"snn","articlesTotal":6},{"id":54,"name":"联想记忆模型","articlesTotal":1},{"id":55,"name":"集合","articlesTotal":1},{"id":56,"name":"并发","articlesTotal":2},{"id":57,"name":"RabbitMQ","articlesTotal":2},{"id":58,"name":"spring","articlesTotal":4},{"id":59,"name":"springboot","articlesTotal":4},{"id":60,"name":"mongodb","articlesTotal":2},{"id":61,"name":"nosql","articlesTotal":1},{"id":62,"name":"redis","articlesTotal":2},{"id":63,"name":"mysql","articlesTotal":1},{"id":64,"name":"jvm","articlesTotal":1},{"id":65,"name":"elasticSearch","articlesTotal":1},{"id":66,"name":"SpringMVC","articlesTotal":1},{"id":67,"name":"计算机网络","articlesTotal":1},{"id":68,"name":"操作系统","articlesTotal":1},{"id":69,"name":"设计模式","articlesTotal":1},{"id":70,"name":"SpringCloud","articlesTotal":2},{"id":71,"name":"微服务","articlesTotal":4},{"id":72,"name":"jmm","articlesTotal":1},{"id":73,"name":"高可用","articlesTotal":3},{"id":74,"name":"系统设计","articlesTotal":3},{"id":76,"name":"海量数据","articlesTotal":1}]} =================================== 
2024-08-04 21:28:36.848 [http-nio-8088-exec-8] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 21:28:36.865 [http-nio-8088-exec-5] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [前台获取分类列表], 入参: {"size":10}, 请求类: CategoryController, 请求方法: findCategoryList =================================== 
2024-08-04 21:28:36.832 [http-nio-8088-exec-6] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [前台获取博客详情], 耗时: 4ms, 出参: {"success":true,"message":null,"errorCode":null,"data":{"logo":"http://110.41.141.141:9000/weblog/weblog/9853f8be13cb4f7fae00e3f5233dd688.png","name":"WebLog","author":"Jacob","introduction":"求知若饥，虚心若愚","avatar":"http://110.41.141.141:9000/weblog/weblog/cf0958d87787449fb05aae4cc84015c6.jpg","githubHomepage":"https://github.com/jdw-art","csdnHomepage":"https://www.csdn.net/?spm=1010.2135.3001.4476","giteeHomepage":"","zhihuHomepage":"https://www.zhihu.com/people/54-10-50-93"}} =================================== 
2024-08-04 21:28:36.868 [http-nio-8088-exec-6] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 21:28:36.902 [http-nio-8088-exec-5] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [前台获取分类列表], 耗时: 37ms, 出参: {"success":true,"message":null,"errorCode":null,"data":[{"id":29,"name":"Java","articlesTotal":7},{"id":31,"name":"SNN","articlesTotal":6},{"id":30,"name":"工具","articlesTotal":3},{"id":33,"name":"SpringBoot","articlesTotal":3},{"id":34,"name":"Redis","articlesTotal":2},{"id":41,"name":"高可用","articlesTotal":2},{"id":32,"name":"Spring","articlesTotal":1},{"id":35,"name":"RabbitMQ","articlesTotal":1},{"id":36,"name":"MySQL","articlesTotal":1},{"id":37,"name":"MongoDB","articlesTotal":1}]} =================================== 
2024-08-04 21:28:36.903 [http-nio-8088-exec-5] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 21:28:36.937 [http-nio-8088-exec-1] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [前台获取统计信息], 耗时: 104ms, 出参: {"success":true,"message":null,"errorCode":null,"data":{"articleTotalCount":31,"categoryTotalCount":15,"tagTotalCount":25,"pvTotalCount":886}} =================================== 
2024-08-04 21:28:36.938 [http-nio-8088-exec-1] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 21:28:37.200 [http-nio-8088-exec-10] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [获取知识库数据], 耗时: 369ms, 出参: {"success":true,"message":null,"errorCode":null,"data":[{"id":6,"cover":"http://110.41.141.141:9000/weblog/fea0e0402a73479ab1603148b4187027.jpg","title":"中间件","summary":"后端开发中间件","isTop":false,"firstArticleId":39},{"id":5,"cover":"http://110.41.141.141:9000/weblog/weblog/ea44696cf3dd408584d04a46452bfbd6.jpg","title":"计算机基础","summary":"计算机基础知识","isTop":false,"firstArticleId":54},{"id":4,"cover":"http://110.41.141.141:9000/weblog/weblog/80061883b8514ae3a557fe8d7db8f6b2.jpg","title":"项目","summary":"项目整理","isTop":false,"firstArticleId":43},{"id":3,"cover":"http://110.41.141.141:9000/weblog/weblog/fb84627b093e4fa98ef9bdf5fbfe8cbc.jpg","title":"系统设计","summary":"系统设计","isTop":false,"firstArticleId":40},{"id":2,"cover":"http://110.41.141.141:9000/weblog/weblog/ca753e837e2344319d9f58a92d25403f.jpg","title":"Java","summary":"Java后端开发","isTop":false,"firstArticleId":33},{"id":1,"cover":"http://110.41.141.141:9000/weblog/weblog/2b484d7d659f4a3aaa9ba74271e55fda.jpg","title":"数据库","summary":"数据库，包括关系型数据库、非关系型数据库以及缓存等","isTop":false,"firstArticleId":47}]} =================================== 
2024-08-04 21:28:37.201 [http-nio-8088-exec-10] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 21:28:39.076 [http-nio-8088-exec-2] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 21:28:39.076 [http-nio-8088-exec-2] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 21:28:39.076 [http-nio-8088-exec-2] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [前台获取博客详情], 入参: , 请求类: BlogSettingsController, 请求方法: findDetail =================================== 
2024-08-04 21:28:39.080 [http-nio-8088-exec-2] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [前台获取博客详情], 耗时: 4ms, 出参: {"success":true,"message":null,"errorCode":null,"data":{"logo":"http://110.41.141.141:9000/weblog/weblog/9853f8be13cb4f7fae00e3f5233dd688.png","name":"WebLog","author":"Jacob","introduction":"求知若饥，虚心若愚","avatar":"http://110.41.141.141:9000/weblog/weblog/cf0958d87787449fb05aae4cc84015c6.jpg","githubHomepage":"https://github.com/jdw-art","csdnHomepage":"https://www.csdn.net/?spm=1010.2135.3001.4476","giteeHomepage":"","zhihuHomepage":"https://www.zhihu.com/people/54-10-50-93"}} =================================== 
2024-08-04 21:28:39.081 [http-nio-8088-exec-4] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 21:28:39.081 [http-nio-8088-exec-4] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 21:28:39.082 [http-nio-8088-exec-2] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 21:28:39.083 [http-nio-8088-exec-4] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [获取文章详情], 入参: {"articleId":47}, 请求类: ArticleController, 请求方法: findArticleDetail =================================== 
2024-08-04 21:28:39.177 [http-nio-8088-exec-7] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 21:28:39.177 [http-nio-8088-exec-7] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 21:28:39.184 [http-nio-8088-exec-9] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ## HeaderUserId2ContextFilter，用户 ID：1
2024-08-04 21:28:39.185 [http-nio-8088-exec-9] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 设置 userId 到 ThreadLocal 中， 用户 ID: 1
2024-08-04 21:28:39.178 [http-nio-8088-exec-7] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [获取知识库目录数据], 入参: {"id":1}, 请求类: WikiController, 请求方法: findWikiCatalogList =================================== 
2024-08-04 21:28:39.231 [http-nio-8088-exec-9] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求开始: [获取知识库文章上下页], 入参: {"id":1,"articleId":47}, 请求类: WikiController, 请求方法: findArticlePreNext =================================== 
2024-08-04 21:28:39.300 [http-nio-8088-exec-7] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [获取知识库目录数据], 耗时: 122ms, 出参: {"success":true,"message":null,"errorCode":null,"data":[{"id":89,"articleId":null,"title":"关系型数据库","level":1,"children":[{"id":90,"articleId":47,"title":"MySQL","level":2,"children":null}]},{"id":91,"articleId":null,"title":"非关系型数据库","level":1,"children":[{"id":92,"articleId":42,"title":"MongoDB","level":2,"children":null}]},{"id":93,"articleId":null,"title":"缓存","level":1,"children":[{"id":94,"articleId":45,"title":"Redis","level":2,"children":null},{"id":95,"articleId":51,"title":"RedisIncrement","level":2,"children":null}]}]} =================================== 
2024-08-04 21:28:39.301 [http-nio-8088-exec-7] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 21:28:39.456 [http-nio-8088-exec-9] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [获取知识库文章上下页], 耗时: 226ms, 出参: {"success":true,"message":null,"errorCode":null,"data":{"preArticle":null,"nextArticle":{"articleId":42,"articleTitle":"MongoDB"}}} =================================== 
2024-08-04 21:28:39.457 [http-nio-8088-exec-9] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
2024-08-04 21:28:39.649 [http-nio-8088-exec-4] INFO  c.j.weblog.event.subscriber.ReadArticleSubscriber - ==> threadName: http-nio-8088-exec-4
2024-08-04 21:28:39.649 [http-nio-8088-exec-4] INFO  c.j.weblog.event.subscriber.ReadArticleSubscriber - ==> 文章阅读事件消费成功，articleId: 47
2024-08-04 21:28:39.683 [http-nio-8088-exec-4] INFO  c.j.weblog.event.subscriber.ReadArticleSubscriber - ==> 文章阅读量 +1 操作成功，articleId: 47
2024-08-04 21:28:39.686 [http-nio-8088-exec-4] INFO  c.j.weblog.event.subscriber.ReadArticleSubscriber - ==> 当日文章 PV 访问量 +1 操作成功，date: 2024-08-04
2024-08-04 21:28:39.706 [http-nio-8088-exec-4] INFO  c.j.w.f.b.o.aspect.ApiOperationLogAspect - ====== 请求结束: [获取文章详情], 耗时: 603ms, 出参: {"success":true,"message":null,"errorCode":null,"data":{"title":"MySQL","content":"<h2 id=\"1事务的四大特性\">1、事务的四大特性</h2>\n<p>事务特性ACID：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability）。</p>\n<ul>\n<li><strong>原子性</strong>：事务包含的所有操作要么全部成功，要么全部失败回滚。</li>\n<li><strong>一致性</strong>：一个事务在执行前和执行后都必须处于一致性状态。如a和b账户共1000块，两人之间转账无论成功还是失败之后，两个账户的总和都必须是1000块。</li>\n<li><strong>隔离性</strong>：和隔离级别相关，如<code>read committed</code>，<strong>一个事务只能读取到已经提交的修改</strong>。</li>\n<li><strong>持久性</strong>：一个事务一旦被提交了，那么对于数据库中的数据的改变将是永久性的，即使在数据库系统遇到故障的情况下也不会丢失提交事务的操作。</li>\n</ul>\n<h2 id=\"2数据库的三大范式\">2、数据库的三大范式</h2>\n<ol>\n<li>\n<p><strong>第一范式1NF</strong></p>\n<p><strong>确保数据库字段的原子性。</strong> 数据库中的字段不可再分</p>\n<p>比如字段 userInfo : 广东省 10086' ，依照第一范式必须拆分成 userInfo : 广东省 userTel : 10086\n两个字段。</p>\n</li>\n<li>\n<p><strong>第二范式2NF</strong></p>\n<p><strong>首先要满足第一范式，另外还要包含两个内容，一是表必须有主键，二是非主键必须完全依赖于主键，而不能依赖于主键的一部分。</strong></p>\n<p>举个例子。假定选课关系表为student_course (student_no, student_name, age, course_name,\ngrade, credit)，主键为(student_no, course_name)。其中学分完全依赖于课程名称，姓名年龄完全依\n赖学号，不符合第二范式，会导致数据冗余（学生选n门课，姓名年龄有n条记录）、插入异常（插入一\n门新课，因为没有学号，无法保存新课记录）等问题。</p>\n<p>应该拆分成三个表：学生： student (stuent_no, student_name, 年龄)；课程：\ncourse (course_name, credit)；选课关系： student_course_relation (student_no, course_name,\ngrade)。</p>\n</li>\n<li>\n<p><strong>第三范式3NF</strong></p>\n<p><strong>首先要满足第二范式，另外非主键列必须直接依赖于主键，不能存在传递依赖。即不能存在：非主键列A依赖于非主键列B，非主键列B依赖于主键的情况。</strong></p>\n<p>假定学生关系表为Student(student_no, student_name, age, academy_id, academy_telephone)，主\n键为&quot;学号&quot;，其中学院id依赖于学号，而学院地点和学院电话依赖于学院id，存在传递依赖，不符合第三\n范式。</p>\n<p>可以把学生关系表分为如下两个表：学生：(student_no, student_name, age, academy_id)；学院：\n(academy_id, academy_telephone)。</p>\n</li>\n</ol>\n<p><strong>2NF和3NF的区别：</strong></p>\n<ul>\n<li>2NF依据是非主键列是否完全依赖主键，还是依赖于主键的一部分。</li>\n<li>3NF依据是非主键列是直接依赖于主键，还是直接依赖于非主键。</li>\n</ul>\n<p><strong>第一范式：<strong>1NF是对属性的</strong>原子性约束</strong>，要求属性具有原子性，不可再分解；\n<strong>第二范式：<strong>2NF是对记录的</strong>惟一性约束</strong>，要求记录有惟一标识，即实体的惟一性；\n<strong>第三范式：<strong>3NF是对</strong>字段冗余性的约束</strong>，即任何字段不能由其他字段派生出来，它要求字段没有冗余。</p>\n<blockquote>\n<p>没有冗余的数据库设计可以做到。但是，没有冗余的数据库未必是最好的数据库，有时为了提高运行效率，就必须降低范式标准，适当保留冗余数据。具体做法是：在概念数据模型设计时遵守第三范式，降低范式标准的工作放到物理数据模型设计时考虑。降低范式就是增加字段，允许冗余。</p>\n</blockquote>\n<h2 id=\"3事务隔离的级别有哪些\">3、事务隔离的级别有哪些</h2>\n<p>先解释一下脏读、不可重复读和幻读的概念：</p>\n<ul>\n<li><strong>脏读：</strong> 在一个事务处理过程中读取了另一个未提交事务中的数据。</li>\n<li><strong>不可重复读：</strong> 对于数据库中的某行记录，一个事务范围内多次查询却返回了不同的数据值，这是由于在查询间隔，另一个事务修改了数据并提交了。</li>\n<li><strong>幻读：</strong> 当某个事务在读取某个范围内的记录时，另一个事务又在该范围内插入了新的记录。对幻读的正确理解是，一个事务内的读取操作的结论不足以支撑之后业务的执行。假设事务要新增一条记录，主键为id，在新增之前执行了select，发现没有id为xxx的记录，但插入时却出现了主键冲突，这就属于幻读，读取不到的记录却发生了主键冲突，是因为记录实际上已经被其他的事务插入了，但当前事务不可见。</li>\n</ul>\n<p>不可重复读和脏读的区别：<strong>脏读时某一事务读取了另一个事务未提交的脏数据，而不可重复读则是读取了前一个事务提交的数据</strong>。</p>\n<p>事务隔离就是为了解决上面提到的脏读、不可重复读、幻读这几个问题。</p>\n<p>MySQL数据库提供了四种隔离级别：</p>\n<ul>\n<li><strong>Serializable（串行化）</strong>：通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。</li>\n<li><strong>Repeatable read（可重复读）</strong>：<strong>MySQL的默认事务隔离级别，它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行，解决了不可重复读的问题</strong>。</li>\n<li><strong>Read committed（读已提交）</strong>：一个事务只能看见已经提交事务所作的改变，可避免脏读的发生。</li>\n<li><strong>Read uncommitted（读未提交）</strong>：所有事务都可以看见其他未提交事务的执行结果。</li>\n</ul>\n<p><strong>查看隔离级别：</strong></p>\n<pre><code class=\"language-sq\">select @@transaction_isolation;\n</code></pre>\n<p><strong>设置隔离级别：</strong></p>\n<pre><code class=\"language-sql\">set session transaction isolation level read uncommitted;\n</code></pre>\n<h2 id=\"4生产环境数据库一般用的隔离级别\">4、生产环境数据库一般用的隔离级别</h2>\n<p>生产环境一般使用读已提交（RC）隔离级别。为什么不使用可重复读（RR）隔离级别？</p>\n<blockquote>\n<p>可重复读(Repeatable Read)，简称为RR 读已提交(Read Commited)，简称为RC</p>\n</blockquote>\n<ul>\n<li>在可重复读（RR）隔离级别下，存在<strong>间隙锁</strong>，导致出现死锁的几率比读已提交（RC）大得多。</li>\n<li>在可重复读（RR）隔离级别下，<strong>条件列未命中索引会锁表</strong>，而在读已提交（RC）隔离级别下，只锁行。也就是说，读以提交的并发性要比可重复读高。</li>\n<li>并且大部分场景下，不可重复读是可以接受的。毕竟数据都已经提交了，读出来本身没有太大问题。</li>\n</ul>\n<h2 id=\"5编码和字符集的关系\">5、编码和字符集的关系</h2>\n<p>我们平时可以在编辑器上输入各种中文英文字母，但这些都是给人读的，不是给计算机读的，其实<strong>计算机真正保存和传输数据都是以二进制0101的格式进行的</strong>。</p>\n<p>那么就需要有一个规则，把中文和英文字母转化为二进制。其中d对应十六进制下的64，它可以转换为01二进制的格式。于是字母和数字就这样一一对应起来了，这就是ASCII编码格式。</p>\n<p>它用<strong>一个字节</strong>，也就是<code>8位</code>来标识字符，基础符号有128个，扩展符号也是128个。也就只能表示下<strong>英文字母和数字</strong>。</p>\n<p>这明显不够用。于是，为了标识<strong>中文</strong>，出现了<strong>GB2312</strong>的编码格式。为了标识<strong>希腊语</strong>，出现了<strong>greek</strong>编码格式，为了标识<strong>俄语</strong>，整了<strong>cp866</strong>编码格式。</p>\n<p>为了统一它们，于是出现了<strong>Unicode编码格式</strong>，它用了2~4个字节来表示字符，这样理论上所有符号都能被收录进去，并且它还完全兼容ASCII的编码，也就是说，同样是字母d，在ASCII用64表示，在Unicode里还是用64来表示。</p>\n<p>但<strong>不同的地方是ASCII编码用1个字节来表示，而Unicode用则两个字节来表示。</strong></p>\n<p>同样都是字母d，unicode比ascii多使用了一个字节，如下：</p>\n<pre><code class=\"language-\\\">D   ASCII:           01100100\nD Unicode:  00000000 01100100\n</code></pre>\n<p>可以看到，上面的unicode编码，前面的都是0，其实用不上，但还占了个字节，有点浪费。如果我们能做到该隐藏时隐藏，这样就能省下不少空间，按这个思路，就是就有了<strong>UTF-8编码</strong>。</p>\n<p>总结一下，<strong>按照一定规则把符号和二进制码对应起来，这就是编码。而把n多这种已经编码的字符聚在一起，就是我们常说的字符集</strong>。</p>\n<p>比如utf-8字符集就是所有utf-8编码格式的字符的合集。</p>\n<p>想看下mysql支持哪些字符集。可以执行 <code>show charset;</code></p>\n<h2 id=\"6utf8和utf8mb4的区别\">6、utf8和utf8mb4的区别</h2>\n<p>上面提到utf-8是在unicode的基础上做的优化，既然unicode有办法表示所有字符，那utf-8也一样可以表示所有字符，为了避免混淆，我在后面叫它<strong>大utf8</strong>。</p>\n<p>mysql支持的字符集中有utf8和utf8mb4。</p>\n<p>先说<strong>utf8mb4</strong>编码，mb4就是<strong>most bytes 4</strong>的意思，从上图最右边的<code>Maxlen</code>可以看到，它最大支持用<strong>4个字节</strong>来表示字符，它几乎可以用来表示目前已知的所有的字符。</p>\n<p>再说mysql字符集里的<strong>utf8</strong>，它是数据库的<strong>默认字符集</strong>。但注意，<strong>此utf8非彼utf8</strong>，我们叫它<strong>小utf8</strong>字符集。为什么这么说，因为从Maxlen可以看出，它最多支持用3个字节去表示字符，按utf8mb4的命名方式，准确点应该叫它<strong>utf8mb3</strong>。</p>\n<p>utf8 就像是阉割版的utf8mb4，只支持部分字符。比如<code>emoji</code>表情，它就不支持。</p>\n<p>而mysql支持的字符集里，第三列，<strong>collation</strong>，它是指<strong>字符集的比较规则</strong>。</p>\n<p>比如，&quot;debug&quot;和&quot;Debug&quot;是同一个单词，但它们大小写不同，该不该判为同一个单词呢。</p>\n<p>这时候就需要用到collation了。</p>\n<p>通过<code>SHOW COLLATION WHERE Charset = 'utf8mb4';</code>可以查看到<code>utf8mb4</code>下支持什么比较规则。</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/e89f7eae13cd4f3a933a6dffd7b73bdb.png\">\n</p>\n<p>如果<code>collation = utf8mb4_general_ci</code>，是指使用utf8mb4字符集的前提下，<strong>挨个字符进行比较</strong>（<code>general</code>），并且不区分大小写（<code>_ci，case insensitice</code>）。</p>\n<p>这种情况下，&quot;debug&quot;和&quot;Debug&quot;是同一个单词。</p>\n<p>如果改成<code>collation=utf8mb4_bin</code>，就是指<strong>挨个比较二进制位大小</strong>。</p>\n<p>于是&quot;debug&quot;和&quot;Debug&quot;就不是同一个单词。</p>\n<p><strong>那utf8mb4对比utf8有什么劣势吗？</strong></p>\n<p>我们知道数据库表里，字段类型如果是<code>char(2)</code>的话，里面的<code>2</code>是指<strong>字符个数</strong>，也就是说<strong>不管这张表用的是什么编码的字符集</strong>，都能放上2个字符。</p>\n<p>而char又是<strong>固定长度</strong>，为了能放下2个utf8mb4的字符，char会默认保留<code>2*4（maxlen=4）= 8</code>个字节的空间。</p>\n<p>如果是utf8mb3，则会默认保留 <code>2 * 3 (maxlen=3) = 6</code>个字节的空间。也就是说，在这种情况下，<strong>utf8mb4会比utf8mb3多使用一些空间。</strong></p>\n<p><strong>对于固定长度的char类型的字符串，utf8mb4会比utf8多使用一些空间</strong>。</p>\n<h2 id=\"7索引\">7、索引</h2>\n<h3 id=\"71索引的定义\">7.1、索引的定义</h3>\n<p>索引是存储引擎用于提高数据库表的访问速度的一种数据结构。</p>\n<h3 id=\"72索引的优缺点\">7.2、索引的优缺点</h3>\n<p><strong>优点：</strong></p>\n<ul>\n<li><strong>加快数据查找速度。</strong></li>\n<li>为用来排序或者是分组的字段添加索引，可以<strong>加快分组和排序的速度。</strong></li>\n<li><strong>加快表与表之间的连接。</strong></li>\n</ul>\n<p><strong>缺点：</strong></p>\n<ul>\n<li>建立索引需要<strong>占用物理空间</strong>。</li>\n<li>会<strong>降低表的增删改的效率</strong>，因为每次对表记录进行增删改，需要进行动态维护索引，导致增删改的时间变长。</li>\n</ul>\n<h3 id=\"73索引的作用\">7.3、索引的作用</h3>\n<p>数据是存储在磁盘上的，查询数据时，如果没有索引，就需要加载所有的数据到内存上，一次进行检索，读取磁盘次数较多。有了索引，就不需要加载全部数据了，因为B+树的高度一般在2-4层，最多只需要读取2-4次磁盘，查询速度大大提升。</p>\n<h3 id=\"74什么情况下需要建立索引\">7.4、什么情况下需要建立索引</h3>\n<ol>\n<li>经常用于<strong>查询</strong>的字段</li>\n<li>经常用于<strong>连接</strong>的字段建立索引，可以加快连接的速度。</li>\n<li>经常需要<strong>排序</strong>的字段建立索引，因为索引已经排好序，可以加快排序查询速度。</li>\n</ol>\n<h3 id=\"75什么情况下不建立索引\">7.5、什么情况下不建立索引</h3>\n<ol>\n<li><strong>where条件中用不到的字段</strong>不适合建立索引。</li>\n<li><strong>表记录较少</strong>。比如只有几百条数据，没必要加索引。</li>\n<li>需要<strong>经常增删改</strong>，需要评估是否适合加索引。</li>\n<li><strong>参与列计算的列</strong>不适合建立索引。</li>\n<li><strong>区分度不高的字段</strong>不适合建立索引，如性别，只有男/女/未知三个值。加了索引，查询效率也不会提高。</li>\n</ol>\n<h3 id=\"76索引的数据结构\">7.6、索引的数据结构</h3>\n<p>索引的数据结构主要有B+树和哈希表，对应的索引分别是<strong>B+树索引和哈希索引</strong>。InnoDB引擎的索引类型有B+树索引和哈希索引，默认的索引类型为B+树索引。</p>\n<p><strong>B+树索引</strong></p>\n<p>B+ 树是基于B 树和叶子节点顺序访问指针进行实现，它具有B树的平衡性，并且通过顺序访问指针来提高区间查询的性能。</p>\n<p>在 B+ 树中，节点中的 <code>key</code> 从左到右递增排列，如果某个指针的左右相邻 <code>key</code> 分别是 keyi 和 keyi+1，则该指针指向节点的所有 <code>key</code> 大于等于 keyi 且小于等于 keyi+1。</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/2d2a398bc5e94876a66d4e8359570895.png\">\n</p>\n<p>进行查找操作时，首先在根节点进行二分查找，找到key 所在的指针，然后递归地在指针所指向的节点进行查找。直到查找到叶子节点，然后在叶子节点上进行二分查找，找出key 所对应的数据项。</p>\n<p>MySQL 数据库使用最多的索引类型是BTREE 索引，底层基于B+树数据结构来实现。</p>\n<pre><code class=\"language-cmd\">mysql&gt; show index from blog\\G;\n*************************** 1. row ***************************\n        Table: blog\n   Non_unique: 0\n     Key_name: PRIMARY\n Seq_in_index: 1\n  Column_name: blog_id\n    Collation: A\n  Cardinality: 4\n     Sub_part: NULL\n       Packed: NULL\n         Null:\n   Index_type: BTREE\n      Comment:\nIndex_comment:\n      Visible: YES\n   Expression: NULL\n\n</code></pre>\n<p><strong>哈希索引</strong></p>\n<p>哈希索引是基于哈希表实现的，对于每一行数据，存储引擎会对索引列进行哈希计算得到哈希码，并且哈希算法要尽量保证不同的列值计算出的哈希码值是不同的，将哈希码的值作为哈希表的key值，将指向数据行的指针作为哈希表的value值。这样查找一个数据的时间复杂度就是O(1)，一般多用于精确查找。</p>\n<h3 id=\"77hash索引和b树索引的区别\">7.7、Hash索引和B+树索引的区别</h3>\n<ul>\n<li>哈希索引<strong>不支持排序</strong>，因为哈希表是无序的。</li>\n<li>哈希索引<strong>不支持范围查找</strong>。</li>\n<li>哈希索引<strong>不支持模糊查询</strong>以及多列索引的最左前缀匹配。</li>\n<li>因为哈希表会存在哈希冲突，所以<strong>哈希索引的性能是不稳定的，而B+树索引的性能相对稳定</strong>，每次查询都是从根节点到叶子节点。</li>\n</ul>\n<h3 id=\"78为什么b树比b树更适合实现数据库索引\">7.8、为什么B+树比B树更适合实现数据库索引</h3>\n<ul>\n<li>由于<strong>B+树的数据都存储在叶子节点中</strong>，叶子节点均为索引，方便扫库，只需要扫一遍叶子节点即可，但是B树因为其分支同样存储着数据，我们要找到具体的数据，需要进行一次中序遍历按序来扫，所以B+树更适合在区间查询的情况，而在数据库中基于范围的查询是非常频繁的，所以通常使用B+树用于数据库索引。</li>\n<li><strong>B+树的节点只存储索引key值，具体信息的地址存在于叶子节点的地址中</strong>。这就使<strong>以页为单位的索引中可以存放更多的节点</strong>。减少更多的I/O支出。</li>\n<li><strong>B+树的查询效率更加稳定，任何关键字的查找必须走一条从根结点到叶子结点的路</strong>。所有关键字查询的路径长度相同，导致每一个数据的查询效率相当。</li>\n</ul>\n<hr />\n<ul>\n<li>B+ 树的非叶子节点不存放实际的记录数据，仅存放索引，因此数据量相同的情况下，相比存储即存索引又存记录的 B 树，B+树的非叶子节点可以存放更多的索引，因此 B+ 树可以比 B 树更「矮胖」，查询底层节点的磁盘 I/O次数会更少。</li>\n<li>B+ 树有大量的冗余节点（所有非叶子节点都是冗余索引），这些冗余索引让 B+ 树在插入、删除的效率都更高，比如删除根节点的时候，不会像 B 树那样会发生复杂的树的变化；</li>\n<li>B+ 树叶子节点之间用链表连接了起来，有利于范围查询，而 B 树要实现范围查询，因此只能通过树的遍历来完成范围查询，这会涉及多个节点的磁盘 I/O 操作，范围查询效率不如 B+ 树。</li>\n</ul>\n<h3 id=\"79索引的分类\">7.9、索引的分类</h3>\n<ol>\n<li>\n<p><strong>主键索引</strong>：名为primary的唯一非空索引，不允许有空值。</p>\n</li>\n<li>\n<p><strong>唯一索引</strong>：索引列的值必须是唯一的，允许有空值。主键索引和唯一索引的区别在于：唯一索引字段可以为null且可以存在多个null值，而主键索引字段不可以为null。唯一索引的用途：唯一标识数据库中的每条记录，主要用来防止重复数据插入。创建唯一索引的SQL语句为：</p>\n<pre><code class=\"language-sql\">ALTER TABLE table_name\nADD CONSTRAINT constraint_name UNIQUE KEY(column_1,column_2,...);\n</code></pre>\n</li>\n<li>\n<p><strong>组合索引</strong>：在表中的多个字段组合上创建的索引，只有在查询条件中使用了这些字段的左边字段时，索引才会被使用，使用组合索引时需遵循最左前缀原则。</p>\n</li>\n<li>\n<p><strong>全文索引</strong>：只能在CHAR 、VARCHAR 和TEXT 类型字段上使用全文索引。</p>\n</li>\n<li>\n<p><strong>普通索引</strong>：普通索引是最基本的索引，它没有任何限制，值可以为空。</p>\n</li>\n</ol>\n<h3 id=\"710什么是最左匹配原则\">7.10、什么是最左匹配原则</h3>\n<p><strong>如果 SQL 语句中用到了组合索引中的最左边的索引，那么这条 SQL 语句就可以利用这个组合索引去进行匹配。当遇到范围查询(<code>&gt;</code>、<code>&lt;</code>、<code>between</code>、<code>like</code>)就会停止匹配，后面的字段不会用到索引</strong>。</p>\n<p>对<code>(a,b,c)</code>建立索引，查询条件使用 a/ab/abc 会走索引，使用 bc 不会走索引。</p>\n<p>对<code>(a,b,c,d)</code>建立索引，查询条件为<code>a = 1 and b = 2 and c &gt; 3 and d = 4</code>，那么a、b和c三个字段能用到索引，而d无法使用索引。因为遇到了范围查询。</p>\n<p>如下图，对(a, b) 建立索引，a 在索引树中是全局有序的，而 b 是全局无序，局部有序（当a相等时，会根据b进行排序）。直接执行<code>b = 2</code>这种查询条件无法使用索引。</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/fe96a1d64f5a4eaf9181803fe8fc1294.png\">\n</p>\n<p>当a的值确定的时候，b是有序的。例如<code>a = 1</code>时，b值为1，2是有序的状态。当<code>a = 2</code>时候，b的值为1，4也是有序状态。 当执行<code>a = 1 and b = 2</code>时a和b字段能用到索引。而执行<code>a &gt; 1 and b = 2</code>时，a字段能用到索引，b字段用不到索引。因为a的值此时是一个范围，不是固定的，在这个范围内b值不是有序的，因此b字段无法使用索引。</p>\n<h3 id=\"711什么是聚集索引\">7.11、什么是聚集索引</h3>\n<p>InnoDB使用<strong>表的主键构造主键索引树</strong>，同时叶子节点中存放的即为整张表的记录数据。聚集索引叶子节点的存储是逻辑上连续的，使用双向链表连接，叶子节点按照主键的顺序排序，因此对于主键的排序查找和范围查找速度比较快。如果表中没有显示指定主键，则会<strong>选择表中的第一个不允许为NULL 的唯一索引</strong>。如果没有主键也没有合适的唯一索引，那么InnoDB 内部会生成一个隐藏的主键作为聚集索引，这个隐藏的主键长度为6个字节，它的值会随着数据的插入自增。</p>\n<h3 id=\"712什么是覆盖索引\">7.12、什么是覆盖索引</h3>\n<p><em>InnoDB 的数据是按「数据页」为单位来读写的，默认数据页大小为 16 KB。每个数据页之间通过双向链表的形式组织起来，物理上不连续，但是逻辑上连续。</em></p>\n<p><em>数据页内包含用户记录，每个记录之间用单向链表的方式组织起来，为了加快在数据页内高效查询记录，设计了一个页目录，页目录存储各个槽（分组），且主键值是有序的，于是可以通过二分查找法的方式进行检索从而提高效率。</em></p>\n<p><em>为了高效查询记录所在的数据页，InnoDB 采用 b+ 树作为索引，每个节点都是一个数据页。</em></p>\n<p><em>如果叶子节点存储的是实际数据的就是聚簇索引，一个表只能有一个聚簇索引；如果叶子节点存储的不是实际数据，而是主键值则就是二级索引，一个表中可以有多个二级索引。</em></p>\n<p><em>在使用二级索引进行查找数据时，如果查询的数据能在二级索引找到，那么就是「索引覆盖」操作，如果查询的数据不在二级索引里，就需要先在二级索引找到主键值，需要去聚簇索引中获得数据行，这个过程就叫作「回表」。</em></p>\n<p><strong>覆盖索引（covering index ，或称为索引覆盖）即从非主键索引中就能查到的记录，而不需要查询主键索引中的记录，避免了回表的产生减少了树的搜索次数，显著提升性能</strong>。不是所有类型的索引都可以成为覆盖索引。覆盖索引要存储索引列的值，而哈希索引、全文索引不存储索引列的值，所以<strong>MySQL使用b+树索引做覆盖索引</strong>。</p>\n<p>对于使用了覆盖索引的查询，在查询前面使用<code>explain</code>，输出的extra列会显示为<code>using index</code>。</p>\n<p>比如<code>user_like</code> 用户点赞表，组合索引为<code>(user_id, blog_id)</code>，<code>user_id</code>和<code>blog_id</code>都不为<code>null</code>。</p>\n<pre><code class=\"language-sql\">explain select blog_id from user_like where user_id = 13;\n</code></pre>\n<p><code>explain</code>结果的<code>Extra</code>列为<code>Using index</code>，查询的列被索引覆盖，并且where筛选条件符合最左前缀原则，通过<strong>索引查找</strong>就能直接找到符合条件的数据，不需要回表查询数据。</p>\n<pre><code class=\"language-sql\">explain select user_id from user_like where blog_id = 1;\n</code></pre>\n<p><code>explain</code>结果的<code>Extra</code>列为<code>Using where; Using index</code>， 查询的列被索引覆盖，where筛选条件不符合最左前缀原则，无法通过索引查找找到符合条件的数据，但可以通过<strong>索引扫描</strong>找到符合条件的数据，也不需要回表查询数据。</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/01d86030cd5e4303bb22af9d6167c413.png\">\n</p>\n<blockquote>\n<p><em>如果某个查询语句使用了二级索引，但是查询的数据不是主键值，这时在二级索引找到主键值后，需要去聚簇索引中获得数据行，这个过程就叫作「回表」，也就是说要查两个 B+ 树才能查到数据。不过，当查询的数据是主键值时，因为只在二级索引就能查询到，不用再去聚簇索引查，这个过程就叫作「索引覆盖」，也就是只需要查一个 B+ 树就能找到数据。</em></p>\n</blockquote>\n<h3 id=\"713索引的设计原则\">7.13、索引的设计原则</h3>\n<ul>\n<li>\n<p>对于经常作为查询条件的字段，应该建立索引，以提高查询速度</p>\n</li>\n<li>\n<p>为经常需要排序、分组和联合操作的字段建立索引</p>\n</li>\n<li>\n<p>索引列的<strong>区分度越高</strong>，索引的效果越好。比如使用性别这种区分度很低的列作为索引，效果就会很差。</p>\n</li>\n<li>\n<p>避免给&quot;大字段&quot;建立索引。尽量使用数据量小的字段作为索引。因为<code>MySQL</code>在维护索引的时候是会将字段值一起维护的，那这样必然会导致索引占用更多的空间，另外在排序的时候需要花费更多的时间去对比。</p>\n</li>\n<li>\n<p>尽量使用<strong>短索引</strong>，对于较长的字符串进行索引时应该指定一个较短的前缀长度，因为较小的索引涉及到的磁盘I/O较少，查询速度更快。</p>\n</li>\n<li>\n<p>索引不是越多越好，每个索引都需要额外的物理空间，维护也需要花费时间。</p>\n</li>\n<li>\n<p>频繁增删改的字段不要建立索引。假设某个字段频繁修改，那就意味着需要频繁的重建索引，这必然影响MySQL的性能</p>\n</li>\n<li>\n<p>利用<strong>最左前缀原则</strong>。</p>\n</li>\n<li>\n<p>自增主键可以让主键索引尽量地保持递增顺序插入，避免了页分裂，因此索引更紧凑，在查询的时候，效率也就更高。</p>\n</li>\n</ul>\n<h3 id=\"714索引什么时候会失效\">7.14、索引什么时候会失效</h3>\n<p>导致索引失效的情况：</p>\n<ul>\n<li>对于组合索引，不是使用组合索引最左边的字段，则不会使用索引</li>\n<li>以%开头的like查询如<code>%abc</code>，无法使用索引；非%开头的like查询如<code>abc%</code>，相当于范围查询，会使用索引</li>\n<li>查询条件中列类型是字符串，没有使用引号，可能会因为类型不同发生隐式转换，使索引失效</li>\n<li>判断索引列是否不等于某个值时</li>\n<li>对索引列进行运算</li>\n<li>查询条件使用<code>or</code>连接，也会导致索引失效</li>\n</ul>\n<h3 id=\"715什么是前缀索引\">7.15、什么是前缀索引</h3>\n<p>有时需要在很长的字符列上创建索引，这会造成索引特别大且慢。使用前缀索引可以避免这个问题。</p>\n<p><strong>前缀索引是指对文本或者字符串的前几个字符建立索引</strong>，这样索引的长度更短，查询速度更快。</p>\n<p>创建前缀索引的关键在于选择足够长的前缀以<strong>保证较高的索引选择性</strong>。索引选择性越高查询效率就越高，因为选择性高的索引可以让MySQL在查找时过滤掉更多的数据行。</p>\n<p>建立前缀索引的方式：</p>\n<pre><code class=\"language-sql\">// email列创建前缀索引\nALTER TABLE table_name ADD KEY(column_name(prefix_length));\n</code></pre>\n<h2 id=\"8常见的存储引擎有哪些\">8、常见的存储引擎有哪些</h2>\n<p>MySQL中常用的四种存储引擎分别是： <strong>MyISAM</strong>、<strong>InnoDB</strong>、<strong>MEMORY</strong>、<strong>ARCHIVE</strong>。MySQL 5.5版本后默认的存储引擎为<code>InnoDB</code>。</p>\n<p><strong>InnoDB存储引擎</strong></p>\n<p>InnoDB是MySQL<strong>默认的事务型存储引擎</strong>，使用最广泛，基于聚簇索引建立的。InnoDB内部做了很多优化，如能够自动在内存中创建自适应hash索引，以加速读操作。</p>\n<p><strong>优点</strong>：支持事务和崩溃修复能力；引入了行级锁和外键约束。</p>\n<p><strong>缺点</strong>：占用的数据空间相对较大。</p>\n<p><strong>适用场景</strong>：需要事务支持，并且有较高的并发读写频率。</p>\n<p><strong>MyISAM存储引擎</strong></p>\n<p>数据以紧密格式存储。对于只读数据，或者表比较小、可以容忍修复操作，可以使用MyISAM引擎。MyISAM会将表存储在两个文件中，数据文件<code>.MYD</code>和索引文件<code>.MYI</code>。</p>\n<p><strong>优点</strong>：访问速度快。</p>\n<p><strong>缺点</strong>：MyISAM不支持事务和行级锁，不支持崩溃后的安全恢复，也不支持外键。</p>\n<p><strong>适用场景</strong>：对事务完整性没有要求；表的数据都是只读的。</p>\n<p><strong>MEMORY存储引擎</strong></p>\n<p>MEMORY引擎将数据全部放在内存中，访问速度较快，但是一旦系统奔溃的话，数据都会丢失。</p>\n<p>MEMORY引擎默认使用哈希索引，将键的哈希值和指向数据行的指针保存在哈希索引中。</p>\n<p><strong>优点</strong>：访问速度较快。</p>\n<p><strong>缺点</strong>：</p>\n<ol>\n<li>哈希索引数据不是按照索引值顺序存储，无法用于排序。</li>\n<li>不支持部分索引匹配查找，因为哈希索引是使用索引列的全部内容来计算哈希值的。</li>\n<li>只支持等值比较，不支持范围查询。</li>\n<li>当出现哈希冲突时，存储引擎需要遍历链表中所有的行指针，逐行进行比较，直到找到符合条件的行。</li>\n</ol>\n<p><strong>ARCHIVE存储引擎</strong></p>\n<p>ARCHIVE存储引擎非常适合存储大量独立的、作为历史记录的数据。ARCHIVE提供了压缩功能，拥有高效的插入速度，但是这种引擎不支持索引，所以查询性能较差。</p>\n<h2 id=\"9myisam和innodb的区别\">9、MyISAM和InnoDB的区别</h2>\n<ul>\n<li>\n<p><strong>存储结构的区别</strong>。每个MyISAM在磁盘上存储成三个文件。文件的名字以表的名字开始，扩展名指出文件类型。 .frm文件存储表定义。数据文件的扩展名为.MYD (MYData)。索引文件的扩展名是.MYI (MYIndex)。InnoDB所有的表都保存在同一个数据文件中（也可能是多个文件，或者是独立的表空间文件），InnoDB表的大小只受限于操作系统文件的大小，一般为2GB。</p>\n</li>\n<li>\n<p><strong>存储空间的区别</strong>。MyISAM支持支持三种不同的存储格式：静态表(默认，但是注意数据末尾不能有空格，会被去掉)、动态表、压缩表。当表在创建之后并导入数据之后，不会再进行修改操作，可以使用压缩表，极大的减少磁盘的空间占用。InnoDB需要更多的内存和存储，它会在主内存中建立其专用的缓冲池用于高速缓冲数据和索引。</p>\n</li>\n<li>\n<p><strong>可移植性、备份及恢复</strong>。MyISAM数据是以文件的形式存储，所以在跨平台的数据转移中会很方便。在备份和恢复时可单独针对某个表进行操作。对于InnoDB，可行的方案是拷贝数据文件、备份 binlog，或者用mysqldump，在数据量达到几十G的时候就相对麻烦了。</p>\n</li>\n<li>\n<p><strong>是否支持行级锁</strong>。MyISAM 只支持表级锁，用户在操作myisam表时，select，update，delete，insert语句都会给表自动加锁，如果加锁以后的表满足insert并发的情况下，可以在表的尾部插入新的数据。而InnoDB 支持行级锁和表级锁，默认为行级锁。行锁大幅度提高了多用户并发操作的性能。</p>\n</li>\n<li>\n<p><strong>是否支持事务和崩溃后的安全恢复</strong>。 MyISAM 不提供事务支持。而InnoDB 提供事务支持，具有事务、回滚和崩溃修复能力。</p>\n</li>\n<li>\n<p><strong>是否支持外键</strong>。MyISAM不支持，而InnoDB支持。</p>\n</li>\n<li>\n<p><strong>是否支持MVCC</strong>。MyISAM不支持，InnoDB支持。应对高并发事务，MVCC比单纯的加锁更高效。</p>\n</li>\n<li>\n<p><strong>是否支持聚集索引</strong>。MyISAM不支持聚集索引，InnoDB支持聚集索引。</p>\n</li>\n<li>\n<p><strong>全文索引</strong>。MyISAM支持 FULLTEXT类型的全文索引。InnoDB不支持FULLTEXT类型的全文索引，但是innodb可以使用sphinx插件支持全文索引，并且效果更好。</p>\n</li>\n<li>\n<p><strong>表主键</strong>。MyISAM允许没有任何索引和主键的表存在，索引都是保存行的地址。对于InnoDB，如果没有设定主键或者非空唯一索引，就会自动生成一个6字节的主键(用户不可见)。</p>\n</li>\n<li>\n<p><strong>表的行数</strong>。MyISAM保存有表的总行数，如果<code>select count(*) from table</code>;会直接取出该值。InnoDB没有保存表的总行数，如果使用select count(*) from table；就会遍历整个表，消耗相当大，但是在加了where条件后，MyISAM和InnoDB处理的方式都一样。</p>\n</li>\n</ul>\n<hr />\n<ul>\n<li>\n<p><strong>存储结构的区别</strong>。每个MyISAM在磁盘上存储成三个文件（索引文件、数据文件和表结构文件）。InnoDB所有的表都保存在同一个数据文件中（也可能是多个文件，或者是独立的表空间文件）。</p>\n</li>\n<li>\n<p><strong>存储空间的区别</strong>。MyISAM支持三种不同的存储格式：静态表、动态表、压缩表。当表在创建之后并导入数据之后，不会再进行修改操作，可以使用压缩表，极大的减少磁盘的空间占用。InnoDB需要更多的内存和存储，它会在主内存中建立其专用的缓冲池用于高速缓冲数据和索引。</p>\n</li>\n<li>\n<p><strong>可移植性、备份及恢复</strong>。MyISAM数据是以文件的形式存储，所以在跨平台的数据转移中会很方便。对于InnoDB，可行的方案是拷贝数据文件、备份 binlog，或者用mysqldump，在数据量达到几十G的时候就相对麻烦了。</p>\n</li>\n<li>\n<p><strong>是否支持行级锁</strong>。MyISAM 只支持表级锁，而InnoDB 支持行级锁和表级锁，默认为行级锁。行锁大幅度提高了多用户并发操作的性能。</p>\n</li>\n<li>\n<p><strong>是否支持事务和崩溃后的安全恢复</strong>。 MyISAM 不提供事务支持。而InnoDB 提供事务支持，具有事务、回滚和崩溃修复能力。</p>\n</li>\n<li>\n<p><strong>是否支持外键</strong>。MyISAM不支持，而InnoDB支持。</p>\n</li>\n<li>\n<p><strong>是否支持MVCC</strong>。MyISAM不支持，InnoDB支持。应对高并发事务，MVCC比单纯的加锁更高效。</p>\n</li>\n<li>\n<p><strong>是否支持聚集索引</strong>。MyISAM不支持聚集索引，InnoDB支持聚集索引。</p>\n</li>\n<li>\n<p><strong>全文索引</strong>。MyISAM支持 FULLTEXT类型的全文索引。InnoDB不支持FULLTEXT类型的全文索引，但是innodb可以使用sphinx插件支持全文索引，并且效果更好。</p>\n</li>\n<li>\n<p><strong>表主键</strong>。MyISAM允许没有任何索引和主键的表存在，索引都是保存行的地址。对于InnoDB，如果没有设定主键或者非空唯一索引，就会自动生成一个6字节的主键(用户不可见)。</p>\n</li>\n<li>\n<p><strong>表的行数</strong>。MyISAM保存有表的总行数，如果<code>select count(*) from table</code>;会直接取出该值。InnoDB没有保存表的总行数，如果使用select count(*) from table；就会遍历整个表，消耗相当大，但是在加了where条件后，MyISAM和InnoDB处理的方式都一样。</p>\n</li>\n</ul>\n<h2 id=\"10mysql有哪些锁\">10、MySQL有哪些锁</h2>\n<p><strong>按锁粒度分类</strong>，有行级锁、表级锁和页级锁。</p>\n<ol>\n<li>行级锁是mysql中锁的粒度最细的一种锁。表示只针对当前操作的行进行加锁。行级锁能大大减少数据库操作的冲突，其加锁粒度最小，但加锁的开销也最大。行级锁的类型主要有三类：\n<ul>\n<li>Record Lock，记录锁，也就是仅仅把一条记录锁上；</li>\n<li>Gap Lock，间隙锁，锁定一个范围，但是不包含记录本身；</li>\n<li>Next-Key Lock：Record Lock + Gap Lock 的组合，锁定一个范围，并且锁定记录本身。</li>\n</ul>\n</li>\n<li>表级锁是mysql中锁定粒度最大的一种锁，表示对当前操作的整张表加锁，它实现简单，资源消耗较少，被大部分mysql引擎支持。最常使用的MyISAM与InnoDB都支持表级锁定。</li>\n<li>页级锁是 MySQL 中锁定粒度介于行级锁和表级锁中间的一种锁。表级锁速度快，但冲突多，行级冲突少，但速度慢。因此，采取了折衷的页级锁，一次锁定相邻的一组记录。</li>\n</ol>\n<p><strong>按锁级别分类</strong>，有共享锁、排他锁和意向锁。</p>\n<ol>\n<li>共享锁又称读锁，是读取操作创建的锁。其他用户可以并发读取数据，但任何事务都不能对数据进行修改（获取数据上的排他锁），直到已释放所有共享锁。</li>\n<li>排他锁又称写锁、独占锁，如果事务T对数据A加上排他锁后，则其他事务不能再对A加任何类型的封锁。获准排他锁的事务既能读数据，又能修改数据。</li>\n<li>意向锁是表级锁，其设计目的主要是为了在一个事务中揭示下一行将要被请求锁的类型。InnoDB 中的两个表锁：\n<ul>\n<li>意向共享锁（IS）：表示事务准备给数据行加入共享锁，也就是说一个数据行加共享锁前必须先取得该表的IS锁；</li>\n<li>意向排他锁（IX）：类似上面，表示事务准备给数据行加入排他锁，说明事务在一个数据行加排他锁前必须先取得该表的IX锁。</li>\n</ul>\n</li>\n</ol>\n<p>意向锁是 InnoDB 自动加的，不需要用户干预。</p>\n<p><strong>对于INSERT、UPDATE和DELETE，InnoDB 会自动给涉及的数据加排他锁；对于一般的SELECT语句，InnoDB 不会加任何锁</strong>，事务可以通过以下语句显式加共享锁或排他锁。</p>\n<p>共享锁：<code>SELECT … LOCK IN SHARE MODE;</code></p>\n<p>排他锁：<code>SELECT … FOR UPDATE;</code></p>\n<h2 id=\"11mvcc-实现原理\">11、MVCC 实现原理</h2>\n<p>我们需要了解两个知识：</p>\n<p>Read View 中四个字段作用；\n聚簇索引记录中两个跟事务有关的隐藏列；\n那 Read View 到底是个什么东西？</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/26e9aa37ce3440db94c8e15459fe2e53.png\">\n</p>\n<p>Read View 有四个重要的字段：</p>\n<ul>\n<li><em><strong>m_ids</strong></em> ：指的是在创建 Read View 时，当前数据库中「活跃事务」的事务 id 列表，注意是一个列表，“活跃事务”指的就是，启动了但还没提交的事务。</li>\n<li><em><strong>min_trx_id</strong></em> ：指的是在创建 Read View 时，当前数据库中「活跃事务」中事务 id 最小的事务，也就是 m_ids 的最小值。</li>\n<li><em><strong>max_trx_id</strong></em> ：这个并不是 m_ids 的最大值，而是创建 Read View 时当前数据库中应该给下一个事务的 id 值，也就是全局事务中最大的事务 id 值 + 1；</li>\n<li><em><strong>creator_trx_id</strong></em> ：指的是创建该 Read View 的事务的事务 id。\n知道了 Read View 的字段，我们还需要了解聚簇索引记录中的两个隐藏列。</li>\n</ul>\n<p>假设在账户余额表插入一条小林余额为 100 万的记录，然后我把这两个隐藏列也画出来，该记录的整个示意图如下：</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/30528a510be44807b7535d94eacd1304.png\">\n</p>\n<p>对于使用 InnoDB 存储引擎的数据库表，它的聚簇索引记录中都包含下面两个隐藏列：</p>\n<ul>\n<li><em><strong>trx_id</strong></em>，当一个事务对某条聚簇索引记录进行改动时，就会把该事务的事务 id 记录在 trx_id 隐藏列里；</li>\n<li><em><strong>roll_pointer</strong></em>，每次对某条聚簇索引记录进行改动时，都会把旧版本的记录写入到 undo 日志中，然后这个隐藏列是个指针，指向每一个旧版本记录，于是就可以通过它找到修改前的记录。\n在创建 Read View 后，我们可以将记录中的 trx_id 划分这三种情况：</li>\n</ul>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/8dc914e744dc4d9aa514ed1bd0fef737.png\">\n</p>\n<p>一个事务去访问记录的时候，除了自己的更新记录总是可见之外，还有这几种情况：</p>\n<ul>\n<li>如果记录的 trx_id 值小于 Read View 中的 min_trx_id 值，表示这个版本的记录是在创建 Read View 前已经提交的事务生成的，所以该版本的记录对当前事务可见。</li>\n<li>如果记录的 trx_id 值大于等于 Read View 中的 max_trx_id 值，表示这个版本的记录是在创建 Read View 后才启动的事务生成的，所以该版本的记录对当前事务不可见。</li>\n<li>如果记录的 trx_id 值在 Read View 的 min_trx_id 和 max_trx_id 之间，需要判断 trx_id 是否在 m_ids 列表中：\n<ul>\n<li>如果记录的 trx_id 在 m_ids 列表中，表示生成该版本记录的活跃事务依然活跃着（还没提交事务），所以该版本的记录对当前事务不可见。</li>\n<li>如果记录的 trx_id 不在 m_ids列表中，表示生成该版本记录的活跃事务已经被提交，所以该版本的记录对当前事务可见。</li>\n</ul>\n</li>\n</ul>\n<p><em><strong>这种通过「版本链」来控制并发事务访问同一个记录时的行为就叫 MVCC（多版本并发控制）</strong></em>。</p>\n<h2 id=\"12快照读和当前读\">12、快照读和当前读</h2>\n<p>表记录有两种读取方式。</p>\n<ul>\n<li>快照读：<strong>读取的是快照版本</strong>。普通的<code>SELECT</code>就是快照读。通过mvcc来进行并发控制的，不用加锁。</li>\n<li>当前读：<strong>读取的是最新版本</strong>。<code>UPDATE、DELETE、INSERT、SELECT … LOCK IN SHARE MODE、SELECT … FOR UPDATE</code>是当前读。</li>\n</ul>\n<p>快照读情况下，InnoDB通过<code>mvcc</code>机制避免了幻读现象。而<code>mvcc</code>机制无法避免当前读情况下出现的幻读现象。因为当前读每次读取的都是最新数据，这时如果两次查询中间有其它事务插入数据，就会产生幻读。</p>\n<p>下面举个例子说明下：</p>\n<p>1、首先，user表只有两条记录，具体如下：</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/a0f3abf69e0e4a45868300c8f96ae0ad.png\">\n</p>\n<p>2、事务a和事务b同时开启事务<code>start transaction</code>；</p>\n<p>3、事务a插入数据然后提交；</p>\n<pre><code class=\"language-sql\">insert into user(user_name, user_password, user_mail, user_state) values('tyson', 'a', 'a', 0);\n</code></pre>\n<p>4、事务b执行全表的update；</p>\n<pre><code class=\"language-sql\">update user set user_name = 'a';\n</code></pre>\n<p>5、事务b然后执行查询，查到了事务a中插入的数据。（下图左边是事务b，右边是事务a。事务开始之前只有两条记录，事务a插入一条数据之后，事务b查询出来是三条数据）</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/f98b40f48d1641018a638e91fda8a69d.png\">\n</p>\n<p>以上就是当前读出现的幻读现象。</p>\n<p><strong>那么MySQL是如何避免幻读？</strong></p>\n<ul>\n<li>针对快照读（普通 select 语句），是通过 MVCC 方式解决了幻读，因为可重复读隔离级别下，事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，即使中途有其他事务插入了一条数据，是查询不出来这条数据的，所以就很好了避免幻读问题。</li>\n<li>针对当前读（select ... for update 等语句），是通过 next-key lock（记录锁+间隙锁）方式解决了幻读，因为当执行 select ... for update 语句的时候，会加上 next-key lock，如果有其他事务在 next-key lock 锁范围内插入了一条记录，那么这个插入语句就会被阻塞，无法成功插入，所以就很好了避免幻读问题。</li>\n</ul>\n<p>next-key包括两部分：行锁和间隙锁。行锁是加在索引上的锁，间隙锁是加在索引之间的。</p>\n<p><code>Serializable</code>隔离级别也可以避免幻读，会锁住整张表，并发性极低，一般不会使用。</p>\n<h2 id=\"13共享锁和排他锁\">13、共享锁和排他锁</h2>\n<p>SELECT 的读取锁定主要分为两种方式：共享锁和排他锁。</p>\n<pre><code class=\"language-sql\">select * from table where id&lt;6 lock in share mode;--共享锁\nselect * from table where id&lt;6 for update;--排他锁\n</code></pre>\n<p>这两种方式主要的不同在于<code>LOCK IN SHARE MODE </code>多个事务同时更新同一个表单时很容易造成死锁。</p>\n<p>申请排他锁的前提是，没有线程对该结果集的任何行数据使用排它锁或者共享锁，否则申请会受到阻塞。在进行事务操作时，MySQL会对查询结果集的每行数据添加排它锁，其他线程对这些数据的更改或删除操作会被阻塞（只能读操作），直到该语句的事务被<code>commit</code>语句或<code>rollback</code>语句结束为止。</p>\n<p><code>SELECT... FOR UPDATE</code> 使用注意事项：</p>\n<ol>\n<li><code>for update</code> 仅适用于innodb，且必须在事务范围内才能生效。</li>\n<li>根据主键进行查询，查询条件为<code>like</code>或者不等于，主键字段产生<strong>表锁</strong>。</li>\n<li>根据非索引字段进行查询，会产生<strong>表锁</strong>。</li>\n</ol>\n<h2 id=\"14bin-logredo-logundo-log\">14、bin log/redo log/undo log</h2>\n<p>MySQL日志主要包括查询日志、慢查询日志、事务日志、错误日志、二进制日志等。其中比较重要的是 <code>bin log</code>（二进制日志）和 <code>redo log</code>（重做日志）和 <code>undo log</code>（回滚日志）。</p>\n<p><strong>bin log</strong></p>\n<p><code>bin log</code>是MySQL数据库级别的文件，记录对MySQL数据库执行修改的所有操作，不会记录select和show语句，主要用于恢复数据库和同步数据库。</p>\n<p><strong>redo log</strong></p>\n<p><code>redo log</code>是innodb引擎级别，用来记录innodb存储引擎的事务日志，不管事务是否提交都会记录下来，用于数据恢复。当数据库发生故障，innoDB存储引擎会使用<code>redo log</code>恢复到发生故障前的时刻，以此来保证数据的完整性。将参数<code>innodb_flush_log_at_tx_commit</code>设置为1，那么在执行commit时会将<code>redo log</code>同步写到磁盘。</p>\n<p><strong>undo log</strong></p>\n<p>除了记录<code>redo log</code>外，当进行数据修改时还会记录<code>undo log</code>，<code>undo log</code>用于数据的撤回操作，它保留了记录修改前的内容。通过<code>undo log</code>可以实现事务回滚，并且可以根据<code>undo log</code>回溯到某个特定的版本的数据，<strong>实现MVCC</strong>。</p>\n<blockquote>\n<ul>\n<li><em><strong>实现事务回滚</strong></em>，保障事务的原子性。事务处理过程中，如果出现了错误或者用户执 行了 ROLLBACK 语句，MySQL 可以利用 undo log 中的历史数据将数据恢复到事务开始之前的状态。</li>\n<li><em><strong>实现 MVCC（多版本并发控制）关键因素之一</strong></em>。MVCC 是通过 ReadView + undo log 实现的。undo log 为每条记录保存多份历史数据，MySQL 在执行快照读（普通 select 语句）的时候，会根据事务的 Read View 里的信息，顺着 undo log 的版本链找到满足其可见性的记录。</li>\n</ul>\n<p><strong>Buffer Poll：</strong> MySQL 的数据都是存在磁盘中的，那么我们要更新一条记录的时候，得先要从磁盘读取该记录，然后在内存中修改这条记录。修改完这条记录后将记录放入缓存中，下次有查询语句命中了这条记录，直接读取缓存中的记录，就不需要从磁盘获取数据了。</p>\n<ul>\n<li>当读取数据时，如果数据存在于 Buffer Pool 中，客户端就会直接读取 Buffer Pool 中的数据，否则再去磁盘中读取。</li>\n<li>当修改数据时，如果数据存在于 Buffer Pool 中，那直接修改 Buffer Pool 中数据所在的页，然后将其页设置为脏页（该页的内存数据和磁盘上的数据已经不一致），为了减少磁盘I/O，不会立即将脏页写入磁盘，后续由后台线程选择一个合适的时机将脏页写入到磁盘。</li>\n</ul>\n<p><strong>redo log：</strong> Buffer Pool 是提高了读写效率没错，但是问题来了，Buffer Pool 是基于内存的，而内存总是不可靠，万一断电重启，还没来得及落盘的脏页数据就会丢失。\n为了防止断电导致数据丢失的问题，当有一条记录需要更新的时候，InnoDB 引擎就会先更新内存（同时标记为脏页），然后将本次对这个页的修改以 redo log 的形式记录下来，这个时候更新就算完成了。</p>\n<p><strong>bin log：</strong> MySQL 在完成一条更新操作后，Server 层还会生成一条 binlog，等之后事务提交的时候，会将该事物执行过程中产生的所有 binlog 统一写 入 binlog 文件。\nbinlog 文件是记录了所有数据库表结构变更和表数据修改的日志，不会记录查询类的操作，比如 SELECT 和 SHOW 操作。</p>\n</blockquote>\n<h2 id=\"15bin-log和redo-log有什么区别\">15、bin log和redo log有什么区别</h2>\n<ul>\n<li>\n<p>binlog 是 MySQL 的 Server 层实现的日志，所有存储引擎都可以使用；redo log 是 Innodb 存储引擎实现的日志；</p>\n</li>\n<li>\n<p>binlog 是追加写，写满一个文件，就创建一个新的文件继续写，不会覆盖以前的日志，保存的是全量的日志。redo log 是循环写，日志空间大小是固定，全部写满就从头开始，保存未被刷入磁盘的脏页日志。</p>\n</li>\n<li>\n<p><code>bin log</code>是逻辑日志，记录的是SQL语句的原始逻辑；<code>redo log</code>是物理日志，记录的是在某个数据页上做了什么修改。</p>\n</li>\n</ul>\n<h2 id=\"16讲一下mysql架构\">16、讲一下MySQL架构</h2>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/3feecf7a27754947b91011da413669b4.png\">\n</p>\n<p>MySQL主要分为 Server 层和存储引擎层：</p>\n<ul>\n<li><strong>Server 层</strong>：主要包括连接器、查询缓存、分析器、优化器、执行器等，所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图，函数等，还有一个通用的日志模块 binglog 日志模块。</li>\n<li><strong>存储引擎</strong>： 主要负责数据的存储和读取。server 层通过api与存储引擎进行通信。</li>\n</ul>\n<blockquote>\n<p>Server 层负责建立连接、分析和执行 SQL。MySQL 大多数的核心功能模块都在这实现，主要包括连接器，查询缓存、解析器、预处理器、优化器、执行器等。另外，所有的内置函数（如日期、时间、数学和加密函数等）和所有跨存储引擎的功能（如存储过程、触发器、视图等。）都在 Server 层实现。</p>\n<p>存储引擎层负责数据的存储和提取。支持 InnoDB、MyISAM、Memory 等多个存储引擎，不同的存储引擎共用一个 Server 层。现在最常用的存储引擎是 InnoDB，从 MySQL 5.5 版本开始， InnoDB 成为了 MySQL 的默认存储引擎。我们常说的索引数据结构，就是由存储引擎层实现的，不同的存储引擎支持的索引类型也不相同，比如 InnoDB 支持索引类型是 B+树 ，且是默认使用，也就是说在数据表中创建的主键索引和二级索引默认使用的是 B+ 树索引。</p>\n</blockquote>\n<p><strong>Server 层基本组件</strong></p>\n<ul>\n<li><strong>连接器：</strong> 当客户端连接 MySQL 时，server层会对其进行身份认证和权限校验。</li>\n<li><strong>查询缓存:</strong> 执行查询语句的时候，会先查询缓存，先校验这个 sql 是否执行过，如果有缓存这个 sql，就会直接返回给客户端，如果没有命中，就会执行后续的操作。</li>\n<li><strong>分析器:</strong> 没有命中缓存的话，SQL 语句就会经过分析器，主要分为两步，词法分析和语法分析，先看 SQL 语句要做什么，再检查 SQL 语句语法是否正确。</li>\n<li><strong>优化器：</strong> 优化器对查询进行优化，包括重写查询、决定表的读写顺序以及选择合适的索引等，生成执行计划。</li>\n<li><strong>执行器：</strong> 首先执行前会校验该用户有没有权限，如果没有权限，就会返回错误信息，如果有权限，就会根据执行计划去调用引擎的接口，返回结果。</li>\n</ul>\n<h2 id=\"17分库分表\">17、分库分表</h2>\n<p>当单表的数据量达到1000W或100G以后，优化索引、添加从库等可能对数据库性能提升效果不明显，此时就要考虑对其进行切分了。切分的目的就在于减少数据库的负担，缩短查询的时间。</p>\n<p>数据切分可以分为两种方式：垂直划分和水平划分。</p>\n<p><strong>垂直划分</strong></p>\n<p>垂直划分数据库是根据业务进行划分，例如购物场景，可以将库中涉及商品、订单、用户的表分别划分出成一个库，通过降低单库的大小来提高性能。同样的，分表的情况就是将一个大表根据业务功能拆分成一个个子表，例如商品基本信息和商品描述，商品基本信息一般会展示在商品列表，商品描述在商品详情页，可以将商品基本信息和商品描述拆分成两张表。</p>\n<p>\n<img src=\"http://110.41.141.141:9000/weblog/weblog/a5ceca78b476467ab019021543fa4244.png\">\n</p>\n<p><strong>优点</strong>：行记录变小，数据页可以存放更多记录，在查询时减少I/O次数。</p>\n<p><strong>缺点</strong>：</p>\n<ul>\n<li>主键出现冗余，需要管理冗余列；</li>\n<li>会引起表连接JOIN操作，可以通过在业务服务器上进行join来减少数据库压力；</li>\n<li>依然存在单表数据量过大的问题。</li>\n</ul>\n<p><strong>水平划分</strong></p>\n<p>水平划分是根据一定规则，例如时间或id序列值等进行数据的拆分。比如根据年份来拆分不同的数据库。每个数据库结构一致，但是数据得以拆分，从而提升性能。\n<img src=\"http://110.41.141.141:9000/weblog/weblog/72decfe442834c83aa1c8c3480fb85ef.png\">\n</p>\n<p><strong>优点</strong>：单库（表）的数据量得以减少，提高性能；切分出的表结构相同，程序改动较少。</p>\n<p><strong>缺点</strong>：</p>\n<ul>\n<li>分片事务一致性难以解决</li>\n<li>跨节点<code>join</code>性能差，逻辑复杂</li>\n<li>数据分片在扩容时需要迁移</li>\n</ul>\n<h2 id=\"18什么是分区表\">18、什么是分区表</h2>\n<p>分区是把一张表的数据分成N多个区块。分区表是一个独立的逻辑表，但是底层由多个物理子表组成。</p>\n<p>当查询条件的数据分布在某一个分区的时候，查询引擎只会去某一个分区查询，而不是遍历整个表。在管理层面，如果需要删除某一个分区的数据，只需要删除对应的分区即可。</p>\n<p>分区一般都是放在单机里的，用的比较多的是时间范围分区，方便归档。只不过分库分表需要代码实现，分区则是mysql内部实现。分库分表和分区并不冲突，可以结合使用。</p>\n<h2 id=\"19分区表类型\">19、分区表类型</h2>\n<p><strong>range分区</strong>，按照范围分区。比如按照时间范围分区</p>\n<pre><code class=\"language-sql\">CREATE TABLE test_range_partition(\n       id INT auto_increment,\n       createdate DATETIME,\n       primary key (id,createdate)\n   ) \n   PARTITION BY RANGE (TO_DAYS(createdate) ) (\n      PARTITION p201801 VALUES LESS THAN ( TO_DAYS('20180201') ),\n      PARTITION p201802 VALUES LESS THAN ( TO_DAYS('20180301') ),\n      PARTITION p201803 VALUES LESS THAN ( TO_DAYS('20180401') ),\n      PARTITION p201804 VALUES LESS THAN ( TO_DAYS('20180501') ),\n      PARTITION p201805 VALUES LESS THAN ( TO_DAYS('20180601') ),\n      PARTITION p201806 VALUES LESS THAN ( TO_DAYS('20180701') ),\n      PARTITION p201807 VALUES LESS THAN ( TO_DAYS('20180801') ),\n      PARTITION p201808 VALUES LESS THAN ( TO_DAYS('20180901') ),\n      PARTITION p201809 VALUES LESS THAN ( TO_DAYS('20181001') ),\n      PARTITION p201810 VALUES LESS THAN ( TO_DAYS('20181101') ),\n      PARTITION p201811 VALUES LESS THAN ( TO_DAYS('20181201') ),\n      PARTITION p201812 VALUES LESS THAN ( TO_DAYS('20190101') )\n   );\n</code></pre>\n<p>在<code>/var/lib/mysql/data/</code>可以找到对应的数据文件，每个分区表都有一个使用#分隔命名的表文件：</p>\n<pre><code class=\"language-sql\">   -rw-r----- 1 MySQL MySQL    65 Mar 14 21:47 db.opt\n   -rw-r----- 1 MySQL MySQL  8598 Mar 14 21:50 test_range_partition.frm\n   -rw-r----- 1 MySQL MySQL 98304 Mar 14 21:50 test_range_partition#P#p201801.ibd\n   -rw-r----- 1 MySQL MySQL 98304 Mar 14 21:50 test_range_partition#P#p201802.ibd\n   -rw-r----- 1 MySQL MySQL 98304 Mar 14 21:50 test_range_partition#P#p201803.ibd\n...\n</code></pre>\n<p><strong>list分区</strong></p>\n<p>list分区和range分区相似，主要区别在于list是枚举值列表的集合，range是连续的区间值的集合。对于list分区，分区字段必须是已知的，如果插入的字段不在分区时的枚举值中，将无法插入。</p>\n<pre><code class=\"language-sql\">create table test_list_partiotion\n   (\n       id int auto_increment,\n       data_type tinyint,\n       primary key(id,data_type)\n   )partition by list(data_type)\n   (\n       partition p0 values in (0,1,2,3,4,5,6),\n       partition p1 values in (7,8,9,10,11,12),\n       partition p2 values in (13,14,15,16,17)\n   );\n</code></pre>\n<p><strong>hash分区</strong></p>\n<p>可以将数据均匀地分布到预先定义的分区中。</p>\n<pre><code class=\"language-sql\">create table test_hash_partiotion\n   (\n       id int auto_increment,\n       create_date datetime,\n       primary key(id,create_date)\n   )partition by hash(year(create_date)) partitions 10;\n</code></pre>\n<h2 id=\"20分区的问题\">20、分区的问题</h2>\n<ul>\n<li>\n<p>打开和锁住所有底层表的成本可能很高。当查询访问分区表时，MySQL 需要打开并锁住所有的底层表，这个操作在分区过滤之前发生，所以无法通过分区过滤来降低此开销，会影响到查询速度。可以通过批量操作来降低此类开销，比如批量插入、<code>LOAD DATA INFILE</code>和一次删除多行数据。</p>\n</li>\n<li>\n<p>维护分区的成本可能很高。例如重组分区，会先创建一个临时分区，然后将数据复制到其中，最后再删除原分区。</p>\n</li>\n<li>\n<p>所有分区必须使用相同的存储引擎。</p>\n</li>\n</ul>\n<h2 id=\"21查询语句执行流程\">21、查询语句执行流程</h2>\n<p>查询语句的执行流程如下：权限校验、查询缓存、分析器、优化器、权限校验、执行器、引擎。</p>\n<p>举个例子，查询语句如下：</p>\n<pre><code class=\"language-sql\">select * from user where id &gt; 1 and name = '大彬';\n</code></pre>\n<ul>\n<li>\n<p>首先检查权限，没有权限则返回错误；</p>\n</li>\n<li>\n<p>MySQL8.0以前会查询缓存，缓存命中则直接返回，没有则执行下一步；</p>\n</li>\n<li>\n<p>词法分析和语法分析。提取表名、查询条件，检查语法是否有错误；</p>\n</li>\n<li>\n<p>两种执行方案，先查 <code>id &gt; 1</code> 还是 <code>name = '大彬'</code>，优化器根据自己的优化算法选择执行效率最好的方案；</p>\n</li>\n<li>\n<p>校验权限，有权限就调用数据库引擎接口，返回引擎的执行结果。</p>\n</li>\n</ul>\n<h2 id=\"22更新语句执行过程\">22、更新语句执行过程</h2>\n<p>更新语句执行流程如下：分析器、权限校验、执行器、引擎、<code>redo log</code>（<code>prepare</code>状态）、<code>binlog</code>、<code>redo log</code>（<code>commit</code>状态）</p>\n<p>举个例子，更新语句如下：</p>\n<pre><code class=\"language-sql\">update user set name = '大彬' where id = 1;\n</code></pre>\n<ol>\n<li>先查询到 id 为1的记录，有缓存会使用缓存。</li>\n<li>拿到查询结果，将 name 更新为大彬，然后调用引擎接口，写入更新数据，innodb 引擎将数据保存在内存中，同时记录<code>redo log</code>，此时<code>redo log</code>进入 <code>prepare</code>状态。</li>\n<li>执行器收到通知后记录<code>binlog</code>，然后调用引擎接口，提交<code>redo log</code>为<code>commit</code>状态。</li>\n<li>更新完成。</li>\n</ol>\n<p>为什么记录完<code>redo log</code>，不直接提交，而是先进入<code>prepare</code>状态？</p>\n<p>假设先写<code>redo log</code>直接提交，然后写<code>binlog</code>，写完<code>redo log</code>后，机器挂了，<code>binlog</code>日志没有被写入，那么机器重启后，这台机器会通过<code>redo log</code>恢复数据，但是这个时候<code>binlog</code>并没有记录该数据，后续进行机器备份的时候，就会丢失这一条数据，同时主从同步也会丢失这一条数据。</p>\n<h2 id=\"23exist和in的区别\">23、exist和in的区别</h2>\n<blockquote>\n<pre><code class=\"language-sql\">SELECT * FROM A WHERE cc IN (SELECT cc FROM B)；\n</code></pre>\n<pre><code class=\"language-sql\">SELECT * FROM A WHERE EXISTS (SELECT cc FROM B WHERE B.cc = A.cc)；\n</code></pre>\n<p><code>IN</code>是在A中选择一条记录，然后在B中查找该记录是否存在。<code>EXISTS</code>是在B中选择一条符合条件的记录，然后在A中查找该记录是否存在。</p>\n</blockquote>\n<ul>\n<li><strong>In</strong>：确定给定的值是否与子查询或列表中的值相匹配。in在查询的时候，首先查询子查询的表，然后将内表和外表做一个笛卡尔积，然后按照条件进行筛选。所以相对内表比较小的时候，in的速度较快。</li>\n<li><strong>exists</strong>：指定一个子查询，检测行的存在。循环遍历外表，然后看外表中的记录有没有和内表的数据一样的。匹配上就将结果放入结果集中。</li>\n</ul>\n<p><strong>子查询的表比较大的时候</strong>，使用<code>exists</code>可以有效减少总的循环次数来提升速度；<strong>当外查询的表比较大的时候</strong>，使用<code>in</code>可以有效减少对外查询表循环遍历来提升速度。</p>\n<h2 id=\"24mysql中int10和char10的区别\">24、MySQL中int(10)和char(10)的区别</h2>\n<p>int(10)中的10表示的是<strong>显示数据</strong>的长度，而char(10)表示的是<strong>存储数据</strong>的长度。</p>\n<h2 id=\"25truncatedelete与drop区别\">25、truncate、delete与drop区别</h2>\n<p><strong>相同点：</strong></p>\n<ol>\n<li><code>truncate</code>和不带<code>where</code>子句的<code>delete</code>、以及<code>drop</code>都会删除表内的数据。</li>\n<li><strong><code>drop</code>、<code>truncate</code>都是<code>DDL</code>语句（数据定义语言）</strong>，执行后会自动提交。</li>\n</ol>\n<p><strong>不同点：</strong></p>\n<ol>\n<li>truncate 和 delete 只删除数据不删除表的结构；<strong>drop 语句将删除表的结构、被依赖的约束、触发器、索引</strong>；</li>\n<li>一般来说，<strong>执行速度: drop &gt; truncate &gt; delete</strong>。</li>\n</ol>\n<h2 id=\"26having和where区别\">26、having和where区别？</h2>\n<ul>\n<li>二者作用的对象不同，<code>where</code>子句作用于表和视图，<code>having</code>作用于组。</li>\n<li><code>where</code>在数据分组前进行过滤，<code>having</code>在数据分组后进行过滤。</li>\n</ul>\n<h2 id=\"27什么是mysql主从同步\">27、什么是MySQL主从同步？</h2>\n<p>主从同步使得数据可以从一个数据库服务器复制到其他服务器上，在复制数据时，一个服务器充当主服务器（<code>master</code>），其余的服务器充当从服务器（<code>slave</code>）。</p>\n<p>因为复制是异步进行的，所以从服务器不需要一直连接着主服务器，从服务器甚至可以通过拨号断断续续地连接主服务器。通过配置文件，可以指定复制所有的数据库，某个数据库，甚至是某个数据库上的某个表。</p>\n<hr />\n<p><em><strong>MySQL 的主从复制</strong></em>依赖于 binlog ，也就是记录 MySQL 上的所有变化并以二进制形式保存在磁盘上。复制的过程就是将 binlog 中的数据从主库传输到从库上。</p>\n<p>这个过程一般是异步的，也就是主库上执行事务操作的线程不会等待复制 binlog 的线程同步完成。\n<img src=\"http://110.41.141.141:9000/weblog/weblog/69437c3c7def4cc1bec0f1de78dc13d3.png\">\n</p>\n<p>MySQL 集群的主从复制过程梳理成 3 个阶段：</p>\n<ul>\n<li><strong>写入 Binlog</strong>：主库写 binlog 日志，提交事务，并更新本地存储数据。</li>\n<li><strong>同步 Binlog</strong>：把 binlog 复制到所有从库上，每个从库把 binlog 写到暂存日志中。</li>\n<li><strong>回放 Binlog</strong>：回放 binlog，并更新存储引擎中的数据。</li>\n</ul>\n<p>具体详细过程如下：</p>\n<ul>\n<li>MySQL 主库在收到客户端提交事务的请求之后，会先写入 binlog，再提交事务，更新存储引擎中的数据，事务提交完成后，返回给客户端“操作成功”的响应。</li>\n<li>从库会创建一个专门的 I/O 线程，连接主库的 log dump 线程，来接收主库的 binlog 日志，再把 binlog 信息写入 relay log 的中继日志里，再返回给主库“复制成功”的响应。</li>\n<li>从库会创建一个用于回放 binlog 的线程，去读 relay log 中继日志，然后回放 binlog 更新存储引擎中的数据，最终实现主从的数据一致性。</li>\n</ul>\n<h2 id=\"28为什么要做主从同步\">28、为什么要做主从同步？</h2>\n<ol>\n<li><strong>读写分离</strong>，使数据库能支撑更大的并发。</li>\n<li><strong>在主服务器上生成实时数据，而在从服务器上分析这些数据</strong>，从而提高主服务器的性能。</li>\n<li><strong>数据备份</strong>，保证数据的安全。</li>\n</ol>\n<h2 id=\"29乐观锁和悲观锁是什么\">29、乐观锁和悲观锁是什么</h2>\n<p>数据库中的并发控制是确保在多个事务同时存取数据库中同一数据时不破坏事务的隔离性和一致性以及数据库的一致性。乐观锁和悲观锁是并发控制主要采用的技术手段。</p>\n<ul>\n<li><strong>悲观锁</strong>：假定会发生并发冲突，会对操作的数据进行加锁，直到提交事务，才会释放锁，其他事务才能进行修改。实现方式：使用数据库中的锁机制。</li>\n<li><strong>乐观锁</strong>：假设不会发生并发冲突，只在提交操作时检查数据是否被修改过。给表增加<code>version</code>字段，在修改提交之前检查<code>version</code>与原来取到的<code>version</code>值是否相等，若相等，表示数据没有被修改，可以更新，否则，数据为脏数据，不能更新。实现方式：乐观锁一般使用版本号机制或<code>CAS</code>算法实现。</li>\n</ul>\n<h2 id=\"30用过processlist吗\">30、用过processlist吗</h2>\n<p><code>show processlist</code> 或 <code>show full processlist</code> 可以查看当前 MySQL 是否有压力，正在运行的<code>SQL</code>，有没有慢<code>SQL</code>正在执行。返回参数如下：</p>\n<ol>\n<li>\n<p><strong>id</strong>：线程ID，可以用<code>kill id</code>杀死某个线程</p>\n</li>\n<li>\n<p><strong>db</strong>：数据库名称</p>\n</li>\n<li>\n<p><strong>user</strong>：数据库用户</p>\n</li>\n<li>\n<p><strong>host</strong>：数据库实例的IP</p>\n</li>\n<li>\n<p><strong>command</strong>：当前执行的命令，比如<code>Sleep</code>，<code>Query</code>，<code>Connect </code>等</p>\n</li>\n<li>\n<p><strong>time</strong>：消耗时间，单位秒</p>\n</li>\n<li>\n<p>state</p>\n<p>：执行状态，主要有以下状态：</p>\n<ul>\n<li>Sleep，线程正在等待客户端发送新的请求</li>\n<li>Locked，线程正在等待锁</li>\n<li>Sending data，正在处理<code>SELECT</code>查询的记录，同时把结果发送给客户端</li>\n<li>Kill，正在执行<code>kill</code>语句，杀死指定线程</li>\n<li>Connect，一个从节点连上了主节点</li>\n<li>Quit，线程正在退出</li>\n<li>Sorting for group，正在为<code>GROUP BY</code>做排序</li>\n<li>Sorting for order，正在为<code>ORDER BY</code>做排序</li>\n</ul>\n</li>\n<li>\n<p><strong>info</strong>：正在执行的<code>SQL</code>语句</p>\n</li>\n</ol>\n<h2 id=\"31mysql查询-limit-100010-和limit-10-速度一样快吗\">31、MySQL查询 limit 1000,10 和limit 10 速度一样快吗</h2>\n<p>两种查询方式。对应 <code>limit offset, size</code> 和 <code>limit size</code> 两种方式。</p>\n<p>而其实 <code>limit size</code> ，相当于 <code>limit 0, size</code>。也就是从0开始取size条数据。</p>\n<p>也就是说，两种方式的<strong>区别在于offset是否为0。</strong></p>\n<p>先来看下limit sql的内部执行逻辑。</p>\n<p>MySQL内部分为<strong>server层</strong>和<strong>存储引擎层</strong>。一般情况下存储引擎都用innodb。</p>\n<p>server层有很多模块，其中需要关注的是<strong>执行器</strong>是用于跟存储引擎打交道的组件。</p>\n<p>执行器可以通过调用存储引擎提供的接口，将一行行数据取出，当这些数据完全符合要求（比如满足其他where条件），则会放到<strong>结果集</strong>中，最后返回给调用mysql的<strong>客户端</strong>。</p>\n<p>以主键索引的limit执行过程为例：</p>\n<p>执行<code>select * from xxx order by id limit 0, 10;</code>，select后面带的是<strong>星号</strong>，也就是要求获得行数据的<strong>所有字段信息。</strong></p>\n<p>server层会调用innodb的接口，在innodb里的主键索引中获取到第0到10条<strong>完整行数据</strong>，依次返回给server层，并放到server层的结果集中，返回给客户端。</p>\n<p>把offset搞大点，比如执行的是：<code>select * from xxx order by id limit 500000, 10;</code></p>\n<p>server层会调用innodb的接口，由于这次的offset=500000，会在innodb里的主键索引中获取到第0到（500000 + 10）条<strong>完整行数据</strong>，<strong>返回给server层之后根据offset的值挨个抛弃，最后只留下最后面的size条</strong>，也就是10条数据，放到server层的结果集中，返回给客户端。</p>\n<p>可以看出，当offset非0时，server层会从引擎层获取到<strong>很多无用的数据</strong>，而获取的这些无用数据都是要耗时的。</p>\n<p>因此，mysql查询中 limit 1000,10 会比 limit 10 更慢。原因是 limit 1000,10 会取出1000+10条数据，并抛弃前1000条，这部分耗时更大。</p>\n<h2 id=\"32深分页怎么优化\">32、深分页怎么优化</h2>\n<p>还是以上面的SQL为空：<code>select * from xxx order by id limit 500000, 10;</code></p>\n<p><strong>方法一</strong>：</p>\n<p>从上面的分析可以看出，当offset非常大时，server层会从引擎层获取到很多无用的数据，而当select后面是*号时，就需要拷贝完整的行信息，<strong>拷贝完整数据</strong>相比<strong>只拷贝行数据里的其中一两个列字段</strong>更耗费时间。</p>\n<p>因为前面的offset条数据最后都是不要的，没有必要拷贝完整字段，所以可以将sql语句修改成：</p>\n<pre><code class=\"language-sql\">select * from xxx  where id &gt;=(select id from xxx order by id limit 500000, 1) order by id limit 10;\n</code></pre>\n<p>先执行子查询 <code>select id from xxx by id limit 500000, 1</code>, 这个操作，其实也是将在innodb中的主键索引中获取到<code>500000+1</code>条数据，然后server层会抛弃前500000条，只保留最后一条数据的id。</p>\n<p>但不同的地方在于，在返回server层的过程中，只会拷贝数据行内的id这一列，而不会拷贝数据行的所有列，当数据量较大时，这部分的耗时还是比较明显的。</p>\n<p>在拿到了上面的id之后，假设这个id正好等于500000，那sql就变成了</p>\n<pre><code class=\"language-sql\">select * from xxx  where id &gt;=500000 order by id limit 10;\n</code></pre>\n<p>这样innodb再走一次<strong>主键索引</strong>，通过B+树快速定位到id=500000的行数据，时间复杂度是lg(n)，然后向后取10条数据。</p>\n<p><strong>方法二：</strong></p>\n<p>将所有的数据<strong>根据id主键进行排序</strong>，然后分批次取，将当前批次的最大id作为下次筛选的条件进行查询。</p>\n<pre><code class=\"language-sql\">select * from xxx where id &gt; start_id order by id limit 10;\n</code></pre>\n<p>通过主键索引，每次定位到start_id的位置，然后往后遍历10个数据，这样不管数据多大，查询性能都较为稳定。</p>\n<ul>\n<li>\n<p>子查询+索引</p>\n<blockquote>\n<p>思路：通过将 select * 转变为 select id，把符合条件的 id 筛选出来后，最后通过嵌套查询的方式按顺序取出 id 对应的行。</p>\n<pre><code class=\"language-sql\">-- 优化前\nselect *\nfrom people\norder by create_time desc\nlimit 5000000, 10;\n\n-- 优化后\nselect a.*\nfrom people a\ninner join(\n select id\n from people\n order by create_time desc\n limit 5000000, 10\n) b ON a.id = b.id;\n</code></pre>\n</blockquote>\n</li>\n<li>\n<p>联合索引</p>\n<blockquote>\n<p>刚才我们优化后的 SQL 语句如下，是没有 where 条件的。</p>\n<pre><code class=\"language-sql\">select a.*\nfrom people a\ninner join(\n select id\n from people\n order by create_time desc\n limit 5000010, 10\n) b ON a.id = b.id;\n</code></pre>\n</blockquote>\n<blockquote>\n<p>为了更接近现实场景，假设我们要把 status = 1 的人筛出来，SQL 就要这么写：</p>\n<pre><code class=\"language-sql\">select a.*\nfrom people a\ninner join(\n select id\n from people\n where status=1\n order by create_time desc\n limit 5000010, 10\n) b ON a.id = b.id;\n</code></pre>\n</blockquote>\n<blockquote>\n<p>为了加速这个查询，我们可以创建一个 create_time 和 status 的联合索引，对应的 SQL 语句是：</p>\n<pre><code class=\"language-sql\">alter table people add index create_time_status(create_time, status);\n</code></pre>\n</blockquote>\n</li>\n<li>\n<p>where id &gt; start_id</p>\n<blockquote>\n<p>将上一页的查询结果中的最大 id 作为下一页查询的 where 条件，这样可以大幅减少扫描行数，提高查询性能。</p>\n<pre><code class=\"language-sql\">select a.*\nfrom people a\ninner join(\n select id\n from people\n where status=1\n       and id &gt; ${id}\n order by create_time desc\n limit 10\n) b ON a.id = b.id;\n</code></pre>\n</blockquote>\n</li>\n</ul>\n<h2 id=\"33高度为3的b树可以存放多少数据\">33、高度为3的B+树，可以存放多少数据</h2>\n<p>InnoDB存储引擎有自己的最小储存单元——页（Page）。</p>\n<p>查询InnoDB页大小的命令如下：</p>\n<pre><code>mysql&gt; show global status like 'innodb_page_size';\n+------------------+-------+\n| Variable_name    | Value |\n+------------------+-------+\n| Innodb_page_size | 16384 |\n+------------------+-------+\n</code></pre>\n<p>可以看出 <strong>innodb 默认的一页大小为 16384B = 16384/1024 = 16kb</strong>。</p>\n<p>在MySQL中，B+树一个节点的大小设为一页或页的倍数最为合适。因为如果一个节点的大小 &lt; 1页，那么读取这个节点的时候其实读取的还是一页，这样就造成了资源的浪费。</p>\n<p>B+树中<strong>非叶子节点存的是key + 指针</strong>；<strong>叶子节点存的是数据行</strong>。</p>\n<p>对于叶子节点，如果一行数据大小为1k，那么一页就能存16条数据。</p>\n<p>对于非叶子节点，如果key使用的是bigint，则为8字节，指针在MySQL中为6字节，一共是14字节，则16k能存放 16 * 1024 / 14 = 1170 个索引指针。</p>\n<p>于是可以算出，对于一颗高度为2的B+树，根节点存储索引指针节点，那么它有1170个叶子节点存储数据，每个叶子节点可以存储16条数据，一共 1170 x 16 = 18720 条数据。而对于高度为3的B+树，就可以存放 1170 x 1170 x 16 = 21902400 条数据（<strong>两千多万条数据</strong>），也就是对于两千多万条的数据，我们只需要<strong>高度为3</strong>的B+树就可以完成，通过主键查询只需要3次IO操作就能查到对应数据。</p>\n<p>所以在 InnoDB 中B+树高度一般为3层时，就能满足千万级的数据存储。</p>\n<h2 id=\"34mysql单表多大进行分库分表\">34、MySQL单表多大进行分库分表</h2>\n<p>目前主流的有两种说法：</p>\n<ol>\n<li>MySQL 单表数据量大于 2000 万行，性能会明显下降，考虑进行分库分表。</li>\n<li>阿里巴巴《Java 开发手册》提出单表行数超过 500 万行或者单表容量超过 2GB，才推荐进行分库分表。</li>\n</ol>\n<p>事实上，这个数值和实际记录的条数无关，而与 MySQL 的配置以及机器的硬件有关。因为MySQL为了提高性能，会将表的索引装载到内存中。在InnoDB buffer size 足够的情况下，其能完成全加载进内存，查询不会有问题。但是，当单表数据库到达某个量级的上限时，导致内存无法存储其索引，使得之后的 SQL 查询会产生磁盘 IO，从而导致性能下降。当然，这个还有具体的表结构的设计有关，最终导致的问题都是内存限制。</p>\n<p>因此，对于分库分表，需要结合实际需求，不宜过度设计，在项目一开始不采用分库与分表设计，而是随着业务的增长，在无法继续优化的情况下，再考虑分库与分表提高系统的性能。对此，阿里巴巴《Java 开发手册》补充到：如果预计三年后的数据量根本达不到这个级别，请不要在创建表时就分库分表。</p>\n<p>至于MySQL单表多大进行分库分表，应当根据机器资源进行评估。</p>\n<h2 id=\"35大表查询慢怎么优化\">35、大表查询慢怎么优化</h2>\n<p>某个表有近千万数据，查询比较慢，如何优化？</p>\n<p>当MySQL单表记录数过大时，数据库的性能会明显下降，一些常见的优化措施如下：</p>\n<ul>\n<li><strong>合理建立索引</strong>。在合适的字段上建立索引，例如在WHERE和ORDER BY命令上涉及的列建立索引，可根据EXPLAIN来查看是否用了索引还是全表扫描</li>\n<li><strong>索引优化，SQL优化</strong>。索引要符合最左匹配原则等，参考：<a href=\"https://topjavaer.cn/database/mysql.html#什么是覆盖索引\" ref=\"nofollow\" target=\"_blank\">https://topjavaer.cn/database/mysql.html#什么是覆盖索引open in new window</a><span><svg xmlns=\"http://www.w3.org/2000/svg\" class=\"inline ml-1\" style=\"color: #aaa;\" aria-hidden=\"true\" focusable=\"false\" x=\"0px\" y=\"0px\" viewBox=\"0 0 100 100\" width=\"15\" height=\"15\" class=\"icon outbound\"><path fill=\"currentColor\" d=\"M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z\"></path> <polygon fill=\"currentColor\" points=\"45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9\"></polygon></svg> <span class=\"sr-only\"></span></span></li>\n<li><strong>建立分区</strong>。对关键字段建立水平分区，比如时间字段，若查询条件往往通过时间范围来进行查询，能提升不少性能</li>\n<li><strong>利用缓存</strong>。利用Redis等缓存热点数据，提高查询效率</li>\n<li><strong>限定数据的范围</strong>。比如：用户在查询历史信息的时候，可以控制在一个月的时间范围内</li>\n<li><strong>读写分离</strong>。经典的数据库拆分方案，主库负责写，从库负责读</li>\n<li>通过<strong>分库分表</strong>的方式进行优化，主要有垂直拆分和水平拆分</li>\n<li><strong>数据异构到es</strong></li>\n<li><strong>冷热数据分离</strong>。几个月之前不常用的数据放到冷库中，最新的数据或比较新的数据放到热库中</li>\n</ul>\n<blockquote>\n<ul>\n<li>\n<p>通过 explain 执行结果，查看 sql 是否走索引，如果不走索引，考虑增加索引。</p>\n</li>\n<li>\n<p>可以通过建立联合索引，实现覆盖索引优化，减少回表，使用联合索引符合最左匹配原则，不然会索引失效</p>\n</li>\n<li>\n<p>避免索引失效，比如不要用左模糊匹配、函数计算、表达式计算等等。</p>\n</li>\n<li>\n<p>联表查询最好要以小表驱动大表，并且被驱动表的字段要有索引，当然最好通过冗余字段的设计，避免联表查询。</p>\n</li>\n<li>\n<p>针对 limit n,y 深分页的查询优化，可以把Limit查询转换成某个位置的查询：select * from tb_sku where id&gt;20000 limit 10，该方案适用于主键自增的表，</p>\n</li>\n<li>\n<p>将字段多的表分解成多个表，有些字段使用频率高，有些低，数据量大时，会由于使用频率低的存在而变慢，可以考虑分开</p>\n</li>\n</ul>\n</blockquote>\n<h2 id=\"36说说count1count和count字段名的区别\">36、说说count(1)、count(*)和count(字段名)的区别</h2>\n<blockquote>\n<ul>\n<li>count(1)、 count(*)、 count(主键字段)在执行的时候，如果表里存在二级索引，优化器就会选择二级索引进行扫描。</li>\n<li>所以，如果要执行 count(1)、 count(*)、 count(主键字段) 时，尽量在数据表上建立二级索引，这样优化器会自动采用 key_len 最小的二级索引进行扫描，相比于扫描主键索引效率会高一些。</li>\n<li>再来，就是不要使用 count(字段) 来统计记录个数，因为它的效率是最差的，会采用全表扫描的方式来统计。如果你非要统计表中该字段不为 NULL 的记录个数，建议给这个字段建立一个二级索引。</li>\n</ul>\n</blockquote>\n<p>嗯，先说说count(1) and count(字段名)的区别。</p>\n<p>两者的主要区别是</p>\n<ol>\n<li>count(1) 会统计表中的所有的记录数，包含字段为null 的记录。</li>\n<li>count(字段名) 会统计该字段在表中出现的次数，忽略字段为null 的情况。即不统计字段为null 的记录。</li>\n</ol>\n<p>接下来看看三者之间的区别。</p>\n<p>执行效果上：</p>\n<ul>\n<li>count(*)包括了所有的列，相当于行数，在统计结果的时候，<strong>不会忽略列值为NULL</strong></li>\n<li>count(1)包括了忽略所有列，用1代表代码行，在统计结果的时候，<strong>不会忽略列值为NULL</strong></li>\n<li>count(字段名)只包括列名那一列，在统计结果的时候，会忽略列值为空（这里的空不是只空字符串或者0，而是表示null）的计数，<strong>即某个字段值为NULL时，不统计</strong>。</li>\n</ul>\n<p>执行效率上：</p>\n<ul>\n<li>列名为主键，count(字段名)会比count(1)快</li>\n<li>列名不为主键，count(1)会比count(列名)快</li>\n<li>如果表多个列并且没有主键，则 count(1) 的执行效率优于 count(*)</li>\n<li>如果有主键，则 select count(主键)的执行效率是最优的</li>\n<li>如果表只有一个字段，则 select count(*)最优。</li>\n</ul>\n<p><strong>COUNT(<code>*</code>)是SQL92定义的标准统计行数的语法，效率高，MySQL对它进行了很多优化，MyISAM中会直接把表的总行数单独记录下来供COUNT(*)查询，而InnoDB则会在扫表的时候选择最小的索引来降低成本</strong>。</p>\n<h2 id=\"37mysql中datetime-和-timestamp有什么区别\">37、MySQL中DATETIME 和 TIMESTAMP有什么区别</h2>\n<p>嗯，<code>TIMESTAMP</code>和<code>DATETIME</code>都可以用来存储时间，它们主要有以下区别：</p>\n<p>1.表示范围</p>\n<ul>\n<li>DATETIME：1000-01-01 00:00:00.000000 到 9999-12-31 23:59:59.999999</li>\n<li>TIMESTAMP：'1970-01-01 00:00:01.000000' UTC 到 '2038-01-09 03:14:07.999999' UTC</li>\n</ul>\n<p><code>TIMESTAMP</code>支持的时间范围比<code>DATATIME</code>要小，容易出现超出的情况。</p>\n<p>2.空间占用</p>\n<ul>\n<li>TIMESTAMP ：占 4 个字节</li>\n<li>DATETIME：在 MySQL 5.6.4 之前，占 8 个字节 ，之后版本，占 5 个字节</li>\n</ul>\n<p>3.存入时间是否会自动转换</p>\n<p><code>TIMESTAMP</code>类型在默认情况下，insert、update 数据时，<code>TIMESTAMP</code>列会自动以当前时间（<code>CURRENT_TIMESTAMP</code>）填充/更新。<code>DATETIME</code>则不会做任何转换，也不会检测时区，你给什么数据，它存什么数据。</p>\n<p>4.<code>TIMESTAMP</code>比较受时区timezone的影响以及MYSQL版本和服务器的SQL MODE的影响。因为<code>TIMESTAMP</code>存的是时间戳，在不同的时区得出的时间不一致。</p>\n<p>5.如果存进NULL，两者实际存储的值不同。</p>\n<ul>\n<li>TIMESTAMP：会自动存储当前时间 now() 。</li>\n<li>DATETIME：不会自动存储当前时间，会直接存入 NULL 值。</li>\n</ul>\n<h2 id=\"38说说为什么不建议用外键\">38、说说为什么不建议用外键</h2>\n<p>外键是一种约束，这个约束的存在，会保证表间数据的关系始终完整。外键的存在，并非全然没有优点。</p>\n<p>外键可以保证数据的完整性和一致性，级联操作方便。而且使用外键可以将数据完整性判断托付给了数据库完成，减少了程序的代码量。</p>\n<p>虽然外键能够保证数据的完整性，但是会给系统带来很多缺陷。</p>\n<p>1、并发问题。在使用外键的情况下，每次修改数据都需要去另外一个表检查数据，需要获取额外的锁。若是在高并发大流量事务场景，使用外键更容易造成死锁。</p>\n<p>2、扩展性问题。比如从<code>MySQL</code>迁移到<code>Oracle</code>，外键依赖于数据库本身的特性，做迁移可能不方便。</p>\n<p>3、不利于分库分表。在水平拆分和分库的情况下，外键是无法生效的。将数据间关系的维护，放入应用程序中，为将来的分库分表省去很多的麻烦。</p>\n<h2 id=\"39使用自增主键有什么好处\">39、使用自增主键有什么好处</h2>\n<p>自增主键可以让主键索引尽量地保持递增顺序插入，避免了页分裂，因此索引更紧凑，在查询的时候，效率也就更高。</p>\n<h2 id=\"40自增主键保存在什么地方\">40、自增主键保存在什么地方</h2>\n<p>不同的引擎对于自增值的保存策略不同：</p>\n<ul>\n<li><strong>MyISAM引擎的自增值保存在数据文件中</strong>。</li>\n<li>在MySQL8.0以前，InnoDB引擎的自增值是存在内存中。MySQL重启之后内存中的这个值就丢失了，每次重启后第一次打开表的时候，会找自增值的最大值max(id)，然后将最大值加1作为这个表的自增值；MySQL8.0版本会将自增值的变更记录在redo log中，重启时依靠redo log恢复。</li>\n</ul>\n<h2 id=\"41自增主键一定是连续的吗\">41、自增主键一定是连续的吗</h2>\n<p>不一定，有几种情况会导致自增主键不连续。</p>\n<p>1、<strong>唯一键冲突导致自增主键不连续</strong>。当我们向一个自增主键的InnoDB表中插入数据的时候，如果违反表中定义的唯一索引的唯一约束，会导致插入数据失败。此时表的自增主键的键值是会向后加1滚动的。下次再次插入数据的时候，就不能再使用上次因插入数据失败而滚动生成的键值了，必须使用新滚动生成的键值。</p>\n<p>2、<strong>事务回滚导致自增主键不连续</strong>。当我们向一个自增主键的InnoDB表中插入数据的时候，如果显式开启了事务，然后因为某种原因最后回滚了事务，此时表的自增值也会发生滚动，而接下里新插入的数据，也将不能使用滚动过的自增值，而是需要重新申请一个新的自增值。</p>\n<p>3、<strong>批量插入导致自增值不连续</strong>。MySQL有一个批量申请自增id的策略：</p>\n<ul>\n<li>语句执行过程中，第一次申请自增id，分配1个自增id</li>\n<li>1个用完以后，第二次申请，会分配2个自增id</li>\n<li>2个用完以后，第三次申请，会分配4个自增id</li>\n<li>依次类推，每次申请都是上一次的两倍（最后一次申请不一定全部使用）</li>\n</ul>\n<p>如果下一个事务再次插入数据的时候，则会基于上一个事务申请后的自增值基础上再申请。此时就出现自增值不连续的情况出现。</p>\n<p>4、<strong>自增步长不是1</strong>，也会导致自增主键不连续。</p>\n<h2 id=\"42innodb的自增值为什么不能回收利用\">42、InnoDB的自增值为什么不能回收利用</h2>\n<p><strong>主要为了提升插入数据的效率和并行度</strong>。</p>\n<p>假设有两个并行执行的事务，在申请自增值的时候，为了避免两个事务申请到相同的自增 id，肯定要加锁，然后顺序申请。</p>\n<p>假设事务 A 申请到了 id=2， 事务 B 申请到 id=3，那么这时候表 t 的自增值是 4，之后继续执行。</p>\n<p>事务 B 正确提交了，但事务 A 出现了唯一键冲突。</p>\n<p>如果允许事务 A 把自增 id 回退，也就是把表 t 的当前自增值改回 2，那么就会出现这样的情况：表里面已经有 id=3 的行，而当前的自增 id 值是 2。</p>\n<p>接下来，继续执行的其他事务就会申请到 id=2，然后再申请到 id=3。这时，就会出现插入语句报错“主键冲突”。</p>\n<p>而为了解决这个主键冲突，有两种方法：</p>\n<ul>\n<li>每次申请 id 之前，先判断表里面是否已经存在这个 id。如果存在，就跳过这个 id。但是，这个方法的成本很高。因为，本来申请 id 是一个很快的操作，现在还要再去主键索引树上判断 id 是否存在。</li>\n<li>把自增 id 的锁范围扩大，必须等到一个事务执行完成并提交，下一个事务才能再申请自增 id。这个方法的问题，就是锁的粒度太大，系统并发能力大大下降。</li>\n</ul>\n<p>可见，这两个方法都会导致性能问题。</p>\n<p>因此，InnoDB 放弃了“允许自增 id 回退”这个设计，语句执行失败也不回退自增 id。</p>\n<h2 id=\"43mysql数据如何同步到redis缓存\">43、MySQL数据如何同步到Redis缓存</h2>\n<p>有两种方案：</p>\n<p>1、通过MySQL自动同步刷新Redis，<strong>MySQL触发器+UDF函数</strong>实现。</p>\n<blockquote>\n<p>UDF函数：常见的函数类型，可以操作单个数据行，且产生一个数据行作为输出，大多数函数为这一类。</p>\n</blockquote>\n<p>过程大致如下：</p>\n<ol>\n<li>在MySQL中对要操作的数据设置触发器Trigger，监听操作</li>\n<li>客户端向MySQL中写入数据时，触发器会被触发，触发之后调用MySQL的UDF函数</li>\n<li>UDF函数可以把数据写入到Redis中，从而达到同步的效果</li>\n</ol>\n<p>2、<strong>解析MySQL的binlog</strong>，实现将数据库中的数据同步到Redis。可以通过canal实现。canal是阿里巴巴旗下的一款开源项目，基于数据库增量日志解析，提供增量数据订阅&amp;消费。</p>\n<p>canal的原理如下：</p>\n<ol>\n<li>canal模拟mysql slave的交互协议，伪装自己为mysql slave，向mysql master发送dump协议</li>\n<li>mysql master收到dump请求，开始推送binary log给canal</li>\n<li>canal解析binary log对象（原始为byte流），将数据同步写入Redis。</li>\n</ol>\n<h2 id=\"44为什么阿里java手册禁止使用存储过程\">44、为什么阿里Java手册禁止使用存储过程</h2>\n<p>先看看什么是存储过程。</p>\n<p>存储过程是在大型数据库系统中，一组为了完成特定功能的SQL 语句集，它存储在数据库中，一次编译后永久有效，用户通过指定存储过程的名字并给出参数（如果该存储过程带有参数）来执行它。</p>\n<p>存储过程主要有以下几个缺点。</p>\n<ol>\n<li><strong>存储过程难以调试</strong>。存储过程的开发一直缺少有效的 IDE 环境。SQL 本身经常很长，调试时要把句子拆开分别独立执行，非常麻烦。</li>\n<li><strong>移植性差</strong>。存储过程的移植困难，一般业务系统总会不可避免地用到数据库独有的特性和语法，更换数据库时这部分代码就需要重写，成本较高。</li>\n<li><strong>管理困难</strong>。存储过程的目录是扁平的，而不是文件系统那样的树形结构，脚本少的时候还好办，一旦多起来，目录就会陷入混乱。</li>\n<li>存储过程是<strong>只优化一次</strong>，有的时候随着数据量的增加或者数据结构的变化，原来存储过程选择的执行计划也许并不是最优的了，所以这个时候需要手动干预或者重新编译了。</li>\n</ol>\n<h2 id=\"45mysql-update-是锁行还是锁表\">45、MySQL update 是锁行还是锁表？</h2>\n<p>首先，InnoDB行锁是通过给索引上的索引项加锁来实现的，只有通过索引条件检索数据，InnoDB才使用行级锁，否则，InnoDB将使用表锁。</p>\n<ol>\n<li>当执行update语句时，where中的过滤条件列，如果用到索引，就是锁行；如果无法用索引，就是锁表。</li>\n<li>如果两个update语句同时执行，第一个先执行触发行锁，但是第二个没有索引触发表锁，因为有个行锁住了，所以还是会等待行锁释放，才能锁表。</li>\n<li>当执行insert或者delete语句时，锁行。</li>\n</ol>\n<h2 id=\"46selectfor-update会锁表还是锁行\">46、select...for update会锁表还是锁行？</h2>\n<p>如果查询条件用了索引/主键，那么<code>select ... for update</code>就会加行锁。</p>\n<p>如果是普通字段(没有索引/主键)，那么<code>select ..... for update</code>就会加表锁。</p>\n<h2 id=\"47mysql的binlog有几种格式分别有什么区别\">47、MySQL的binlog有几种格式？分别有什么区别？</h2>\n<p>有三种格式，<strong>statement，row和mixed</strong>。</p>\n<ul>\n<li>statement：<strong>每一条会修改数据的sql都会记录在binlog中</strong>。不需要记录每一行的变化，减少了binlog日志量，节约了IO，提高性能。由于sql的执行是有上下文的，因此<strong>在保存的时候需要保存相关的信息</strong>，同时还有一些使用了函数之类的语句无法被记录复制。</li>\n<li>row：<strong>不记录sql语句上下文相关信息，仅保存哪条记录被修改</strong>。记录单元为每一行的改动，由于很多操作，会导致大量行的改动(比如alter table)，因此这种模式的文件保存的信息太多，日志量太大。</li>\n<li>mixed：一种折中的方案，普通操作使用statement记录，当无法使用statement的时候使用row。</li>\n</ul>\n<h2 id=\"48阿里手册为什么禁止使用-count列名或-count常量来替代-count\">48、阿里手册为什么禁止使用 count(列名)或 count(常量)来替代 count(*)</h2>\n<p>先看下这几种方式的区别。</p>\n<p>count(主键id)：InnoDB引擎会遍历整张表，把每一行id值都取出来，返给server层。server层拿到id后，判断是不可能为空的，就按行累加，不再对每个值进行NULL判断。</p>\n<p>count(常量)：InnoDB引擎会遍历整张表，但不取值。server层对于返回的每一行，放一个常量进去，判断是不可能为空的，按行累加，不再对每个值进行NULL判断。count(常量)比count(主键id)执行的要快，因为从引擎放回id会涉及解析数据行，以及拷贝字段值的操作。</p>\n<p>count(字段)：全表扫描，分情况讨论。</p>\n<p>1、如果参数字段定义NOT NULL，判断是不可能为空的，按行累加，不再对每个值进行NULL判断。 2、如果参数字段定义允许为NULL，那么执行的时候，判断可能是NULL，还要把值取出来再判断一下，不是NULL才累加。</p>\n<p>count(*)：统计所有的列，相当于行数，统计结果中会包含字段值为null的列；</p>\n<p><strong>COUNT(<code>*</code>)是SQL92定义的标准统计行数的语法，效率高，MySQL对它进行了很多优化，MyISAM中会直接把表的总行数单独记录下来供COUNT(*)查询，而InnoDB则会在扫表的时候选择最小的索引来降低成本</strong>。</p>\n<p>所以，建议使用COUNT(*)查询表的行数！</p>\n<h2 id=\"49存储md5值应该用varchar还是用char\">49、存储MD5值应该用VARCHAR还是用CHAR？</h2>\n<p>首先说说CHAR和VARCHAR的区别：</p>\n<p>1、存储长度：</p>\n<p><strong>CHAR类型的长度是固定的</strong></p>\n<p>当我们当定义CHAR(10)，输入的值是&quot;abc&quot;，但是它占用的空间一样是10个字节，会包含7个空字节。当输入的字符长度超过指定的数时，CHAR会截取超出的字符。而且，当存储为CHAR的时候，MySQL会自动删除输入字符串末尾的空格。</p>\n<p><strong>VARCHAR的长度是可变的</strong></p>\n<p>比如VARCHAR(10)，然后输入abc三个字符，那么实际存储大小为3个字节。</p>\n<p>除此之外，<strong>VARCHAR还会保留1个或2个额外的字节来记录字符串的实际长度</strong>。如果定义的最大长度小于等于255个字节，那么，就会预留1个字节；如果定义的最大长度大于255个字节，那么就会预留2个字节。</p>\n<p>2、存储效率</p>\n<p><strong>CHAR类型每次修改后的数据长度不变，效率更高</strong>。</p>\n<p>VARCHAR每次修改的数据要更新数据长度，效率更低。</p>\n<p>3、存储空间</p>\n<p>CHAR存储空间是<strong>初始的预计长度字符串再加上一个记录字符串长度的字节</strong>，可能会存在多余的空间。</p>\n<p>VARCHAR存储空间的时候是<strong>实际字符串再加上一个记录字符串长度的字节</strong>，占用空间较小。</p>\n<p>根据以上的分析，由于<strong>MD5是一个定长的值，所以MD5值适合使用CHAR存储</strong>。对于固定长度的非常短的列，CHAR比VARCHAR效率也更高。</p>\n","createTime":"2024-05-07 22:20:34","categoryId":36,"categoryName":"MySQL","readNum":86,"tags":[{"id":63,"name":"mysql","articlesTotal":null}],"preArticle":{"articleId":48,"articleTitle":"JVM"},"nextArticle":{"articleId":45,"articleTitle":"Redis"},"totalWords":22922,"readTime":"约 76 分钟","updateTime":"2024-06-03 20:17:54"}} =================================== 
2024-08-04 21:28:39.782 [http-nio-8088-exec-4] INFO  c.j.w.f.b.c.filter.HeaderUserId2ContextFilter - ===== 删除 ThreadLocal， userId: 1
